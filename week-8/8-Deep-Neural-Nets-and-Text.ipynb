{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week - 8 - Deep Neural Nets and Text\n",
    "\n",
    "In this week we introduce the use of Deep Neural Networks to work with text. We have already seen some uses of neural networks for text in our classification HW, where we used a simple neural network--the one-layer perceptron--to classify text. It performed quite well, but comes up short in more sophisticated classification tasks, such as in predicting intent. We have also seen slightly deeper, 2-level neural nets in the form of word embeddings such as Word2Vec. While they work well, they have some drawbacks, such as representing words with multiple meanings in a singular space. \n",
    "\n",
    "BERT, which is a language model built using bidirectional encoders, allows us to take advantage of a powerful pre-trained model which we can then use to perform our own tasks based on data we analyze. \n",
    "\n",
    "In this notebook we use ```huggingface/transformers```, a python package that allows for easy interface to use pre-trained BERT models. It is built using Tensorflow and PyTorch, two computational graph packages which are built specifically for creating powerful neural networks. We will also be introducing Keras, which allows us to easily build Neural Networks in an abstracted way. Keras is a popular way to understand how we can stack layers to create such Neural Networks, but to reach state-of-the-art results we will stick with using BERT and similar models that can be tuned to extremely high performance on specific language understanding and generation tasks.\n",
    "\n",
    "To demonstrate this, we begin by using the [Corpus of Linguistic Acceptability](https://nyu-mll.github.io/CoLA/). We will also use BERT by learning how to extract embeddings from such a model and use it to semantically probe sentences. There are a number of new packages and methods we will be using so be sure to update lucem_illud_2020.\n",
    "\n",
    "## NOTE\n",
    "\n",
    "This notebook **requires** GPUs for training models in section 1 and section 3. To train models, please use this [Google Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) to create the models. Note that I have only given you view access: please create your own colab file to train your models, using the code and instructions I have given in the Colab file. So while you have to do the homework on this notebook, the models which you will train should be done on Google Colab, which has GPU access. If you happen to have GPU access on your personal machines or some other way to train the models, you are welcome to do that too.\n",
    "\n",
    "Note that if you run the computationally intensive models on your local computer they will take a long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # pip install torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig # pip install tranformers==2.4.1\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lucem_illud #pip install -U git+git://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoLA Dataset and pre-processing\n",
    "\n",
    "We start with loading our dataset and pre-processing it. The pre-processing follows similar steps as we have done in the past, but we will be using pre-written modules offered by the transformers package. These are some of the things we have to take care of when using this particular BERT model.\n",
    "\n",
    "    -special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP])\n",
    "    -tokens that conforms with the fixed vocabulary used in BERT\n",
    "    -token IDs from BERT’s tokenizer\n",
    "    -mask IDs to indicate which elements in the sequence are tokens and which are padding elements\n",
    "    -segment IDs used to distinguish different sentences\n",
    "    -positional embeddings used to show token position within the sequence\n",
    "\n",
    "\n",
    "We will be using parts of the code from [this notebook](https://colab.research.google.com/drive/1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO#scrollTo=BJR6t_gCQe_x) which walks us through the process of using a pre-trained BERT model. The interface to use these models comes from the package [huggingface/transformers](https://github.com/huggingface/transformers). \n",
    "\n",
    "We start by setting up our GPU if we can - this may not work on your machine, so it has been commented out.\n",
    "\n",
    "An aside: check out this tutorial too - https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if gpu:\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e78ba4cbd1d4db5a25a0e368fb88848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenize the first sentence:\n",
      "['[CLS]', 'our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Deep Neural Nets\n",
    "\n",
    "A popular, simplified package for introducing deep neural networks is [Keras](https://keras.io). It is a high level package in that we don't bother with every detail or hyper-parameter associated with the neural network (e.g., regularizers), and can stack on layers directly. For a rapid tutorial on neural networks for text such as the LSTM or the Recurrent Neural Network, Colah's blog is a great start. [LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) is an article on LSTMs, and if you'd like to  learn about RNN, Andrej Karpathy does a great job in [this blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), in addition to our reading from the newest online verion of Jurafsky & Martin's review of deep learning methods in their book on speech and language processing, chapters [6,7,9,10](https://web.stanford.edu/~jurafsky/slp3/), and the [*Deep Learning*](https://www.deeplearningbook.org/) book by Goodfellow, Bengio & Courville.\n",
    "\n",
    "In the following cells we build a basic deep net that has an embedding layer and an LSTM to perform classification. This is to illustrate the process of using Keras, which is a very popular library for such work. It may not yield state of the art performance because it constrains the hyperparameters you can tune, but is nonetheless an useful tool and works well on some datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_in_size = tokenizer.vocab_size\n",
    "embedding_dim = 32\n",
    "unit = 100\n",
    "no_labels = len(np.unique(train_labels))\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 128, 32)           976704    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,030,106\n",
      "Trainable params: 1,030,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm.add(LSTM(unit))\n",
    "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "241/241 [==============================] - 16s 61ms/step - loss: 0.6216 - accuracy: 0.7026\n",
      "Epoch 2/10\n",
      "241/241 [==============================] - 15s 61ms/step - loss: 0.6027 - accuracy: 0.7118\n",
      "Epoch 3/10\n",
      "241/241 [==============================] - 15s 63ms/step - loss: 0.6068 - accuracy: 0.7063\n",
      "Epoch 4/10\n",
      "241/241 [==============================] - 15s 63ms/step - loss: 0.6030 - accuracy: 0.7113\n",
      "Epoch 5/10\n",
      "241/241 [==============================] - 16s 66ms/step - loss: 0.6130 - accuracy: 0.6978\n",
      "Epoch 6/10\n",
      "241/241 [==============================] - 16s 66ms/step - loss: 0.6098 - accuracy: 0.7023\n",
      "Epoch 7/10\n",
      "241/241 [==============================] - 15s 62ms/step - loss: 0.6054 - accuracy: 0.7074\n",
      "Epoch 8/10\n",
      "241/241 [==============================] - 17s 70ms/step - loss: 0.6078 - accuracy: 0.7045\n",
      "Epoch 9/10\n",
      "241/241 [==============================] - 16s 66ms/step - loss: 0.6176 - accuracy: 0.6931\n",
      "Epoch 10/10\n",
      "241/241 [==============================] - 16s 68ms/step - loss: 0.6165 - accuracy: 0.6940\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
    "                              epochs=10,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy of this model isn't terrible, but it still hovers around 70%. Below there is code for a slightly modified neural network - how does this one perform? Note that in this model, I have added another LSTM layer. You are encouraged to explore the [Keras documentaion](https://keras.io/layers/about-keras-layers/) to explore what kind of layers you can add and how they change performances for different tasks. Different kinds of losses, optimizers, activations and layers can change the flavour of your net dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_lstm2 = Sequential()\n",
    "# model_lstm2.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "# model_lstm2.add(LSTM(units))\n",
    "# model_lstm2.add(LSTM(units))\n",
    "# model_lstm2.add(Dense(1, activation='sigmoid'))\n",
    "# model_lstm2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# history_lstm2 = model_lstm2.fit(input_data_train, labels, epochs=10, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On with BERT!\n",
    "\n",
    "So while Neural Networks can do a good job with some kind of classification tasks, they don't perform too well on intent classification. Let us see how a bidirectional transformer embedding like BERT might do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our Models\n",
    "\n",
    "### Train Model\n",
    "Now that our input data is properly formatted, it's time to fine tune the BERT model.\n",
    "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n",
    "We'll load BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
    "\n",
    "### Structure of Fine-Tuning Model\n",
    "\n",
    "As we've showed beforehand, the first token of every sequence is the special classification token ([CLS]). Unlike the hidden state vector corresponding to a normal word token, the hidden state corresponding to this special token is designated by the authors of BERT as an aggregate representation of the whole sentence used for classification tasks. As such, when we feed in an input sentence to our model during training, the output is the length 768 hidden state vector corresponding to this token. The additional layer that we've added on top consists of untrained linear neurons of size [hidden_state, number_of_labels], so [768,2], meaning that the output of BERT plus our classification layer is a vector of two numbers representing the \"score\" for \"grammatical/non-grammatical\" that are then fed into cross-entropy loss.\n",
    "\n",
    "### The Fine-Tuning Process\n",
    "\n",
    "Because the pre-trained BERT layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it's as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task.\n",
    "Sometimes practicioners will opt to \"freeze\" certain layers when fine-tuning, or to apply different learning rates, apply diminishing learning rates, etc. all in an effort to preserve the good quality weights in the network and speed up training (often considerably). In fact, recent research on BERT specifically has demonstrated that freezing the majority of the weights results in only minimal accuracy declines, but there are exceptions and broader rules of transfer learning that should also be considered. For example, if your task and fine-tuning dataset is very different from the dataset used to train the transfer learning model, freezing the weights may not be a good idea. OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\").\n",
    "\n",
    "Credit to Michel Kana's [tutorial](https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03) and the [tutorial](https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP#scrollTo=GuE5BqICAne2) by Chris McCormick and Nick Ryan who describe the workings of BERT and the way it is used by the ```transformers``` package. \n",
    "\n",
    "## WARNING: SHIFT TO A GPU ENABLED MACHINE (e.g., Google Colab)\n",
    "\n",
    "Note that you only want to run the following code if you have a GPU. Otherwise, rerun the **same** cells we just ran on the Colab file to train your model, download it to your local, and load it by running\n",
    "```model = BertForSequenceClassification.from_pretrained(\"my_model_directory\", num_labels=2)```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following cell can take upto 12 hours or longer if run without a GPU. The [Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) demonstrates how to fine-tune models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # This training code is based on the `run_glue.py` script here:\n",
    "# # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# # Set the seed value all over the place to make this reproducible.\n",
    "# seed_val = 42\n",
    "\n",
    "# random.seed(seed_val)\n",
    "# np.random.seed(seed_val)\n",
    "# torch.manual_seed(seed_val)\n",
    "# torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# # Store the average loss after each epoch so we can plot them.\n",
    "# loss_values = []\n",
    "\n",
    "# # For each epoch...\n",
    "# for epoch_i in range(0, epochs):\n",
    "    \n",
    "#     # ========================================\n",
    "#     #               Training\n",
    "#     # ========================================\n",
    "    \n",
    "#     # Perform one full pass over the training set.\n",
    "\n",
    "#     print(\"\")\n",
    "#     print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "#     print('Training...')\n",
    "\n",
    "#     # Measure how long the training epoch takes.\n",
    "#     t0 = time.time()\n",
    "\n",
    "#     # Reset the total loss for this epoch.\n",
    "#     total_loss = 0\n",
    "\n",
    "#     # Put the model into training mode. Don't be mislead--the call to \n",
    "#     # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "#     # `dropout` and `batchnorm` layers behave differently during training\n",
    "#     # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "#     model.train()\n",
    "\n",
    "#     # For each batch of training data...\n",
    "#     for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "#         # Progress update every 40 batches.\n",
    "#         if step % 40 == 0 and not step == 0:\n",
    "#             # Calculate elapsed time in minutes.\n",
    "#             elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "#             # Report progress.\n",
    "#             print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "#         # Unpack this training batch from our dataloader. \n",
    "#         #\n",
    "#         # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "#         # `to` method.\n",
    "#         #\n",
    "#         # `batch` contains three pytorch tensors:\n",
    "#         #   [0]: input ids \n",
    "#         #   [1]: attention masks\n",
    "#         #   [2]: labels \n",
    "#         b_input_ids = batch[0].to(device)\n",
    "#         b_input_mask = batch[1].to(device)\n",
    "#         b_labels = batch[2].to(device)\n",
    "\n",
    "#         # Always clear any previously calculated gradients before performing a\n",
    "#         # backward pass. PyTorch doesn't do this automatically because \n",
    "#         # accumulating the gradients is \"convenient while training RNNs\". \n",
    "#         # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "#         model.zero_grad()        \n",
    "\n",
    "#         # Perform a forward pass (evaluate the model on this training batch).\n",
    "#         # This will return the loss (rather than the model output) because we\n",
    "#         # have provided the `labels`.\n",
    "#         # The documentation for this `model` function is here: \n",
    "#         # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "#         outputs = model(b_input_ids, \n",
    "#                     token_type_ids=None, \n",
    "#                     attention_mask=b_input_mask, \n",
    "#                     labels=b_labels)\n",
    "        \n",
    "#         # The call to `model` always returns a tuple, so we need to pull the \n",
    "#         # loss value out of the tuple.\n",
    "#         loss = outputs[0]\n",
    "\n",
    "#         # Accumulate the training loss over all of the batches so that we can\n",
    "#         # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "#         # single value; the `.item()` function just returns the Python value \n",
    "#         # from the tensor.\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Perform a backward pass to calculate the gradients.\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Clip the norm of the gradients to 1.0.\n",
    "#         # This is to help prevent the \"exploding gradients\" problem.\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "#         # Update parameters and take a step using the computed gradient.\n",
    "#         # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "#         # modified based on their gradients, the learning rate, etc.\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Update the learning rate.\n",
    "#         scheduler.step()\n",
    "\n",
    "#     # Calculate the average loss over the training data.\n",
    "#     avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "#     # Store the loss value for plotting the learning curve.\n",
    "#     loss_values.append(avg_train_loss)\n",
    "\n",
    "#     print(\"\")\n",
    "#     print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "#     print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "#     # ========================================\n",
    "#     #               Validation\n",
    "#     # ========================================\n",
    "#     # After the completion of each training epoch, measure our performance on\n",
    "#     # our validation set.\n",
    "\n",
    "#     print(\"\")\n",
    "#     print(\"Running Validation...\")\n",
    "\n",
    "#     t0 = time.time()\n",
    "\n",
    "#     # Put the model in evaluation mode--the dropout layers behave differently\n",
    "#     # during evaluation.\n",
    "#     model.eval()\n",
    "\n",
    "#     # Tracking variables \n",
    "#     eval_loss, eval_accuracy = 0, 0\n",
    "#     nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "#     # Evaluate data for one epoch\n",
    "#     for batch in validation_dataloader:\n",
    "        \n",
    "#         # Add batch to GPU\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "#         # Unpack the inputs from our dataloader\n",
    "#         b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "#         # Telling the model not to compute or store gradients, saving memory and\n",
    "#         # speeding up validation\n",
    "#         with torch.no_grad():        \n",
    "\n",
    "#             # Forward pass, calculate logit predictions.\n",
    "#             # This will return the logits rather than the loss because we have\n",
    "#             # not provided labels.\n",
    "#             # token_type_ids is the same as the \"segment ids\", which \n",
    "#             # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "#             # The documentation for this `model` function is here: \n",
    "#             # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "#             outputs = model(b_input_ids, \n",
    "#                             token_type_ids=None, \n",
    "#                             attention_mask=b_input_mask)\n",
    "        \n",
    "#         # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "#         # values prior to applying an activation function like the softmax.\n",
    "#         logits = outputs[0]\n",
    "\n",
    "#         # Move logits and labels to CPU\n",
    "#         logits = logits.detach().cpu().numpy()\n",
    "#         label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "#         # Calculate the accuracy for this batch of test sentences.\n",
    "#         tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "#         # Accumulate the total accuracy.\n",
    "#         eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "#         # Track the number of batches\n",
    "#         nb_eval_steps += 1\n",
    "\n",
    "#     # Report the final accuracy for this validation run.\n",
    "#     print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "#     print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "# print(\"\")\n",
    "# print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Use plot styling from seaborn.\n",
    "# sns.set(style='darkgrid')\n",
    "\n",
    "# # Increase the plot size and font size.\n",
    "# sns.set(font_scale=1.5)\n",
    "# plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# # Plot the learning curve.\n",
    "# plt.plot(loss_values, 'b-o')\n",
    "\n",
    "# # Label the plot.\n",
    "# plt.title(\"Training loss\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COME BACK TO THIS NOTEBOOK to load and work with your trained model\n",
    "\n",
    "Once you tune your model on Colab (or on your own machine if you decided to do that instead), you load it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"model_save\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"../data/cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask) \n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 516 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 354 of 516 (68.60%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhargavvader/open_source/Content-Analysis-2020/venv/lib/python3.5/site-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "\n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "\n",
    "    # Calculate and store the coef for this batch.  \n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.049286405809014416,\n",
       " 0.014456362470655182,\n",
       " 0.4732058754737091,\n",
       " 0.4414147946478204,\n",
       " 0.44440090347500916,\n",
       " 0.7410010097502685,\n",
       " 0.6201736729460423,\n",
       " 0.47519096331149147,\n",
       " 1.0,\n",
       " 0.5659164584181102,\n",
       " 0.7679476477883045,\n",
       " 0.647150228929434,\n",
       " 0.8150678894028793,\n",
       " 0.647150228929434,\n",
       " 0.3268228676411533,\n",
       " 0.5844155844155844,\n",
       " 0.0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.550\n"
     ]
    }
   ],
   "source": [
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty good performance. Note that we used [Matthews Correlation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) to meausure this. It ranges from -1 to 1, with +1 being the best. The Google BERT model has a similar score too, so this model performed quite well. It took a long time though, approximately a day with no GPU. It would be significantly faster if a CUDA enabled machine ran this. Hence, we recommend that you run this on the Collab notebook.\n",
    "\n",
    "The following lines save the model to disk, if you would like to: note that we ran this in the colab file to save it to disk there as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "# output_dir = './model_save/'\n",
    "\n",
    "# # Create output directory if needed\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# # They can then be reloaded using `from_pretrained()`\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "# model_to_save.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# # Good practice: save your training arguments together with the trained model\n",
    "# # torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that estimate a deep classification model with Keras (and LSTM) and also BERT in order to predict pre-established data labels relevant to your final project (as for week 3's homework). Which works better? Are the errors the same or different?\n",
    "\n",
    "<span style=\"color:red\">***Stretch***</span>: <span style=\"color:red\">Now alter the neural network by stacking network layers, adjusting the embedding dimension, compare its performance with your model above, and interpret why it might be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>book</th>\n",
       "      <th>year</th>\n",
       "      <th>label</th>\n",
       "      <th>elisabeth_published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26476</th>\n",
       "      <td>I am not much more than an animal which hath b...</td>\n",
       "      <td>zarathustra</td>\n",
       "      <td>1883</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23597</th>\n",
       "      <td>Happiness can be promised only by Being: chang...</td>\n",
       "      <td>will3-4</td>\n",
       "      <td>1901</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22645</th>\n",
       "      <td>There was no room found for dialectics.</td>\n",
       "      <td>will1-2</td>\n",
       "      <td>1901</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18151</th>\n",
       "      <td>Maybe I know the Germans, perhaps I may tell t...</td>\n",
       "      <td>twilight</td>\n",
       "      <td>1888</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>In it humanity does not even make the first st...</td>\n",
       "      <td>antichrist</td>\n",
       "      <td>1895</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text         book  year  \\\n",
       "26476  I am not much more than an animal which hath b...  zarathustra  1883   \n",
       "23597  Happiness can be promised only by Being: chang...      will3-4  1901   \n",
       "22645            There was no room found for dialectics.      will1-2  1901   \n",
       "18151  Maybe I know the Germans, perhaps I may tell t...     twilight  1888   \n",
       "558    In it humanity does not even make the first st...   antichrist  1895   \n",
       "\n",
       "       label  elisabeth_published  \n",
       "26476      1                    0  \n",
       "23597      1                    1  \n",
       "22645      1                    1  \n",
       "18151      1                    0  \n",
       "558        1                    1  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label = 1 means that the text is from Nietzsche's work after 1879, \n",
    "# the year where his health conditions force him to resign his professorship at Basel.\n",
    "ndf = pd.read_csv('../../corpus/nietzsche/nietzsche_gutenberg.csv')\n",
    "ndf.sample(5, random_state=20210310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out a holdout set from data\n",
    "main_set, holdout_set = train_test_split(ndf, test_size=0.2, random_state=20210310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_set.to_csv('../../corpus/nietzsche/nietzsche_gutenberg_main.csv', index=False)\n",
    "holdout_set.to_csv('../../corpus/nietzsche/nietzsche_gutenberg_holdout.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = main_set.text.values\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = main_set.label.values\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'he', 'who', 'can', 'manage', 'this', 'explanation', 'with', 'the', 'simpler', 'and', 'more', 'known', 'forces', ',', 'especially', 'the', 'mechanical', 'ones', ',', 'he', 'who', 'de', '##du', '##ces', 'the', 'existing', 'ed', '##ifice', 'of', 'the', 'world', 'out', 'of', 'the', 'smallest', 'possible', 'number', 'of', 'forces', ',', 'will', 'always', 'be', 'preferred', 'to', 'him', 'who', 'allows', 'the', 'more', 'complicated', 'and', 'less', '-', 'known', 'forces', ',', 'and', 'these', 'moreover', 'in', 'greater', 'number', ',', 'to', 'carry', 'on', 'a', 'world', '-', 'creating', 'play', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = []\n",
    "\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model parameters\n",
    "vocab_in_size = tokenizer.vocab_size\n",
    "embedding_dim = 32\n",
    "unit = 100\n",
    "no_labels = len(np.unique(train_labels))\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 128, 32)           976704    \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,030,106\n",
      "Trainable params: 1,030,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The first LSTM model, with one hidden layer\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm.add(LSTM(unit))\n",
    "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "757/757 [==============================] - 52s 67ms/step - loss: 0.5813 - accuracy: 0.7370\n",
      "Epoch 2/10\n",
      "757/757 [==============================] - 52s 69ms/step - loss: 0.5666 - accuracy: 0.7386\n",
      "Epoch 3/10\n",
      "757/757 [==============================] - 50s 66ms/step - loss: 0.5768 - accuracy: 0.7351\n",
      "Epoch 4/10\n",
      "757/757 [==============================] - 52s 68ms/step - loss: 0.5731 - accuracy: 0.7419\n",
      "Epoch 5/10\n",
      "757/757 [==============================] - 50s 66ms/step - loss: 0.5684 - accuracy: 0.7434\n",
      "Epoch 6/10\n",
      "757/757 [==============================] - 50s 66ms/step - loss: 0.5715 - accuracy: 0.7419\n",
      "Epoch 7/10\n",
      "757/757 [==============================] - 50s 66ms/step - loss: 0.5697 - accuracy: 0.7417\n",
      "Epoch 8/10\n",
      "757/757 [==============================] - 51s 67ms/step - loss: 0.5755 - accuracy: 0.7368\n",
      "Epoch 9/10\n",
      "757/757 [==============================] - 48s 63ms/step - loss: 0.4782 - accuracy: 0.7888\n",
      "Epoch 10/10\n",
      "757/757 [==============================] - 49s 64ms/step - loss: 0.3052 - accuracy: 0.8780\n"
     ]
    }
   ],
   "source": [
    "nietzsche_lstm = model_lstm.fit(train_inputs, train_labels, \n",
    "                              epochs=10,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 128, 32)           976704    \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 128, 100)          53200     \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,110,405\n",
      "Trainable params: 1,110,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "757/757 [==============================] - 98s 126ms/step - loss: 0.5890 - accuracy: 0.7344\n",
      "Epoch 2/10\n",
      "757/757 [==============================] - 95s 126ms/step - loss: 0.5757 - accuracy: 0.7386\n",
      "Epoch 3/10\n",
      "757/757 [==============================] - 95s 126ms/step - loss: 0.5795 - accuracy: 0.7351\n",
      "Epoch 4/10\n",
      "757/757 [==============================] - 92s 121ms/step - loss: 0.5825 - accuracy: 0.7313\n",
      "Epoch 5/10\n",
      "757/757 [==============================] - 93s 122ms/step - loss: 0.5789 - accuracy: 0.7351\n",
      "Epoch 6/10\n",
      "757/757 [==============================] - 94s 124ms/step - loss: 0.5787 - accuracy: 0.7353\n",
      "Epoch 7/10\n",
      "757/757 [==============================] - 95s 125ms/step - loss: 0.5782 - accuracy: 0.7353\n",
      "Epoch 8/10\n",
      "757/757 [==============================] - 94s 124ms/step - loss: 0.5743 - accuracy: 0.7390\n",
      "Epoch 9/10\n",
      "757/757 [==============================] - 109s 144ms/step - loss: 0.5770 - accuracy: 0.7365\n",
      "Epoch 10/10\n",
      "757/757 [==============================] - 98s 130ms/step - loss: 0.5740 - accuracy: 0.7394\n"
     ]
    }
   ],
   "source": [
    "# The second LSTM model, with two hidden layers\n",
    "\n",
    "model_lstm2 = Sequential()\n",
    "model_lstm2.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm2.add(LSTM(unit, return_sequences=True))\n",
    "model_lstm2.add(LSTM(unit))\n",
    "model_lstm2.add(Dense(1, activation='sigmoid'))\n",
    "model_lstm2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm2.summary()\n",
    "\n",
    "nietzsche_lstm2 = model_lstm2.fit(train_inputs, train_labels, epochs=10, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of layers\n",
    "We can observe that the additional layer isn't improving the model, possibly because the second model is overfitting the training set on the second layer, making the validation result worse. Our classification problem is not complex enough that we need two hidden layers to solve.\n",
    "\n",
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Model trained on colab, please refer to colab notebook\n",
    "model = BertForSequenceClassification.from_pretrained(\"model_nietzsche_clf\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = holdout_set.text.values\n",
    "labels = holdout_set.label.values\n",
    "\n",
    "input_ids = []\n",
    "\n",
    "for sent in sentences:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True, \n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks = []\n",
    "\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask) \n",
    "\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 6,726 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 6023 of 8551 (70.44%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment\n",
    "The BERT model overall is performing a bit better than the LSTM model. During the training in collab, the validation accuracy reached 88%. The LSTM model (one hidden layer) comes close, tops to 87.8% in the last epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings, Context Words\n",
    "\n",
    "We saw how a bootstrapped BERT model performed so much better than a model trained from scatch. Because BERT's method of capturing context is bidirectional, meaning that words can now have different word embedding values based on their location within a sentence. Let us use the same BERT model to capture sentence and word embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through the sentence format for the BERT model, as well as how our vocabulary looks like. Note that you have to use the BERT tokenizer to use the BERT model because of the similar vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERTS model uses a WordPiece technique to do its tokenizing, as described in the paper. That's why the word embedding is split up the way it is.\n",
    "A quick peek at what the voabulary looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['peninsula',\n",
       " 'adults',\n",
       " 'novels',\n",
       " 'emerged',\n",
       " 'vienna',\n",
       " 'metro',\n",
       " 'debuted',\n",
       " 'shoes',\n",
       " 'tamil',\n",
       " 'songwriter',\n",
       " 'meets',\n",
       " 'prove',\n",
       " 'beating',\n",
       " 'instance',\n",
       " 'heaven',\n",
       " 'scared',\n",
       " 'sending',\n",
       " 'marks',\n",
       " 'artistic',\n",
       " 'passage',\n",
       " 'superior',\n",
       " '03',\n",
       " 'significantly',\n",
       " 'shopping',\n",
       " '##tive',\n",
       " 'retained',\n",
       " '##izing',\n",
       " 'malaysia',\n",
       " 'technique',\n",
       " 'cheeks']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[6000:6030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "after         2,044\n",
      "stealing     11,065\n",
      "money         2,769\n",
      "from          2,013\n",
      "the           1,996\n",
      "bank          2,924\n",
      "vault        11,632\n",
      ",             1,010\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "was           2,001\n",
      "seen          2,464\n",
      "fishing       5,645\n",
      "on            2,006\n",
      "the           1,996\n",
      "mississippi   5,900\n",
      "river         2,314\n",
      "bank          2,924\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indices.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment ID\n",
    "\n",
    "BERT is trained on and expects sentence pairs, using 1s and 0s to distinguish between the two sentences. That is, for each token in “tokenized_text,” we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). For our purposes, single-sentence inputs only require a series of 1s, so we will create a vector of 1s for each token in our input sentence.\n",
    "\n",
    "If you want to process two sentences, assign each word in the first sentence plus the ‘[SEP]’ token a 0, and all tokens of the second sentence a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did for classification, we now convert these segments to tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer is the hidden state layer, and this is what we pick up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model_embedding = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model_embedding.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_embedding(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.4964, -0.1831, -0.5231,  ..., -0.1902,  0.3738,  0.3964],\n",
       "         [-0.1323, -0.2762, -0.3495,  ..., -0.4567,  0.3786, -0.1096],\n",
       "         [-0.3626, -0.4002,  0.0676,  ..., -0.3207, -0.2709, -0.3004],\n",
       "         ...,\n",
       "         [ 0.2961, -0.2856, -0.0382,  ..., -0.6056, -0.5163,  0.2005],\n",
       "         [ 0.4878, -0.0909, -0.2358,  ..., -0.0017, -0.5945, -0.2431],\n",
       "         [-0.2517, -0.3519, -0.4688,  ...,  0.2500,  0.0336, -0.2627]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.6031, -0.3342, -0.7174,  0.3347,  0.5145, -0.1722,  0.4502,  0.2768,\n",
       "         -0.3769, -0.9998, -0.3657,  0.7535,  0.9817, -0.0192,  0.7959, -0.3459,\n",
       "         -0.1338, -0.3026,  0.1097,  0.5836,  0.5736,  0.9999,  0.1798,  0.1845,\n",
       "          0.2250,  0.9109, -0.5653,  0.8616,  0.8994,  0.7423, -0.2525,  0.0394,\n",
       "         -0.9894, -0.1331, -0.7763, -0.9826,  0.2223, -0.6115,  0.1941,  0.0177,\n",
       "         -0.7634,  0.2312,  0.9999, -0.7000,  0.4623, -0.2202, -1.0000,  0.1908,\n",
       "         -0.8150,  0.6483,  0.5878,  0.8198,  0.1014,  0.3185,  0.3963, -0.3216,\n",
       "         -0.1701,  0.0588, -0.1544, -0.4987, -0.5284,  0.1228, -0.4823, -0.7788,\n",
       "          0.6954,  0.0891, -0.0855, -0.1500,  0.0390, -0.0760,  0.6154,  0.2662,\n",
       "         -0.0129, -0.7253,  0.1352,  0.2921, -0.5613,  1.0000,  0.1536, -0.9681,\n",
       "          0.7166,  0.2600,  0.4519,  0.5470, -0.2798, -1.0000,  0.3419, -0.2645,\n",
       "         -0.9863,  0.1263,  0.5249, -0.2000,  0.5980,  0.4752, -0.2355, -0.4808,\n",
       "         -0.3786, -0.7284, -0.0909,  0.0124, -0.0689, -0.2531, -0.1324, -0.2361,\n",
       "          0.1732, -0.3216, -0.0188,  0.2302, -0.3221,  0.4996,  0.4346, -0.1935,\n",
       "          0.2968, -0.9292,  0.5326, -0.3695, -0.9876, -0.4770, -0.9902,  0.6349,\n",
       "         -0.1863, -0.2612,  0.9123, -0.1930,  0.3110,  0.0803, -0.7598, -1.0000,\n",
       "          0.0292, -0.0628, -0.1086, -0.2135, -0.9671, -0.9521,  0.3334,  0.8675,\n",
       "          0.2254,  0.9995, -0.2908,  0.9420,  0.0336, -0.4542,  0.4123, -0.4455,\n",
       "          0.4558, -0.4442, -0.0685,  0.3043,  0.0575,  0.1843, -0.6577, -0.3121,\n",
       "         -0.1069, -0.7729, -0.2328,  0.9032, -0.4681, -0.5580,  0.3359, -0.1644,\n",
       "         -0.1572,  0.6441,  0.2810,  0.2651,  0.1532,  0.4485, -0.5299,  0.2616,\n",
       "         -0.7413, -0.0347,  0.2560, -0.2659, -0.6092, -0.9868, -0.2346,  0.4682,\n",
       "          0.9771,  0.5646,  0.2360,  0.4567, -0.2170,  0.1420, -0.9554,  0.9832,\n",
       "         -0.0935,  0.2621, -0.7901,  0.5758, -0.7642, -0.5362,  0.6374, -0.3891,\n",
       "         -0.6368, -0.0045, -0.2658, -0.1721, -0.7466,  0.4832, -0.3128, -0.2640,\n",
       "          0.0435,  0.8879,  0.5961,  0.2972,  0.0742,  0.3041, -0.7367, -0.2117,\n",
       "         -0.0500,  0.0825,  0.0273,  0.9870, -0.5612, -0.0353, -0.8011, -0.9833,\n",
       "         -0.1874, -0.7143,  0.0476, -0.4776,  0.4406, -0.7087, -0.3319,  0.1035,\n",
       "         -0.2959, -0.6915,  0.2840, -0.5485,  0.3292, -0.3018,  0.8687,  0.7867,\n",
       "         -0.5158, -0.2411,  0.9175, -0.7601, -0.6967, -0.0763, -0.1411,  0.6739,\n",
       "         -0.6006,  0.9645,  0.6723,  0.3451, -0.9112, -0.6897, -0.3042, -0.0059,\n",
       "         -0.0918, -0.6402,  0.5602,  0.3850,  0.2737,  0.7868, -0.4155,  0.8343,\n",
       "         -0.9288, -0.9377, -0.9803,  0.1936, -0.9873,  0.7978,  0.1702,  0.6547,\n",
       "         -0.3664, -0.3837, -0.9602,  0.2690,  0.0832,  0.8077, -0.5984, -0.5181,\n",
       "         -0.4783, -0.9236, -0.0773, -0.0542,  0.1906,  0.0659, -0.8882,  0.3529,\n",
       "          0.5142,  0.4257, -0.8504,  0.9686,  1.0000,  0.9745,  0.7975,  0.3701,\n",
       "         -0.9993, -0.9159,  0.9999, -0.9649, -1.0000, -0.8484, -0.5494,  0.2254,\n",
       "         -1.0000, -0.0892,  0.0449, -0.8989,  0.2432,  0.9673,  0.7904, -1.0000,\n",
       "          0.8539,  0.8224, -0.5027,  0.8026, -0.2741,  0.9723,  0.3954,  0.5613,\n",
       "         -0.2778,  0.4943, -0.7842, -0.5614, -0.4786, -0.7206,  0.9944, -0.0361,\n",
       "         -0.2827, -0.8623,  0.6206, -0.0204, -0.2777, -0.9243, -0.2374,  0.5576,\n",
       "          0.4833,  0.1621,  0.2181, -0.4744,  0.1432, -0.0688, -0.3646,  0.5594,\n",
       "         -0.8221, -0.1388,  0.5845, -0.0662,  0.0617, -0.9573,  0.9249, -0.4059,\n",
       "          0.5819,  1.0000,  0.8847, -0.6462,  0.4425,  0.1442,  0.2397,  1.0000,\n",
       "          0.3757, -0.9790, -0.5021,  0.5227, -0.4537, -0.4564,  0.9975, -0.1468,\n",
       "         -0.4530, -0.2211,  0.9849, -0.9892,  0.9874, -0.6354, -0.9533,  0.9617,\n",
       "          0.9100, -0.3272, -0.5809,  0.1221,  0.2137,  0.1775, -0.6459,  0.5595,\n",
       "          0.3555,  0.0030,  0.7183,  0.1490, -0.4345,  0.2424, -0.5088,  0.0014,\n",
       "          0.9025,  0.3578, -0.0445, -0.0357, -0.2723, -0.8130, -0.9344,  0.4227,\n",
       "          1.0000, -0.1721,  0.8243,  0.0665,  0.0115, -0.1497,  0.4186,  0.3168,\n",
       "         -0.3061, -0.5515,  0.7428, -0.7339, -0.9949,  0.1418,  0.0288,  0.1303,\n",
       "          0.9994,  0.6172,  0.1452,  0.4381,  0.9631, -0.0898, -0.1024,  0.3720,\n",
       "          0.9655, -0.2082,  0.4160,  0.2856, -0.4470, -0.0806, -0.5101, -0.1494,\n",
       "         -0.8861,  0.3496, -0.9617,  0.9112,  0.9032,  0.4198,  0.1110,  0.5989,\n",
       "          1.0000, -0.9781,  0.0478,  0.8043, -0.0370, -0.9994, -0.4571, -0.3764,\n",
       "         -0.0341, -0.3013,  0.0021,  0.1011, -0.9597,  0.1910,  0.7922, -0.5135,\n",
       "         -0.9862,  0.0285,  0.4023,  0.2365, -0.9728, -0.3717, -0.4726,  0.2997,\n",
       "         -0.0355, -0.9264,  0.3197, -0.3812,  0.3245, -0.1921,  0.4575,  0.3563,\n",
       "          0.9515, -0.8870, -0.3251, -0.1148, -0.6957,  0.4810, -0.2975, -0.7607,\n",
       "         -0.1648,  1.0000, -0.4754,  0.5410,  0.3828,  0.1635, -0.2463,  0.1942,\n",
       "          0.8299,  0.1894,  0.0876, -0.6173,  0.7333, -0.2475,  0.3843,  0.7312,\n",
       "         -0.0760,  0.6759,  0.7408,  0.1733,  0.0710,  0.0930,  0.9232, -0.0188,\n",
       "         -0.2704, -0.3332, -0.0659, -0.3261,  0.7431,  1.0000,  0.1622,  0.5590,\n",
       "         -0.9939, -0.7758, -0.7434,  1.0000,  0.8649, -0.6865,  0.5643,  0.4938,\n",
       "         -0.2507, -0.0745, -0.1468, -0.2566,  0.2531,  0.0162,  0.9603, -0.5751,\n",
       "         -0.9828, -0.4233,  0.3365, -0.9327,  0.9997, -0.5279, -0.2036, -0.2868,\n",
       "         -0.4147, -0.9348, -0.0477, -0.9812, -0.1836,  0.1210,  0.9609,  0.2963,\n",
       "         -0.4743, -0.7886,  0.8539,  0.4805, -0.8080, -0.9297,  0.9734, -0.8989,\n",
       "          0.3318,  1.0000,  0.4983,  0.0046,  0.1519, -0.2174,  0.3163, -0.3975,\n",
       "          0.5479, -0.9144, -0.0836, -0.1866,  0.3943, -0.1005, -0.9010,  0.5822,\n",
       "          0.0541, -0.4141, -0.4825, -0.0623,  0.3400,  0.6302, -0.1646, -0.0293,\n",
       "          0.1773,  0.0035, -0.7814, -0.2836, -0.3813, -0.9999,  0.5009, -1.0000,\n",
       "          0.6917, -0.3492, -0.2238,  0.7105,  0.8107,  0.7481, -0.4549, -0.4793,\n",
       "          0.7201,  0.6103, -0.1528, -0.2139, -0.4457,  0.2485,  0.0196,  0.1607,\n",
       "         -0.3812,  0.5145, -0.2807,  1.0000,  0.0829, -0.3200, -0.5929,  0.2053,\n",
       "         -0.2679,  1.0000, -0.2241, -0.9651,  0.1681, -0.5537, -0.5055,  0.4764,\n",
       "         -0.0439, -0.7710, -0.8442,  0.7623,  0.3223, -0.5828,  0.5065, -0.2420,\n",
       "         -0.2434, -0.0287,  0.8268,  0.9844,  0.8406,  0.3518, -0.9611, -0.3503,\n",
       "          0.9309,  0.1013, -0.1565, -0.0012,  1.0000,  0.4139, -0.7002,  0.0942,\n",
       "         -0.8437, -0.2698, -0.7333,  0.2633,  0.0600,  0.9015, -0.2510,  0.9170,\n",
       "         -0.7876, -0.0735, -0.5044,  0.2206,  0.2786, -0.8753, -0.9837, -0.9885,\n",
       "          0.5545, -0.2806, -0.0823,  0.3698,  0.1557,  0.2687,  0.4354, -1.0000,\n",
       "          0.9126,  0.3169,  0.7036,  0.9720,  0.4816,  0.6401,  0.3309, -0.9813,\n",
       "         -0.6150, -0.3007, -0.1601,  0.3913,  0.3273,  0.6264,  0.2662, -0.3612,\n",
       "         -0.6613, -0.3084, -0.9703, -0.9852,  0.3886,  0.0251, -0.3822,  0.9373,\n",
       "         -0.0961,  0.0824,  0.4406, -0.5929,  0.1701,  0.6097,  0.1687, -0.0525,\n",
       "          0.5533,  0.8141,  0.7725,  0.9789, -0.7193,  0.2995, -0.5652,  0.3757,\n",
       "          0.9063, -0.9190,  0.0740,  0.3566, -0.1535,  0.1921, -0.1948, -0.4410,\n",
       "          0.9270, -0.0852,  0.3333, -0.2275,  0.1727, -0.3694, -0.1233, -0.7592,\n",
       "         -0.5162,  0.4856, -0.1530,  0.8161,  0.7259, -0.0103, -0.4483, -0.1682,\n",
       "          0.0424, -0.8891,  0.2156, -0.0442,  0.6451,  0.3764, -0.4232,  0.9787,\n",
       "         -0.0989, -0.3853, -0.3173, -0.5348,  0.5307, -0.6883, -0.3786, -0.4665,\n",
       "          0.7207,  0.3239,  0.9999, -0.5526, -0.4825, -0.3439, -0.3580,  0.1069,\n",
       "         -0.2620, -1.0000,  0.2673, -0.3037,  0.4706, -0.6132,  0.8511, -0.4910,\n",
       "         -0.7231, -0.1092,  0.5412,  0.5265, -0.4635, -0.2461,  0.4946, -0.3469,\n",
       "          0.9149,  0.5902, -0.1862,  0.7004,  0.5465, -0.2839, -0.5723,  0.6648]],\n",
       "       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0][0][0]), len(output[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "This kind of forward pass returns us the last layer of the net, which we will use to make our vectors. The first object returned contains the batch number, followed by each of the tokens and their vector values. The second object contains a vector value, which I suspect is the sentence vector of the tokens. \n",
    "\n",
    "The first index is the batch size, and our batch size is 1, so we just choose the 0th index and work with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = output.last_hidden_state \n",
    "sentence_embedding = output.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4964, -0.1831, -0.5231,  ..., -0.1902,  0.3738,  0.3964],\n",
       "         [-0.1323, -0.2762, -0.3495,  ..., -0.4567,  0.3786, -0.1096],\n",
       "         [-0.3626, -0.4002,  0.0676,  ..., -0.3207, -0.2709, -0.3004],\n",
       "         ...,\n",
       "         [ 0.2961, -0.2856, -0.0382,  ..., -0.6056, -0.5163,  0.2005],\n",
       "         [ 0.4878, -0.0909, -0.2358,  ..., -0.0017, -0.5945, -0.2431],\n",
       "         [-0.2517, -0.3519, -0.4688,  ...,  0.2500,  0.0336, -0.2627]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a quick look at the range of values for a given layer and token.\n",
    "\n",
    "You’ll find that the range is fairly similar for all layers and tokens, with the majority of values falling between [-2, 2], and a small smattering of values around -10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-b3c486538d13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plot the values as a histogram to show their distribution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "vec = word_embeddings[0][0]\n",
    "vec = vec.detach().numpy()\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are grouped by layer - we can use the permute function to make it grouped by each individual token instead. Let us look at what the later looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors\n",
    "\n",
    "So each of those tokens have embedding values - let us try and compare them with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_vecs = []\n",
    "# For each token in the sentence...\n",
    "for embedding in word_embeddings[0]:\n",
    "    cat_vec = embedding.detach().numpy()\n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs.append(cat_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method to create the vectors is to sum the last four layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Vector\n",
    "\n",
    "To get a single vector for our entire sentence we have multiple application-dependent strategies - we could just average all the tokens in our sentence. We can also use this oppurtunity to see if the second vector returned is a sentence vector too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_embedding_0 = sentence_embedding.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_embedding_1 = np.mean(token_vecs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_embedding_0), len(sentence_embedding_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the power of these vectors is how they are context dependant - our sentence had multiple uses of the word bank. Let us see the index and the word of the sentence and check the context accordingly. We'll then print the simlarity values for the similar and different meanings and see how it turns out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 after\n",
      "2 stealing\n",
      "3 money\n",
      "4 from\n",
      "5 the\n",
      "6 bank\n",
      "7 vault\n",
      "8 ,\n",
      "9 the\n",
      "10 bank\n",
      "11 robber\n",
      "12 was\n",
      "13 seen\n",
      "14 fishing\n",
      "15 on\n",
      "16 the\n",
      "17 mississippi\n",
      "18 river\n",
      "19 bank\n",
      "20 .\n",
      "21 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "    print(i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 vector values for each instance of \"bank\".\n",
      "\n",
      "bank vault    [ 0.9001056  -0.53804165 -0.16690847  0.22416186  0.6896585 ]\n",
      "bank robber   [ 0.7977126  -0.52172744 -0.1983698   0.18898535  0.59409326]\n",
      "river bank    [ 0.29608926 -0.28563383 -0.03818326  0.16736214  0.7712624 ]\n"
     ]
    }
   ],
   "source": [
    "print('First 5 vector values for each instance of \"bank\".')\n",
    "print('')\n",
    "print(\"bank vault   \", str(token_vecs[6][:5]))\n",
    "print(\"bank robber  \", str(token_vecs[10][:5]))\n",
    "print(\"river bank   \", str(token_vecs[19][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.95\n",
      "Vector similarity for *different* meanings:  0.70\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(token_vecs[10], token_vecs[19])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = 1 - cosine(token_vecs[10], token_vecs[6])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense! Let us see if the mean value of all the tokens and what we think is the sentence vector is the same thing, by checking their cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008313187398016453"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine(sentence_embedding_0, sentence_embedding_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is good - it seems it is indeed the sentence vector, so we can now write two functions which calculate the word and sentence vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(text, word_id, model, tokenizer):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    m = model(tokens_tensor)\n",
    "    word_embeddings = m.last_hidden_state\n",
    "    sentence_embeddings = m.pooler_output  \n",
    "    vector = word_embeddings[0][word_id].detach().numpy()\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_10 = word_vector(text, 6, model_embedding, tokenizer)\n",
    "word_6 = word_vector(text, 10, model_embedding, tokenizer)\n",
    "word_19 = word_vector(text, 19, model_embedding, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector(text, model, tokenizer, method=\"average\"):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    # the output cannot be sliced, need to be referenced\n",
    "    m = model(tokens_tensor)\n",
    "    word_embeddings = m.last_hidden_state\n",
    "    sentence_embeddings = m.pooler_output\n",
    "    token_vecs = []\n",
    "    \n",
    "    for embedding in word_embeddings[0]:\n",
    "        cat_vec = embedding.detach().numpy()\n",
    "        token_vecs.append(cat_vec)\n",
    "        \n",
    "    if method == \"average\":\n",
    "        sentence_embedding = np.mean(token_vecs, axis=0)\n",
    "    if method == \"model\":\n",
    "        sentence_embedding = sentence_embeddings\n",
    "    # do something\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sen_vec_0 = sentence_vector(text, model_embedding, tokenizer)\n",
    "sen_vec_1 = sentence_vector(text, model_embedding, tokenizer, method=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity metrics\n",
    "It is worth noting that word-level similarity comparisons are not appropriate with BERT embeddings because these embeddings are contextually dependent, meaning that the word vector changes depending on the sentence it appears in. This enables direct sensitivity to polysemy so that, e.g., your representation encodes river “bank” and not a financial institution “bank”. Nevertheless, it makes direct word-to-word similarity comparisons less valuable. For sentence embeddings, however, similarity comparison is still valid such that one can query, for example, a single sentence against a dataset of other sentences in order to find the most similar. Depending on the similarity metric used, the resulting similarity values will be less informative than the relative ranking of similarity outputs as some similarity metrics make assumptions about the vector space (equally-weighted dimensions, for example) that do not hold for our 768-dimensional vector space.\n",
    "\n",
    "### Using the Vectors\n",
    "\n",
    "Without fine-tuning, BERT features may be less useful than plain GloVe or word2vec.\n",
    "They start to be interesting when you fine-tune a classifier on top of BERT. \n",
    "\n",
    "### Using Transformers Pipelines\n",
    "\n",
    "The context vectors make the other pipeline functions which transformers has built in a lot more powerful. \n",
    "\n",
    "### NOTE\n",
    "The pipeline functionality in transformers is currently being worked on and might be broken, so it is an optional part of the exercise. Do try to uncomment the lines of code and try to see if it works, though! If you have managed to get transformers v2.5.1 installed, it will work - I managed to get it to work sometimes, it can be annoying to get it to work but when it works it works well.\n",
    "\n",
    "Consider the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8bdf8751864ef79a38fc036e5e3826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=629.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ac2f5670364248b7e931bb57f24663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=267844284.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f04f8baa91428abe1b0b610b545157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7410d17f31145e7aa0f1746e081f7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Allocate a pipeline for sentiment-analysis\n",
    "nlp_sentiment = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997735023498535}]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"This BERT model is so good at classifiying sentiment, I love it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a strong positive sentiment, which we'd expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.999719500541687}]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"I'm so sad that I have to spend this weekend just doing HW and readings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative label, bingo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9888eabea5472cad06e14aea8f43f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63aeb17dd864469bb5478a5691b93c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=260793700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4729a4c8e9424d8ea316942b1439d6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=213450.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e96831ecb44c41a5ee80f56a3fddbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=435797.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Allocate a pipeline for question-answering\n",
    "nlp_question = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9860534071922302,\n",
       " 'start': 34,\n",
       " 'end': 64,\n",
       " 'answer': 'analysing complex textual data'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "     'question': 'What is my favorite thing to do on weekends ?',\n",
    "     'context': 'There is nothing I like more than analysing complex textual data all weekend '\n",
    " })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also great at question-answering tasks!\n",
    "We can also extract features, as we manually did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9743d284854b79990852f7a4c7f95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=411.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d59d309682c4b7182ee709863e97b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=263273408.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0112cf3323e40fc990bedcb4823c101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=213450.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374a4390a1914397a77ccfdd70cead74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=435797.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " nlp_feature = pipeline('feature-extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = nlp_feature(\"Just sitting here exploring data all day long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.2807973325252533,\n",
       "   0.06644465029239655,\n",
       "   -0.1676395833492279,\n",
       "   -0.1689605563879013,\n",
       "   -0.22660788893699646,\n",
       "   -0.14127133786678314,\n",
       "   0.4210294187068939,\n",
       "   -0.09199244529008865,\n",
       "   -0.03190796077251434,\n",
       "   -0.8537277579307556,\n",
       "   -0.2683807611465454,\n",
       "   0.08569946885108948,\n",
       "   -0.13404320180416107,\n",
       "   -0.03496014326810837,\n",
       "   -0.5356059074401855,\n",
       "   -0.02900679036974907,\n",
       "   0.20110869407653809,\n",
       "   0.08040313422679901,\n",
       "   -0.08755529671907425,\n",
       "   -0.08443009108304977,\n",
       "   0.05363824591040611,\n",
       "   -0.18426798284053802,\n",
       "   0.5852550864219666,\n",
       "   -0.17160028219223022,\n",
       "   0.051204338669776917,\n",
       "   0.12319852411746979,\n",
       "   0.3730117976665497,\n",
       "   0.161519393324852,\n",
       "   -0.17832426726818085,\n",
       "   0.5117566585540771,\n",
       "   -0.02019650675356388,\n",
       "   0.19047726690769196,\n",
       "   0.062378112226724625,\n",
       "   0.057217277586460114,\n",
       "   -0.36198359727859497,\n",
       "   0.2284061461687088,\n",
       "   -0.18473583459854126,\n",
       "   -0.2177046537399292,\n",
       "   -0.07018111646175385,\n",
       "   -0.09863684326410294,\n",
       "   -0.5033515691757202,\n",
       "   0.11645958572626114,\n",
       "   0.5387319326400757,\n",
       "   -0.18301954865455627,\n",
       "   0.045671917498111725,\n",
       "   -0.4805179536342621,\n",
       "   0.07694834470748901,\n",
       "   0.039038389921188354,\n",
       "   -0.08155320584774017,\n",
       "   0.1363121122121811,\n",
       "   -0.0981350988149643,\n",
       "   0.09356572479009628,\n",
       "   -0.19313707947731018,\n",
       "   -0.040969591587781906,\n",
       "   0.15128131210803986,\n",
       "   0.08282726258039474,\n",
       "   -0.06292305141687393,\n",
       "   0.13535545766353607,\n",
       "   -0.5575189590454102,\n",
       "   0.26149624586105347,\n",
       "   -0.07125777006149292,\n",
       "   0.03385505452752113,\n",
       "   0.3390795886516571,\n",
       "   0.06560908257961273,\n",
       "   -0.22296348214149475,\n",
       "   0.07824648171663284,\n",
       "   0.045698538422584534,\n",
       "   0.21588370203971863,\n",
       "   -0.19988062977790833,\n",
       "   -0.2116689532995224,\n",
       "   0.09120098501443863,\n",
       "   0.227187842130661,\n",
       "   0.30304333567619324,\n",
       "   0.8904076218605042,\n",
       "   0.22981882095336914,\n",
       "   -0.21156880259513855,\n",
       "   0.33373379707336426,\n",
       "   -0.04453244432806969,\n",
       "   -0.027119604870676994,\n",
       "   -0.09593844413757324,\n",
       "   0.05773622542619705,\n",
       "   0.1991046667098999,\n",
       "   -0.11624044179916382,\n",
       "   -0.22660021483898163,\n",
       "   -0.028790052980184555,\n",
       "   -0.13013103604316711,\n",
       "   0.10612833499908447,\n",
       "   -0.11570253223180771,\n",
       "   -0.08082098513841629,\n",
       "   0.1543547809123993,\n",
       "   0.16014015674591064,\n",
       "   -0.15008307993412018,\n",
       "   -0.24606412649154663,\n",
       "   0.04691615328192711,\n",
       "   -0.20780415832996368,\n",
       "   0.10484100878238678,\n",
       "   -0.04270915314555168,\n",
       "   0.0752374678850174,\n",
       "   6.085381031036377,\n",
       "   -0.06166774034500122,\n",
       "   -0.2319580763578415,\n",
       "   0.06998210400342941,\n",
       "   0.09388382732868195,\n",
       "   -0.11683938652276993,\n",
       "   0.2404942810535431,\n",
       "   -0.32886454463005066,\n",
       "   -0.03494854271411896,\n",
       "   -0.4698139429092407,\n",
       "   0.0344100259244442,\n",
       "   0.49799203872680664,\n",
       "   0.397334486246109,\n",
       "   0.0845702737569809,\n",
       "   0.12022998183965683,\n",
       "   -0.10584181547164917,\n",
       "   -0.09611060470342636,\n",
       "   -0.32829272747039795,\n",
       "   0.09671978652477264,\n",
       "   0.04970398172736168,\n",
       "   0.07413186132907867,\n",
       "   -0.21588192880153656,\n",
       "   0.15652701258659363,\n",
       "   0.005386224016547203,\n",
       "   0.9506378769874573,\n",
       "   0.10336947441101074,\n",
       "   -0.1121620237827301,\n",
       "   -0.0011059680255129933,\n",
       "   -0.001170710427686572,\n",
       "   -0.2742651104927063,\n",
       "   0.10748100280761719,\n",
       "   -0.16263094544410706,\n",
       "   -0.4893863797187805,\n",
       "   -0.22806020081043243,\n",
       "   -0.07158298790454865,\n",
       "   -0.14923249185085297,\n",
       "   0.10891952365636826,\n",
       "   0.008096225559711456,\n",
       "   -0.042152948677539825,\n",
       "   -0.03626147657632828,\n",
       "   -0.8590203523635864,\n",
       "   0.11949529498815536,\n",
       "   0.09703489392995834,\n",
       "   -0.09249439835548401,\n",
       "   0.09470707178115845,\n",
       "   -0.1625538021326065,\n",
       "   -0.2843359708786011,\n",
       "   2.624412775039673,\n",
       "   -0.12164177000522614,\n",
       "   0.030225716531276703,\n",
       "   -0.12604978680610657,\n",
       "   0.05357104539871216,\n",
       "   -0.08468551933765411,\n",
       "   -0.18748125433921814,\n",
       "   -0.24895016849040985,\n",
       "   0.23051664233207703,\n",
       "   -0.2518126368522644,\n",
       "   -0.215270534157753,\n",
       "   0.18333996832370758,\n",
       "   0.2167528122663498,\n",
       "   0.0543196015059948,\n",
       "   0.09860707819461823,\n",
       "   -1.020213007926941,\n",
       "   -0.007301492616534233,\n",
       "   -0.22835972905158997,\n",
       "   0.23675397038459778,\n",
       "   0.13464240729808807,\n",
       "   -0.23988941311836243,\n",
       "   -0.0015056190313771367,\n",
       "   -0.6380894780158997,\n",
       "   0.023235926404595375,\n",
       "   0.3602570593357086,\n",
       "   0.04916898533701897,\n",
       "   0.03163645789027214,\n",
       "   -2.1947338581085205,\n",
       "   0.3170282542705536,\n",
       "   0.13879568874835968,\n",
       "   0.13678261637687683,\n",
       "   0.003398426342755556,\n",
       "   0.13875295221805573,\n",
       "   0.005030585452914238,\n",
       "   -0.2804129421710968,\n",
       "   -0.1710401475429535,\n",
       "   0.3840467631816864,\n",
       "   0.13680830597877502,\n",
       "   0.08550245314836502,\n",
       "   -0.4032357931137085,\n",
       "   0.01594998687505722,\n",
       "   0.1298864185810089,\n",
       "   -0.19909211993217468,\n",
       "   -0.039330512285232544,\n",
       "   0.007921217009425163,\n",
       "   -0.13287615776062012,\n",
       "   -0.33360227942466736,\n",
       "   -0.12062516063451767,\n",
       "   0.19588854908943176,\n",
       "   0.19319650530815125,\n",
       "   0.1331092119216919,\n",
       "   0.05858074128627777,\n",
       "   -0.09095698595046997,\n",
       "   -0.06136387586593628,\n",
       "   0.1706715226173401,\n",
       "   -0.18688490986824036,\n",
       "   -0.1209692731499672,\n",
       "   -0.14287233352661133,\n",
       "   0.14427243173122406,\n",
       "   0.36059844493865967,\n",
       "   0.19136768579483032,\n",
       "   -0.03786735609173775,\n",
       "   0.11734561622142792,\n",
       "   0.17427365481853485,\n",
       "   -0.06699502468109131,\n",
       "   -0.19577863812446594,\n",
       "   0.025702418759465218,\n",
       "   0.0849103257060051,\n",
       "   0.42135971784591675,\n",
       "   0.5814639329910278,\n",
       "   -0.39639267325401306,\n",
       "   -0.03314346820116043,\n",
       "   -0.08839777112007141,\n",
       "   0.09495886415243149,\n",
       "   0.046528130769729614,\n",
       "   -0.10590074211359024,\n",
       "   0.12356502562761307,\n",
       "   -0.1706322431564331,\n",
       "   -0.11698141694068909,\n",
       "   -0.23107434809207916,\n",
       "   0.055374886840581894,\n",
       "   0.2940724492073059,\n",
       "   0.2025466412305832,\n",
       "   0.011806138791143894,\n",
       "   -0.3329519033432007,\n",
       "   0.20288260281085968,\n",
       "   0.18493343889713287,\n",
       "   0.09737397730350494,\n",
       "   -0.2933533191680908,\n",
       "   -0.014712806791067123,\n",
       "   -0.03792181983590126,\n",
       "   -0.01711193099617958,\n",
       "   0.0876159742474556,\n",
       "   -0.030812004581093788,\n",
       "   -0.08504317700862885,\n",
       "   0.07453297823667526,\n",
       "   -0.0010542124509811401,\n",
       "   0.3211292028427124,\n",
       "   0.02436826564371586,\n",
       "   0.007990183308720589,\n",
       "   0.06423578411340714,\n",
       "   0.3407348096370697,\n",
       "   0.29837679862976074,\n",
       "   0.07283446192741394,\n",
       "   -0.023526042699813843,\n",
       "   0.6100216507911682,\n",
       "   0.10051991045475006,\n",
       "   -0.1800355613231659,\n",
       "   -0.2914906442165375,\n",
       "   -0.5349464416503906,\n",
       "   -0.1487496793270111,\n",
       "   -0.043226055800914764,\n",
       "   -1.1983940601348877,\n",
       "   -0.11274398863315582,\n",
       "   -0.33366066217422485,\n",
       "   0.27879253029823303,\n",
       "   -2.934847593307495,\n",
       "   0.024573786184191704,\n",
       "   0.24009476602077484,\n",
       "   -0.001540558529086411,\n",
       "   -0.10958192497491837,\n",
       "   0.2914097309112549,\n",
       "   0.09774906933307648,\n",
       "   -0.4551193118095398,\n",
       "   0.23169519007205963,\n",
       "   -0.46620795130729675,\n",
       "   0.021126896142959595,\n",
       "   -0.6199560165405273,\n",
       "   0.06250537186861038,\n",
       "   -0.12774646282196045,\n",
       "   0.25704053044319153,\n",
       "   -0.062075454741716385,\n",
       "   -0.05455482751131058,\n",
       "   -0.23015941679477692,\n",
       "   -0.21192120015621185,\n",
       "   -0.14584368467330933,\n",
       "   0.05948435887694359,\n",
       "   -0.3123905062675476,\n",
       "   0.4359772205352783,\n",
       "   -0.2780085802078247,\n",
       "   0.3652205765247345,\n",
       "   0.34398308396339417,\n",
       "   0.1992199718952179,\n",
       "   0.10600943863391876,\n",
       "   3.6819162368774414,\n",
       "   0.27654990553855896,\n",
       "   -0.15455570816993713,\n",
       "   0.0923784151673317,\n",
       "   -0.3022403419017792,\n",
       "   0.013120015151798725,\n",
       "   0.04309051111340523,\n",
       "   -0.4590860605239868,\n",
       "   0.030745303258299828,\n",
       "   -0.07204145938158035,\n",
       "   -0.08520263433456421,\n",
       "   -0.23873856663703918,\n",
       "   0.05929238721728325,\n",
       "   -0.4444524943828583,\n",
       "   -0.05480571836233139,\n",
       "   -0.4397673010826111,\n",
       "   0.045697055757045746,\n",
       "   0.03808171674609184,\n",
       "   0.10586246103048325,\n",
       "   -0.624274730682373,\n",
       "   -0.11423879861831665,\n",
       "   0.07951866090297699,\n",
       "   0.08646106719970703,\n",
       "   -0.04021656513214111,\n",
       "   0.06479848176240921,\n",
       "   -0.005205215886235237,\n",
       "   -0.28250113129615784,\n",
       "   -0.381591796875,\n",
       "   0.2738402485847473,\n",
       "   0.016155287623405457,\n",
       "   -0.7521313428878784,\n",
       "   0.2528271973133087,\n",
       "   0.17298536002635956,\n",
       "   -0.3031435012817383,\n",
       "   -0.05463682860136032,\n",
       "   0.06651034951210022,\n",
       "   -0.06294229626655579,\n",
       "   -0.04816786199808121,\n",
       "   -0.17646127939224243,\n",
       "   -0.2346605807542801,\n",
       "   -0.013834750279784203,\n",
       "   -0.16199515759944916,\n",
       "   0.0945359617471695,\n",
       "   -0.300756573677063,\n",
       "   -0.1798965334892273,\n",
       "   -0.19597958028316498,\n",
       "   -0.14192038774490356,\n",
       "   -0.10136280953884125,\n",
       "   0.26560530066490173,\n",
       "   -0.21154846251010895,\n",
       "   0.030699487775564194,\n",
       "   -0.1906120479106903,\n",
       "   0.044668104499578476,\n",
       "   -0.1634967029094696,\n",
       "   -0.008528752252459526,\n",
       "   0.2678464949131012,\n",
       "   -0.07709278911352158,\n",
       "   0.3252992630004883,\n",
       "   -0.053822655230760574,\n",
       "   0.07142581790685654,\n",
       "   0.1828598827123642,\n",
       "   -0.028946200385689735,\n",
       "   0.09669443964958191,\n",
       "   0.11392468214035034,\n",
       "   0.10665468126535416,\n",
       "   -0.0422862209379673,\n",
       "   0.05124237388372421,\n",
       "   -0.08662617951631546,\n",
       "   -0.24653717875480652,\n",
       "   -0.014914225786924362,\n",
       "   0.1490688920021057,\n",
       "   -0.06340409815311432,\n",
       "   -1.8984959125518799,\n",
       "   0.17997226119041443,\n",
       "   0.05148743465542793,\n",
       "   -0.2742823362350464,\n",
       "   -0.02263481542468071,\n",
       "   -0.12399697303771973,\n",
       "   0.014951626770198345,\n",
       "   -0.03199763968586922,\n",
       "   0.3755047619342804,\n",
       "   0.5077053308486938,\n",
       "   0.051684241741895676,\n",
       "   0.39391615986824036,\n",
       "   -0.017290517687797546,\n",
       "   -0.3436945378780365,\n",
       "   0.1256110668182373,\n",
       "   0.106472447514534,\n",
       "   0.5563493371009827,\n",
       "   0.16544225811958313,\n",
       "   -0.43806928396224976,\n",
       "   -0.2936632037162781,\n",
       "   0.13457050919532776,\n",
       "   0.4124666750431061,\n",
       "   -0.10533394664525986,\n",
       "   0.14395824074745178,\n",
       "   0.03666134923696518,\n",
       "   0.08479971438646317,\n",
       "   -0.3691878318786621,\n",
       "   0.012730136513710022,\n",
       "   -0.16313436627388,\n",
       "   0.431342750787735,\n",
       "   -0.15211854875087738,\n",
       "   0.02971789240837097,\n",
       "   -0.19789932668209076,\n",
       "   -0.11578569561243057,\n",
       "   -0.102890744805336,\n",
       "   -0.11375569552183151,\n",
       "   0.16022787988185883,\n",
       "   0.3630652129650116,\n",
       "   -0.3239903450012207,\n",
       "   -0.06578413397073746,\n",
       "   0.007671630475670099,\n",
       "   0.00181823899038136,\n",
       "   0.1339440941810608,\n",
       "   -0.1764812171459198,\n",
       "   -0.2676340639591217,\n",
       "   0.15191712975502014,\n",
       "   -0.10827606916427612,\n",
       "   -1.3623851537704468,\n",
       "   -0.07092954963445663,\n",
       "   -0.18885047733783722,\n",
       "   -0.16689437627792358,\n",
       "   -0.14233027398586273,\n",
       "   0.14373190701007843,\n",
       "   0.20127643644809723,\n",
       "   -0.05581730976700783,\n",
       "   0.034042902290821075,\n",
       "   -0.12135730683803558,\n",
       "   0.3132980167865753,\n",
       "   0.32362428307533264,\n",
       "   0.3016141653060913,\n",
       "   0.07194539904594421,\n",
       "   -0.14758718013763428,\n",
       "   -0.35291188955307007,\n",
       "   -0.1290404200553894,\n",
       "   0.22641095519065857,\n",
       "   0.12182255834341049,\n",
       "   -0.2276514321565628,\n",
       "   -0.04301606863737106,\n",
       "   0.1455872505903244,\n",
       "   -0.5860741138458252,\n",
       "   -0.1756955087184906,\n",
       "   -0.010849305428564548,\n",
       "   -0.2800425887107849,\n",
       "   -0.32879316806793213,\n",
       "   -0.19692227244377136,\n",
       "   0.04764223098754883,\n",
       "   -0.1102699264883995,\n",
       "   0.0810319036245346,\n",
       "   4.825358867645264,\n",
       "   -0.32434600591659546,\n",
       "   0.10100054740905762,\n",
       "   -0.059740714728832245,\n",
       "   -0.2943129241466522,\n",
       "   -0.12848226726055145,\n",
       "   0.17817433178424835,\n",
       "   0.03799823299050331,\n",
       "   0.7144266366958618,\n",
       "   0.060373544692993164,\n",
       "   -0.24456249177455902,\n",
       "   -0.2453724890947342,\n",
       "   0.040366917848587036,\n",
       "   0.05328484624624252,\n",
       "   -0.4414786994457245,\n",
       "   0.482692152261734,\n",
       "   0.028515417128801346,\n",
       "   -0.11976703256368637,\n",
       "   0.03353557735681534,\n",
       "   -0.12164277583360672,\n",
       "   -0.049542300403118134,\n",
       "   -0.01958044059574604,\n",
       "   0.20781533420085907,\n",
       "   -0.18230171501636505,\n",
       "   0.12229609489440918,\n",
       "   0.025663796812295914,\n",
       "   0.08802040666341782,\n",
       "   0.09810695797204971,\n",
       "   -0.17344923317432404,\n",
       "   -0.2508699893951416,\n",
       "   -0.30231139063835144,\n",
       "   0.15177945792675018,\n",
       "   0.22892190515995026,\n",
       "   0.24345561861991882,\n",
       "   0.1089436262845993,\n",
       "   -0.113632433116436,\n",
       "   0.005332373082637787,\n",
       "   0.016617154702544212,\n",
       "   0.26598060131073,\n",
       "   0.09045703709125519,\n",
       "   0.35676002502441406,\n",
       "   0.13098536431789398,\n",
       "   0.9233291149139404,\n",
       "   -0.06542517989873886,\n",
       "   0.2404167801141739,\n",
       "   0.3053032159805298,\n",
       "   -0.13723722100257874,\n",
       "   -0.03032034821808338,\n",
       "   0.3635057508945465,\n",
       "   0.1491258144378662,\n",
       "   -0.22279635071754456,\n",
       "   0.2239559292793274,\n",
       "   0.13512296974658966,\n",
       "   -0.15118080377578735,\n",
       "   0.059755753725767136,\n",
       "   0.06742772459983826,\n",
       "   0.02008097805082798,\n",
       "   -0.14436697959899902,\n",
       "   -0.13655754923820496,\n",
       "   0.057090625166893005,\n",
       "   -0.029614735394716263,\n",
       "   -0.07548591494560242,\n",
       "   -0.1239301934838295,\n",
       "   -0.17754460871219635,\n",
       "   0.18264049291610718,\n",
       "   -0.2513381540775299,\n",
       "   0.28174251317977905,\n",
       "   -0.1688460409641266,\n",
       "   -0.27026376128196716,\n",
       "   -0.1716892421245575,\n",
       "   -0.0832272544503212,\n",
       "   0.01090169046074152,\n",
       "   -0.5502950549125671,\n",
       "   0.2607719302177429,\n",
       "   0.2789183557033539,\n",
       "   -0.01403226051479578,\n",
       "   -0.3823212683200836,\n",
       "   0.1668199896812439,\n",
       "   -0.061552662402391434,\n",
       "   -0.22987566888332367,\n",
       "   -0.11973477154970169,\n",
       "   -0.1564907729625702,\n",
       "   -0.02026805654168129,\n",
       "   -0.3056457042694092,\n",
       "   0.05130106955766678,\n",
       "   0.19486723840236664,\n",
       "   -0.28823646903038025,\n",
       "   -0.3124903738498688,\n",
       "   -0.030300423502922058,\n",
       "   0.29838111996650696,\n",
       "   0.10992018133401871,\n",
       "   -0.23986901342868805,\n",
       "   -0.047199416905641556,\n",
       "   -0.021207215264439583,\n",
       "   0.02098372019827366,\n",
       "   0.3496758043766022,\n",
       "   -0.07224922627210617,\n",
       "   -0.11469590663909912,\n",
       "   -0.0013626695144921541,\n",
       "   -0.07586885988712311,\n",
       "   0.3944661021232605,\n",
       "   0.18480409681797028,\n",
       "   0.015379798598587513,\n",
       "   -0.004689262714236975,\n",
       "   0.14852625131607056,\n",
       "   -0.1958693116903305,\n",
       "   -0.11002077162265778,\n",
       "   0.2183452993631363,\n",
       "   -0.03235173225402832,\n",
       "   0.11129415780305862,\n",
       "   0.28366848826408386,\n",
       "   0.10111206024885178,\n",
       "   0.20406998693943024,\n",
       "   -0.19913946092128754,\n",
       "   -0.045815132558345795,\n",
       "   0.1286441832780838,\n",
       "   -0.009162157773971558,\n",
       "   -0.3210267424583435,\n",
       "   -7.354915618896484,\n",
       "   0.28148818016052246,\n",
       "   0.007648976519703865,\n",
       "   0.1433407962322235,\n",
       "   -0.27960601449012756,\n",
       "   -0.09527616947889328,\n",
       "   0.290213942527771,\n",
       "   0.0996512621641159,\n",
       "   0.17775657773017883,\n",
       "   0.06827160716056824,\n",
       "   -0.29400700330734253,\n",
       "   -0.13373519480228424,\n",
       "   -1.7726613283157349,\n",
       "   0.08600623160600662,\n",
       "   -0.07312257587909698,\n",
       "   0.1307162642478943,\n",
       "   -0.1263093799352646,\n",
       "   -0.849641740322113,\n",
       "   -0.012155610136687756,\n",
       "   0.34730538725852966,\n",
       "   -0.24470075964927673,\n",
       "   0.16649079322814941,\n",
       "   0.17103534936904907,\n",
       "   0.2513572871685028,\n",
       "   0.2390676736831665,\n",
       "   0.055686887353658676,\n",
       "   -0.028490232303738594,\n",
       "   -0.04394451528787613,\n",
       "   0.0737142562866211,\n",
       "   -0.08893963694572449,\n",
       "   0.024265391752123833,\n",
       "   -0.04721669480204582,\n",
       "   0.39583051204681396,\n",
       "   -0.06935456395149231,\n",
       "   0.0019108346896246076,\n",
       "   -0.2702769339084625,\n",
       "   -0.0108344042673707,\n",
       "   -0.1416129320859909,\n",
       "   0.20045176148414612,\n",
       "   -0.33274391293525696,\n",
       "   -0.09886788576841354,\n",
       "   -0.03103606402873993,\n",
       "   0.16586527228355408,\n",
       "   0.2994542717933655,\n",
       "   -0.36826080083847046,\n",
       "   -0.18471330404281616,\n",
       "   -0.4640103280544281,\n",
       "   -2.1590561866760254,\n",
       "   0.12460509687662125,\n",
       "   0.11814294755458832,\n",
       "   0.3193494379520416,\n",
       "   0.13232390582561493,\n",
       "   0.01676200143992901,\n",
       "   -0.06380075216293335,\n",
       "   -0.15379349887371063,\n",
       "   0.1839068979024887,\n",
       "   -0.009505859576165676,\n",
       "   0.097384974360466,\n",
       "   0.2518867552280426,\n",
       "   -0.2057875543832779,\n",
       "   -0.21482282876968384,\n",
       "   -0.016515547409653664,\n",
       "   0.5897840261459351,\n",
       "   0.05243944376707077,\n",
       "   -0.22500233352184296,\n",
       "   0.30145323276519775,\n",
       "   -0.30591678619384766,\n",
       "   -0.056211210787296295,\n",
       "   -0.19069890677928925,\n",
       "   -0.3809802830219269,\n",
       "   -0.12730611860752106,\n",
       "   -0.013144312426447868,\n",
       "   -0.1182652935385704,\n",
       "   0.21745498478412628,\n",
       "   0.24585261940956116,\n",
       "   0.20594313740730286,\n",
       "   -0.13156172633171082,\n",
       "   0.03846149891614914,\n",
       "   -0.217179074883461,\n",
       "   -0.019957564771175385,\n",
       "   -0.3735862076282501,\n",
       "   -0.13257239758968353,\n",
       "   0.5215295553207397,\n",
       "   0.23170995712280273,\n",
       "   0.21484817564487457,\n",
       "   -0.20708847045898438,\n",
       "   -0.03268013522028923,\n",
       "   0.29884660243988037,\n",
       "   -0.06309196352958679,\n",
       "   -0.04420733079314232,\n",
       "   -0.11364055424928665,\n",
       "   0.005607442930340767,\n",
       "   0.07338639348745346,\n",
       "   -0.07835117727518082,\n",
       "   -0.1860550343990326,\n",
       "   0.1959027647972107,\n",
       "   -0.23281018435955048,\n",
       "   0.07054179161787033,\n",
       "   -0.24805185198783875,\n",
       "   -0.08754748851060867,\n",
       "   -0.18889029324054718,\n",
       "   -7.174167694756761e-05,\n",
       "   0.043953731656074524,\n",
       "   -0.3712466359138489,\n",
       "   0.0524507574737072,\n",
       "   -0.18718110024929047,\n",
       "   2.4119415283203125,\n",
       "   0.041348837316036224,\n",
       "   -0.007798132952302694,\n",
       "   -0.055225253105163574,\n",
       "   -0.032032985240221024,\n",
       "   0.2589864134788513,\n",
       "   -0.026213156059384346,\n",
       "   0.22680601477622986,\n",
       "   1.4288944005966187,\n",
       "   0.09991711378097534,\n",
       "   -0.21881429851055145,\n",
       "   -0.034306734800338745,\n",
       "   -0.01179698295891285,\n",
       "   -0.06754491478204727,\n",
       "   -0.02779707871377468,\n",
       "   0.3310866057872772,\n",
       "   -0.09177809208631516,\n",
       "   0.13208669424057007,\n",
       "   0.06792215257883072,\n",
       "   -0.030220279470086098,\n",
       "   0.2578144669532776,\n",
       "   -0.11922546476125717,\n",
       "   0.2023175209760666,\n",
       "   -0.040757108479738235,\n",
       "   -0.04223103076219559,\n",
       "   -0.06886118650436401,\n",
       "   -0.2922442555427551,\n",
       "   -0.23770850896835327,\n",
       "   0.2981763482093811,\n",
       "   -0.3711101710796356,\n",
       "   0.139102965593338,\n",
       "   -0.0005589955835603178,\n",
       "   -0.23229849338531494,\n",
       "   0.9053872227668762,\n",
       "   -0.13803403079509735,\n",
       "   0.1583106964826584,\n",
       "   0.2712419629096985,\n",
       "   -0.13938623666763306,\n",
       "   0.08495022356510162,\n",
       "   -0.42064040899276733,\n",
       "   -0.06801237910985947,\n",
       "   -0.03080429881811142,\n",
       "   -0.44098323583602905,\n",
       "   0.4754045605659485,\n",
       "   -0.07190143316984177,\n",
       "   0.03937526419758797,\n",
       "   -0.03677168861031532,\n",
       "   -0.20400354266166687,\n",
       "   0.15076139569282532,\n",
       "   -0.08466320484876633,\n",
       "   -0.003745422698557377,\n",
       "   -0.23786517977714539,\n",
       "   -0.2641667425632477,\n",
       "   0.26233193278312683,\n",
       "   -0.2391541600227356,\n",
       "   0.07545848935842514,\n",
       "   -0.5911701321601868,\n",
       "   -0.08598645776510239,\n",
       "   0.1406150758266449,\n",
       "   -0.08961526304483414,\n",
       "   -0.04917090758681297,\n",
       "   0.011406759731471539,\n",
       "   -0.011288225650787354,\n",
       "   0.05178019404411316,\n",
       "   -0.043529022485017776,\n",
       "   0.10861764848232269,\n",
       "   0.14760959148406982,\n",
       "   -0.14580798149108887,\n",
       "   -0.11427360028028488,\n",
       "   -0.26506301760673523,\n",
       "   -0.08526569604873657,\n",
       "   0.16805730760097504,\n",
       "   -2.1287760734558105,\n",
       "   -0.12182038277387619,\n",
       "   0.1302778422832489,\n",
       "   -0.2915593087673187,\n",
       "   -0.26336443424224854,\n",
       "   -0.29721197485923767,\n",
       "   0.2892126441001892,\n",
       "   0.1707596331834793,\n",
       "   0.17035478353500366,\n",
       "   0.35742098093032837,\n",
       "   -0.0811486765742302,\n",
       "   1.8210909366607666,\n",
       "   -0.10423395037651062,\n",
       "   -0.03336145356297493,\n",
       "   -0.08374715596437454,\n",
       "   -0.017731469124555588,\n",
       "   0.1813722401857376,\n",
       "   0.16613119840621948,\n",
       "   -0.12657372653484344,\n",
       "   -0.44644489884376526,\n",
       "   -0.09487166255712509,\n",
       "   1.5146442651748657,\n",
       "   0.1415015459060669,\n",
       "   0.34425902366638184,\n",
       "   0.27152755856513977,\n",
       "   -0.20204037427902222,\n",
       "   -0.05952530726790428,\n",
       "   0.10047201067209244,\n",
       "   0.18689687550067902,\n",
       "   -0.009166537784039974,\n",
       "   -0.18844275176525116,\n",
       "   0.29771706461906433,\n",
       "   0.015029455535113811],\n",
       "  [-0.00015277678903657943,\n",
       "   0.08012718707323074,\n",
       "   -0.38847246766090393,\n",
       "   0.2897639572620392,\n",
       "   0.1003538966178894,\n",
       "   -0.11797279119491577,\n",
       "   0.26589858531951904,\n",
       "   -0.3086325228214264,\n",
       "   0.11265422403812408,\n",
       "   0.4406268000602722,\n",
       "   0.037027306854724884,\n",
       "   0.4588068723678589,\n",
       "   0.26594141125679016,\n",
       "   0.09318453073501587,\n",
       "   -0.3193682134151459,\n",
       "   -0.01104665920138359,\n",
       "   -0.11461175978183746,\n",
       "   0.06018749997019768,\n",
       "   0.030406147241592407,\n",
       "   0.03136984631419182,\n",
       "   -0.10687676072120667,\n",
       "   -0.07048772275447845,\n",
       "   0.01766248047351837,\n",
       "   0.15770231187343597,\n",
       "   -0.3637044131755829,\n",
       "   -0.06969459354877472,\n",
       "   0.41403496265411377,\n",
       "   0.016984587535262108,\n",
       "   -0.020112132653594017,\n",
       "   0.2584333121776581,\n",
       "   -0.054241083562374115,\n",
       "   0.11880519241094589,\n",
       "   0.3490239381790161,\n",
       "   0.27299171686172485,\n",
       "   -0.1218067929148674,\n",
       "   0.24036958813667297,\n",
       "   0.02343333326280117,\n",
       "   0.6490574479103088,\n",
       "   -0.12018181383609772,\n",
       "   0.05010819062590599,\n",
       "   0.0727444663643837,\n",
       "   0.09980976581573486,\n",
       "   0.026752648875117302,\n",
       "   -0.26077625155448914,\n",
       "   0.26716917753219604,\n",
       "   0.10708976536989212,\n",
       "   -0.150785893201828,\n",
       "   0.1502743661403656,\n",
       "   0.1682789921760559,\n",
       "   -0.2969588339328766,\n",
       "   0.15839609503746033,\n",
       "   -0.18971024453639984,\n",
       "   -0.1185358464717865,\n",
       "   -0.11579345166683197,\n",
       "   -0.1717553734779358,\n",
       "   -0.15409883856773376,\n",
       "   0.15222033858299255,\n",
       "   0.02688920684158802,\n",
       "   -0.36224889755249023,\n",
       "   0.6280123591423035,\n",
       "   -0.5350470542907715,\n",
       "   0.08478552103042603,\n",
       "   0.18905965983867645,\n",
       "   -0.029971150681376457,\n",
       "   -0.18735691905021667,\n",
       "   0.22901184856891632,\n",
       "   -0.07461296021938324,\n",
       "   -0.0116616515442729,\n",
       "   0.12498156726360321,\n",
       "   -0.2056175172328949,\n",
       "   0.42308667302131653,\n",
       "   -0.4493594169616699,\n",
       "   -0.23516727983951569,\n",
       "   0.014312622137367725,\n",
       "   -0.2371276319026947,\n",
       "   -0.30790185928344727,\n",
       "   0.06486811488866806,\n",
       "   -0.12396042048931122,\n",
       "   0.12174247950315475,\n",
       "   -0.2739604711532593,\n",
       "   -0.08850348740816116,\n",
       "   0.3755233585834503,\n",
       "   0.014344295486807823,\n",
       "   -0.2604106068611145,\n",
       "   0.02897527441382408,\n",
       "   0.16294527053833008,\n",
       "   0.23932917416095734,\n",
       "   0.10503631830215454,\n",
       "   -0.2757388651371002,\n",
       "   0.266903281211853,\n",
       "   -0.12328895926475525,\n",
       "   -0.22090260684490204,\n",
       "   -0.25630876421928406,\n",
       "   -0.16973038017749786,\n",
       "   -0.2010660469532013,\n",
       "   0.10432065278291702,\n",
       "   -0.18833084404468536,\n",
       "   0.1075998842716217,\n",
       "   0.21901696920394897,\n",
       "   0.2219957858324051,\n",
       "   0.018200552091002464,\n",
       "   0.13149137794971466,\n",
       "   -0.5911527276039124,\n",
       "   -0.03853911906480789,\n",
       "   -0.19770696759223938,\n",
       "   0.16444148123264313,\n",
       "   0.20242074131965637,\n",
       "   -0.16990076005458832,\n",
       "   -0.056786078959703445,\n",
       "   0.662399172782898,\n",
       "   0.2702387273311615,\n",
       "   -0.12408793717622757,\n",
       "   0.15222707390785217,\n",
       "   -0.2540279030799866,\n",
       "   -0.3162497878074646,\n",
       "   0.4545537233352661,\n",
       "   0.17869549989700317,\n",
       "   -0.00036879139952361584,\n",
       "   0.15513600409030914,\n",
       "   0.15764030814170837,\n",
       "   0.13469667732715607,\n",
       "   -0.06942041963338852,\n",
       "   0.16245611011981964,\n",
       "   -0.14433355629444122,\n",
       "   0.1598188430070877,\n",
       "   -0.07638800144195557,\n",
       "   0.13210728764533997,\n",
       "   -0.17590856552124023,\n",
       "   0.3516256809234619,\n",
       "   0.23922279477119446,\n",
       "   -0.3323219418525696,\n",
       "   0.2902434170246124,\n",
       "   0.22360773384571075,\n",
       "   -0.3945877254009247,\n",
       "   0.06739289313554764,\n",
       "   0.16413749754428864,\n",
       "   -0.10280150920152664,\n",
       "   0.40574419498443604,\n",
       "   -0.4385486841201782,\n",
       "   0.09518937766551971,\n",
       "   0.020760541781783104,\n",
       "   0.07459361851215363,\n",
       "   0.33550649881362915,\n",
       "   0.10091523826122284,\n",
       "   -0.10441383719444275,\n",
       "   0.1840469241142273,\n",
       "   -0.6618700623512268,\n",
       "   -0.3159269392490387,\n",
       "   -0.1722395271062851,\n",
       "   0.2372971624135971,\n",
       "   -0.2502628266811371,\n",
       "   -0.017625929787755013,\n",
       "   -0.0839921087026596,\n",
       "   0.10148407518863678,\n",
       "   -0.3418377935886383,\n",
       "   -0.10066715627908707,\n",
       "   -0.012530962936580181,\n",
       "   0.2818213701248169,\n",
       "   0.3661155700683594,\n",
       "   0.09728746116161346,\n",
       "   -0.2708801329135895,\n",
       "   -0.04316065087914467,\n",
       "   -0.503059983253479,\n",
       "   0.07583273202180862,\n",
       "   -0.19294418394565582,\n",
       "   0.0775647684931755,\n",
       "   -0.3759893774986267,\n",
       "   0.19966378808021545,\n",
       "   -0.31780341267585754,\n",
       "   0.250580757856369,\n",
       "   0.03841876611113548,\n",
       "   -0.04967213794589043,\n",
       "   -0.1039886325597763,\n",
       "   0.4728974997997284,\n",
       "   -0.3985272943973541,\n",
       "   -0.3259817957878113,\n",
       "   -0.11022546887397766,\n",
       "   -0.11366540193557739,\n",
       "   -0.3342837691307068,\n",
       "   -0.3177690804004669,\n",
       "   0.14207682013511658,\n",
       "   -0.11092091351747513,\n",
       "   0.13650619983673096,\n",
       "   -0.06089162826538086,\n",
       "   0.004765147343277931,\n",
       "   -0.17864692211151123,\n",
       "   0.17380592226982117,\n",
       "   -0.3304837942123413,\n",
       "   -0.161875382065773,\n",
       "   0.1387740671634674,\n",
       "   0.26607614755630493,\n",
       "   -0.23538270592689514,\n",
       "   -0.5548015832901001,\n",
       "   0.35084274411201477,\n",
       "   -0.012734206393361092,\n",
       "   -0.0018338485388085246,\n",
       "   -0.007569975685328245,\n",
       "   0.20163094997406006,\n",
       "   0.08954035490751266,\n",
       "   0.1518246978521347,\n",
       "   -0.46814098954200745,\n",
       "   0.15189293026924133,\n",
       "   -0.10995224118232727,\n",
       "   0.0850309282541275,\n",
       "   -0.033146291971206665,\n",
       "   0.038498323410749435,\n",
       "   0.4278874695301056,\n",
       "   -0.09630043804645538,\n",
       "   -0.27916306257247925,\n",
       "   -0.12436123937368393,\n",
       "   -0.2936549484729767,\n",
       "   -0.3437182605266571,\n",
       "   -0.18815191090106964,\n",
       "   0.24043641984462738,\n",
       "   -0.045665182173252106,\n",
       "   -0.20900610089302063,\n",
       "   -0.18979695439338684,\n",
       "   -0.21167300641536713,\n",
       "   -0.22652918100357056,\n",
       "   0.2818281948566437,\n",
       "   0.4957156181335449,\n",
       "   -0.14773207902908325,\n",
       "   0.1221211776137352,\n",
       "   0.0011844473192468286,\n",
       "   -0.005463038571178913,\n",
       "   -0.17044617235660553,\n",
       "   -0.051546644419431686,\n",
       "   0.23919712007045746,\n",
       "   0.10938863456249237,\n",
       "   -0.09988319128751755,\n",
       "   -0.16368809342384338,\n",
       "   0.319742888212204,\n",
       "   0.35496044158935547,\n",
       "   -0.1381356418132782,\n",
       "   0.12288526445627213,\n",
       "   0.04264026880264282,\n",
       "   0.13763143122196198,\n",
       "   -0.143959641456604,\n",
       "   -0.014802123419940472,\n",
       "   -0.21556253731250763,\n",
       "   -0.10527381300926208,\n",
       "   0.05362594500184059,\n",
       "   0.27002304792404175,\n",
       "   0.35781314969062805,\n",
       "   -0.16150999069213867,\n",
       "   -0.018870938569307327,\n",
       "   -0.08956122398376465,\n",
       "   0.2152428776025772,\n",
       "   0.2800583839416504,\n",
       "   -0.0252903513610363,\n",
       "   0.1204889714717865,\n",
       "   -0.10716887563467026,\n",
       "   -0.06003742665052414,\n",
       "   0.025192739441990852,\n",
       "   0.2860791087150574,\n",
       "   0.2313193827867508,\n",
       "   0.23700496554374695,\n",
       "   0.1806301325559616,\n",
       "   -0.08796030282974243,\n",
       "   -0.6042125821113586,\n",
       "   0.03309336677193642,\n",
       "   0.01919255405664444,\n",
       "   -0.09953449666500092,\n",
       "   0.13113076984882355,\n",
       "   -0.1855752170085907,\n",
       "   0.00671287951990962,\n",
       "   -0.11718926578760147,\n",
       "   -0.05463673174381256,\n",
       "   -0.4797157347202301,\n",
       "   -0.006999901495873928,\n",
       "   0.05780712515115738,\n",
       "   -0.15878607332706451,\n",
       "   -0.091851145029068,\n",
       "   0.4564584791660309,\n",
       "   -0.32182398438453674,\n",
       "   0.2441236972808838,\n",
       "   -0.012574588879942894,\n",
       "   -0.09229477494955063,\n",
       "   0.10339879244565964,\n",
       "   0.05599668622016907,\n",
       "   0.06368497014045715,\n",
       "   0.14838506281375885,\n",
       "   0.06993471831083298,\n",
       "   0.5977005958557129,\n",
       "   0.04449938237667084,\n",
       "   0.256723016500473,\n",
       "   0.2049681693315506,\n",
       "   0.017163122072815895,\n",
       "   -0.06512826681137085,\n",
       "   0.02914907969534397,\n",
       "   -0.033197518438100815,\n",
       "   -0.20121818780899048,\n",
       "   -0.056525349617004395,\n",
       "   0.09818069636821747,\n",
       "   -0.29535070061683655,\n",
       "   -0.1122468039393425,\n",
       "   -0.44622036814689636,\n",
       "   0.4261649549007416,\n",
       "   -0.4157567322254181,\n",
       "   -0.27478867769241333,\n",
       "   0.03416977450251579,\n",
       "   0.053460896015167236,\n",
       "   0.003058481030166149,\n",
       "   -0.08163154125213623,\n",
       "   -0.27619966864585876,\n",
       "   0.040117427706718445,\n",
       "   0.07726006209850311,\n",
       "   0.343242883682251,\n",
       "   0.26715609431266785,\n",
       "   0.2691323757171631,\n",
       "   0.3970053791999817,\n",
       "   0.08045616745948792,\n",
       "   0.12817569077014923,\n",
       "   -0.27313026785850525,\n",
       "   -0.07787743955850601,\n",
       "   -0.49394896626472473,\n",
       "   -0.26179569959640503,\n",
       "   -0.4598731994628906,\n",
       "   -0.3760283887386322,\n",
       "   -0.28620630502700806,\n",
       "   0.3783944547176361,\n",
       "   0.106338731944561,\n",
       "   -0.1531219333410263,\n",
       "   0.17784017324447632,\n",
       "   -0.05095304921269417,\n",
       "   -0.04477221146225929,\n",
       "   -0.18031495809555054,\n",
       "   0.3877369463443756,\n",
       "   -0.04976806417107582,\n",
       "   0.029177628457546234,\n",
       "   0.05523164942860603,\n",
       "   -0.07215318828821182,\n",
       "   -0.25619420409202576,\n",
       "   0.0517551451921463,\n",
       "   -0.6389034986495972,\n",
       "   -0.12020571529865265,\n",
       "   -0.421546071767807,\n",
       "   0.28700515627861023,\n",
       "   -0.01677580736577511,\n",
       "   0.4312610924243927,\n",
       "   0.12270751595497131,\n",
       "   -0.052671145647764206,\n",
       "   0.2948862612247467,\n",
       "   -0.13097381591796875,\n",
       "   0.0009662463562563062,\n",
       "   0.12286768108606339,\n",
       "   0.14946043491363525,\n",
       "   0.16690945625305176,\n",
       "   -0.056042786687612534,\n",
       "   0.32926517724990845,\n",
       "   0.016555821523070335,\n",
       "   0.2051035761833191,\n",
       "   -0.15346166491508484,\n",
       "   -0.13668034970760345,\n",
       "   -0.3815598487854004,\n",
       "   0.09475284069776535,\n",
       "   0.09593746066093445,\n",
       "   -0.09257320314645767,\n",
       "   -0.2515583038330078,\n",
       "   0.576768696308136,\n",
       "   0.18879230320453644,\n",
       "   0.22871646285057068,\n",
       "   0.09541831910610199,\n",
       "   0.2692566514015198,\n",
       "   0.09543058276176453,\n",
       "   0.26321592926979065,\n",
       "   -0.13490748405456543,\n",
       "   0.41186490654945374,\n",
       "   0.16759954392910004,\n",
       "   -0.09127165377140045,\n",
       "   0.63633131980896,\n",
       "   0.018297910690307617,\n",
       "   0.2899838089942932,\n",
       "   -0.4428274929523468,\n",
       "   -0.1846695989370346,\n",
       "   -0.17261795699596405,\n",
       "   -0.0045248898677527905,\n",
       "   0.07285390049219131,\n",
       "   0.1767006814479828,\n",
       "   -0.3789082169532776,\n",
       "   -0.08752312511205673,\n",
       "   0.0701403096318245,\n",
       "   0.6047358512878418,\n",
       "   -0.0848473310470581,\n",
       "   -0.11216235160827637,\n",
       "   -0.4263547360897064,\n",
       "   0.2407153993844986,\n",
       "   -0.21940702199935913,\n",
       "   0.13784639537334442,\n",
       "   0.10833149403333664,\n",
       "   -0.33782634139060974,\n",
       "   -0.049378402531147,\n",
       "   0.26000580191612244,\n",
       "   -0.009205709211528301,\n",
       "   -0.09147603809833527,\n",
       "   -0.5635217428207397,\n",
       "   0.10002728551626205,\n",
       "   0.3946206271648407,\n",
       "   -0.04102378338575363,\n",
       "   -0.42000502347946167,\n",
       "   -0.1276376098394394,\n",
       "   0.27446797490119934,\n",
       "   0.0782332792878151,\n",
       "   0.2829037010669708,\n",
       "   -0.2729302942752838,\n",
       "   -0.5816158056259155,\n",
       "   0.6044453382492065,\n",
       "   0.09110786020755768,\n",
       "   0.4109428822994232,\n",
       "   -0.34195613861083984,\n",
       "   0.10724379122257233,\n",
       "   -0.3546341359615326,\n",
       "   -0.09115082770586014,\n",
       "   -0.020411917939782143,\n",
       "   -0.05969966948032379,\n",
       "   -0.27264752984046936,\n",
       "   0.19546383619308472,\n",
       "   0.031518805772066116,\n",
       "   0.12798848748207092,\n",
       "   -0.0035049307625740767,\n",
       "   0.1812427043914795,\n",
       "   0.20460204780101776,\n",
       "   -0.10051033645868301,\n",
       "   -0.42367273569107056,\n",
       "   0.13038837909698486,\n",
       "   -0.1466955542564392,\n",
       "   0.20964232087135315,\n",
       "   -0.06844857335090637,\n",
       "   -0.2637392282485962,\n",
       "   0.023391760885715485,\n",
       "   -0.32233622670173645,\n",
       "   0.025356274098157883,\n",
       "   0.07607362419366837,\n",
       "   0.05684927850961685,\n",
       "   -0.5167766809463501,\n",
       "   0.3545914888381958,\n",
       "   -0.05172931030392647,\n",
       "   0.06382778286933899,\n",
       "   0.14279386401176453,\n",
       "   -0.00339004909619689,\n",
       "   0.12436970323324203,\n",
       "   -0.1528158038854599,\n",
       "   -0.1600319743156433,\n",
       "   -0.258735716342926,\n",
       "   0.07290701568126678,\n",
       "   -0.3047444820404053,\n",
       "   0.1550656110048294,\n",
       "   0.16690321266651154,\n",
       "   0.20128075778484344,\n",
       "   -0.07304373383522034,\n",
       "   0.09385000169277191,\n",
       "   -0.2209155261516571,\n",
       "   0.0654422864317894,\n",
       "   -0.40450653433799744,\n",
       "   0.3262268006801605,\n",
       "   0.28132888674736023,\n",
       "   -0.312520831823349,\n",
       "   -0.1384183019399643,\n",
       "   -0.13947243988513947,\n",
       "   -0.33873599767684937,\n",
       "   -0.07362597435712814,\n",
       "   0.1697259247303009,\n",
       "   -0.06862638145685196,\n",
       "   -0.1762857884168625,\n",
       "   -0.21169540286064148,\n",
       "   0.18761402368545532,\n",
       "   0.2377990186214447,\n",
       "   0.3058035671710968,\n",
       "   -0.5417351722717285,\n",
       "   -0.19614125788211823,\n",
       "   0.08289586007595062,\n",
       "   0.1368594765663147,\n",
       "   -0.09357739984989166,\n",
       "   0.43649110198020935,\n",
       "   -0.12487814575433731,\n",
       "   0.28953588008880615,\n",
       "   -0.022411150857806206,\n",
       "   0.00882812775671482,\n",
       "   -0.4945322871208191,\n",
       "   0.15528489649295807,\n",
       "   -0.057119082659482956,\n",
       "   0.1409750133752823,\n",
       "   -0.05202484503388405,\n",
       "   0.09517776221036911,\n",
       "   0.5504319071769714,\n",
       "   -0.4137965738773346,\n",
       "   0.3002837002277374,\n",
       "   0.12164141982793808,\n",
       "   0.5905413627624512,\n",
       "   0.019206956028938293,\n",
       "   0.1360812783241272,\n",
       "   -0.3568137586116791,\n",
       "   0.044689930975437164,\n",
       "   -0.20468363165855408,\n",
       "   0.020639263093471527,\n",
       "   -0.13248153030872345,\n",
       "   -0.3848799467086792,\n",
       "   -0.3477112352848053,\n",
       "   0.017020225524902344,\n",
       "   0.07902857661247253,\n",
       "   -0.07676685601472855,\n",
       "   -0.030579013749957085,\n",
       "   -0.1602574735879898,\n",
       "   0.04591904208064079,\n",
       "   -0.42163577675819397,\n",
       "   0.41087606549263,\n",
       "   0.0858176052570343,\n",
       "   -0.06589194387197495,\n",
       "   0.048959773033857346,\n",
       "   -0.061272427439689636,\n",
       "   0.10846813023090363,\n",
       "   -0.26583826541900635,\n",
       "   0.3738950788974762,\n",
       "   -0.0541815459728241,\n",
       "   0.23127961158752441,\n",
       "   -0.5158989429473877,\n",
       "   0.18100184202194214,\n",
       "   -0.05507655441761017,\n",
       "   -0.048769544810056686,\n",
       "   -0.13609980046749115,\n",
       "   0.20625212788581848,\n",
       "   0.29409733414649963,\n",
       "   -0.2825429141521454,\n",
       "   -0.08931492269039154,\n",
       "   0.12599197030067444,\n",
       "   0.03713361546397209,\n",
       "   -0.004584525711834431,\n",
       "   0.1924717277288437,\n",
       "   0.21793213486671448,\n",
       "   0.13368797302246094,\n",
       "   -0.2948600649833679,\n",
       "   0.021680984646081924,\n",
       "   -0.34881213307380676,\n",
       "   -0.1665637493133545,\n",
       "   0.7536072731018066,\n",
       "   -0.04590833932161331,\n",
       "   -0.10698407888412476,\n",
       "   0.044153910130262375,\n",
       "   -0.14341260492801666,\n",
       "   0.35052451491355896,\n",
       "   0.307734876871109,\n",
       "   0.0733330249786377,\n",
       "   -0.28633207082748413,\n",
       "   0.2774145305156708,\n",
       "   -0.1514964997768402,\n",
       "   0.011389678344130516,\n",
       "   0.196104034781456,\n",
       "   0.020383894443511963,\n",
       "   0.15385369956493378,\n",
       "   0.24062424898147583,\n",
       "   0.32627028226852417,\n",
       "   0.1983088254928589,\n",
       "   -0.5969808101654053,\n",
       "   0.15447551012039185,\n",
       "   0.2453775256872177,\n",
       "   -0.056402575224637985,\n",
       "   -0.10178404301404953,\n",
       "   -9.280999183654785,\n",
       "   -0.00821209978312254,\n",
       "   -0.22246064245700836,\n",
       "   0.28255707025527954,\n",
       "   -0.1357843428850174,\n",
       "   0.02255372889339924,\n",
       "   -0.05896007642149925,\n",
       "   0.08270816504955292,\n",
       "   -0.34902724623680115,\n",
       "   0.14156952500343323,\n",
       "   0.05112536996603012,\n",
       "   0.03700355067849159,\n",
       "   0.5797576308250427,\n",
       "   -0.4449831545352936,\n",
       "   -0.07033441215753555,\n",
       "   -0.2966597378253937,\n",
       "   0.09307072311639786,\n",
       "   -0.2503170967102051,\n",
       "   0.04351326823234558,\n",
       "   0.05314554646611214,\n",
       "   0.04294048249721527,\n",
       "   0.10169792920351028,\n",
       "   -0.005132030695676804,\n",
       "   0.24255284667015076,\n",
       "   0.4041635990142822,\n",
       "   0.28174304962158203,\n",
       "   -0.36682558059692383,\n",
       "   -0.3072362244129181,\n",
       "   0.39874663949012756,\n",
       "   -0.2663612961769104,\n",
       "   -0.1436215341091156,\n",
       "   0.32195061445236206,\n",
       "   -0.21467743813991547,\n",
       "   -0.012920144945383072,\n",
       "   0.5197744965553284,\n",
       "   -0.050329726189374924,\n",
       "   0.1904941350221634,\n",
       "   -0.013730823993682861,\n",
       "   0.021955417469143867,\n",
       "   -0.38185012340545654,\n",
       "   -0.49257612228393555,\n",
       "   -0.089421346783638,\n",
       "   -0.3465169668197632,\n",
       "   0.25916239619255066,\n",
       "   -0.06432106345891953,\n",
       "   0.15702340006828308,\n",
       "   0.10262425988912582,\n",
       "   0.2639559209346771,\n",
       "   -0.10086306184530258,\n",
       "   -0.0517757423222065,\n",
       "   0.12811239063739777,\n",
       "   0.15974508225917816,\n",
       "   0.36122190952301025,\n",
       "   -0.045797135680913925,\n",
       "   0.15572436153888702,\n",
       "   0.14501690864562988,\n",
       "   0.06949777156114578,\n",
       "   0.4253215193748474,\n",
       "   -0.19818896055221558,\n",
       "   -0.07091181725263596,\n",
       "   0.0812055915594101,\n",
       "   -0.30696508288383484,\n",
       "   0.5813225507736206,\n",
       "   -0.3947487473487854,\n",
       "   -0.26802459359169006,\n",
       "   0.13773825764656067,\n",
       "   0.2123439908027649,\n",
       "   0.07204285264015198,\n",
       "   0.17776158452033997,\n",
       "   -0.31464558839797974,\n",
       "   -0.22207903861999512,\n",
       "   0.3809051513671875,\n",
       "   -0.16518403589725494,\n",
       "   -0.10402192175388336,\n",
       "   -0.14159256219863892,\n",
       "   0.0006808352773077786,\n",
       "   -0.03485676646232605,\n",
       "   0.03225473314523697,\n",
       "   0.12432114779949188,\n",
       "   0.036029152572155,\n",
       "   -0.09786795824766159,\n",
       "   0.1310725212097168,\n",
       "   0.7350972890853882,\n",
       "   -0.12630526721477509,\n",
       "   -0.04537206515669823,\n",
       "   0.07449905574321747,\n",
       "   0.02834080532193184,\n",
       "   0.4409317076206207,\n",
       "   0.04460582882165909,\n",
       "   0.14423160254955292,\n",
       "   0.00786091573536396,\n",
       "   -0.0653071254491806,\n",
       "   0.08805753290653229,\n",
       "   0.15228520333766937,\n",
       "   0.1474866271018982,\n",
       "   0.3313276767730713,\n",
       "   -0.21360823512077332,\n",
       "   0.37479090690612793,\n",
       "   0.030666926875710487,\n",
       "   0.34952378273010254,\n",
       "   -0.00016968470299616456,\n",
       "   0.0522589348256588,\n",
       "   0.167293518781662,\n",
       "   -0.3643808960914612,\n",
       "   0.2641409635543823,\n",
       "   -0.16352227330207825,\n",
       "   0.20189151167869568,\n",
       "   -0.2148844450712204,\n",
       "   0.292927086353302,\n",
       "   -0.0205210093408823,\n",
       "   0.001335791894234717,\n",
       "   0.3619031310081482,\n",
       "   0.2978455126285553,\n",
       "   -0.08858594298362732,\n",
       "   0.29858434200286865,\n",
       "   0.15602822601795197,\n",
       "   -0.05518170818686485,\n",
       "   -0.10142115503549576,\n",
       "   0.45412129163742065,\n",
       "   -0.12852653861045837,\n",
       "   -0.012932575307786465,\n",
       "   -0.49733051657676697,\n",
       "   -0.1524803638458252,\n",
       "   -0.206443190574646,\n",
       "   0.3394472301006317,\n",
       "   0.13662824034690857,\n",
       "   -0.36329397559165955,\n",
       "   0.013814877718687057,\n",
       "   0.0682472437620163,\n",
       "   -0.1540287733078003,\n",
       "   0.31624284386634827,\n",
       "   0.41283485293388367,\n",
       "   -0.47701603174209595,\n",
       "   0.454755038022995,\n",
       "   0.1920890510082245,\n",
       "   -0.19714759290218353,\n",
       "   -0.181948721408844,\n",
       "   0.3014408051967621,\n",
       "   -0.6145215630531311,\n",
       "   0.20278556644916534,\n",
       "   0.01793740876019001,\n",
       "   -0.14070455729961395,\n",
       "   0.18682001531124115,\n",
       "   -0.39693042635917664,\n",
       "   0.5365515947341919,\n",
       "   -0.20806680619716644,\n",
       "   -0.35245928168296814,\n",
       "   -0.2564418911933899,\n",
       "   -0.0641777515411377,\n",
       "   -0.03768649324774742,\n",
       "   -0.09187242388725281,\n",
       "   0.47090375423431396,\n",
       "   0.4521096348762512,\n",
       "   -0.011661971919238567,\n",
       "   0.3087194859981537,\n",
       "   -0.050444308668375015,\n",
       "   0.14938952028751373,\n",
       "   -0.08899784833192825,\n",
       "   -0.0871298536658287,\n",
       "   0.3049817681312561,\n",
       "   -0.2803282141685486,\n",
       "   0.1358739286661148,\n",
       "   0.2201906442642212,\n",
       "   -0.2669582664966583,\n",
       "   -0.037611790001392365,\n",
       "   -0.3200801610946655,\n",
       "   -0.08830070495605469,\n",
       "   -0.10875895619392395,\n",
       "   0.16118484735488892,\n",
       "   -0.24312594532966614,\n",
       "   -0.2122786045074463,\n",
       "   0.14210659265518188,\n",
       "   -0.059031933546066284,\n",
       "   0.22269603610038757,\n",
       "   -0.11649787425994873,\n",
       "   0.39549145102500916,\n",
       "   -0.36930644512176514,\n",
       "   -0.2010616809129715,\n",
       "   -0.0684690922498703,\n",
       "   0.28492435812950134,\n",
       "   0.25130143761634827,\n",
       "   0.10105607658624649,\n",
       "   -0.2658402621746063,\n",
       "   -0.4231838881969452,\n",
       "   0.16404466331005096,\n",
       "   0.26750487089157104,\n",
       "   -0.10294128209352493,\n",
       "   0.2213691771030426,\n",
       "   -0.002261730609461665,\n",
       "   0.18123869597911835,\n",
       "   -0.11931999027729034,\n",
       "   0.0009457825217396021,\n",
       "   0.15420351922512054,\n",
       "   0.21422259509563446,\n",
       "   0.4453599452972412,\n",
       "   0.05038221925497055,\n",
       "   -0.02172928862273693,\n",
       "   -0.07024792581796646,\n",
       "   -0.05593550577759743,\n",
       "   -0.12208323180675507,\n",
       "   0.04233649745583534,\n",
       "   0.1664443016052246,\n",
       "   -0.020931437611579895,\n",
       "   -0.01906181499361992,\n",
       "   0.03800160065293312,\n",
       "   0.11045050621032715,\n",
       "   -0.03967335447669029,\n",
       "   0.6590478420257568,\n",
       "   0.19384030997753143,\n",
       "   0.1713157743215561,\n",
       "   -0.008039215579628944],\n",
       "  [-0.053628452122211456,\n",
       "   -0.2054617553949356,\n",
       "   -0.0830690860748291,\n",
       "   0.053809281438589096,\n",
       "   -0.007553432136774063,\n",
       "   -0.5140758752822876,\n",
       "   0.21959693729877472,\n",
       "   -0.02055160328745842,\n",
       "   0.0942189022898674,\n",
       "   0.11344608664512634,\n",
       "   0.27188828587532043,\n",
       "   0.1452200710773468,\n",
       "   -0.4942186176776886,\n",
       "   0.35814782977104187,\n",
       "   -0.5362508893013,\n",
       "   -0.1498909294605255,\n",
       "   -0.36375537514686584,\n",
       "   -0.23759396374225616,\n",
       "   -0.016823062673211098,\n",
       "   -0.018630752339959145,\n",
       "   0.08365566283464432,\n",
       "   0.047601185739040375,\n",
       "   -0.16935256123542786,\n",
       "   0.2833705246448517,\n",
       "   0.12189825624227524,\n",
       "   -0.49170470237731934,\n",
       "   0.5224190950393677,\n",
       "   0.08158306777477264,\n",
       "   0.0778520479798317,\n",
       "   0.8906058669090271,\n",
       "   0.10127491503953934,\n",
       "   -0.4020182192325592,\n",
       "   0.1718466579914093,\n",
       "   0.3042067587375641,\n",
       "   -0.3309768736362457,\n",
       "   0.4277558922767639,\n",
       "   -0.2868656814098358,\n",
       "   0.5523383021354675,\n",
       "   -0.28912046551704407,\n",
       "   0.37291038036346436,\n",
       "   -0.04067906737327576,\n",
       "   -0.05551299452781677,\n",
       "   -0.1368761956691742,\n",
       "   -0.29994189739227295,\n",
       "   0.2482590228319168,\n",
       "   0.1874229907989502,\n",
       "   0.10128322243690491,\n",
       "   -0.26125845313072205,\n",
       "   -0.32035741209983826,\n",
       "   -0.17901715636253357,\n",
       "   0.06644222140312195,\n",
       "   0.23667441308498383,\n",
       "   -0.07292050123214722,\n",
       "   0.03712818771600723,\n",
       "   0.004287541378289461,\n",
       "   -0.13084149360656738,\n",
       "   0.2671529948711395,\n",
       "   -0.20510001480579376,\n",
       "   -0.3359943926334381,\n",
       "   0.6633272767066956,\n",
       "   -0.28976041078567505,\n",
       "   -0.16201432049274445,\n",
       "   -8.557720138924196e-05,\n",
       "   0.04405521601438522,\n",
       "   0.03685908764600754,\n",
       "   -0.01735534705221653,\n",
       "   0.037494827061891556,\n",
       "   -0.25034651160240173,\n",
       "   0.23044921457767487,\n",
       "   -0.1916426420211792,\n",
       "   0.28215891122817993,\n",
       "   0.12021803110837936,\n",
       "   -0.33890965580940247,\n",
       "   -0.1547224372625351,\n",
       "   -0.2756355404853821,\n",
       "   -0.0788247138261795,\n",
       "   0.1807769387960434,\n",
       "   0.06167595833539963,\n",
       "   -0.043881606310606,\n",
       "   -0.4297832250595093,\n",
       "   -0.12242642790079117,\n",
       "   0.486751914024353,\n",
       "   -0.001787192770279944,\n",
       "   0.5329033732414246,\n",
       "   -0.20114347338676453,\n",
       "   0.19622430205345154,\n",
       "   0.1717783808708191,\n",
       "   0.6112410426139832,\n",
       "   -0.23867346346378326,\n",
       "   0.41717612743377686,\n",
       "   -0.36971744894981384,\n",
       "   0.03788125142455101,\n",
       "   0.2343595176935196,\n",
       "   -0.003829679451882839,\n",
       "   0.09341982752084732,\n",
       "   0.25496184825897217,\n",
       "   -0.34543150663375854,\n",
       "   -0.13669228553771973,\n",
       "   -0.07532760500907898,\n",
       "   0.3132837414741516,\n",
       "   0.1745225191116333,\n",
       "   0.2923644781112671,\n",
       "   -0.4504219591617584,\n",
       "   -0.03181267902255058,\n",
       "   -0.10080824792385101,\n",
       "   0.05315269157290459,\n",
       "   0.28282982110977173,\n",
       "   -0.7349063158035278,\n",
       "   -0.10437394678592682,\n",
       "   0.517967700958252,\n",
       "   0.28394782543182373,\n",
       "   0.044260550290346146,\n",
       "   0.06968232989311218,\n",
       "   -0.25437670946121216,\n",
       "   -0.25282952189445496,\n",
       "   -0.32133615016937256,\n",
       "   0.23128573596477509,\n",
       "   -0.27021875977516174,\n",
       "   -0.11190053075551987,\n",
       "   0.5673364996910095,\n",
       "   0.2181716114282608,\n",
       "   -0.6245008707046509,\n",
       "   -0.14326846599578857,\n",
       "   -0.28146234154701233,\n",
       "   0.01965477131307125,\n",
       "   0.08600940555334091,\n",
       "   -0.012269902974367142,\n",
       "   -0.314688503742218,\n",
       "   0.08279579877853394,\n",
       "   0.48005250096321106,\n",
       "   -0.2396518886089325,\n",
       "   0.3956538438796997,\n",
       "   -0.008915312588214874,\n",
       "   -0.15879026055335999,\n",
       "   0.15312863886356354,\n",
       "   0.20323233306407928,\n",
       "   -0.32763224840164185,\n",
       "   0.7224745750427246,\n",
       "   -0.5931823253631592,\n",
       "   0.21834924817085266,\n",
       "   -0.08925063163042068,\n",
       "   0.12112456560134888,\n",
       "   0.46267569065093994,\n",
       "   0.11335776001214981,\n",
       "   0.45373407006263733,\n",
       "   -0.27412745356559753,\n",
       "   -0.1726679503917694,\n",
       "   -0.3696821331977844,\n",
       "   -0.38106676936149597,\n",
       "   0.01423456147313118,\n",
       "   -0.5636183023452759,\n",
       "   -0.040171097964048386,\n",
       "   -0.23603129386901855,\n",
       "   0.3600660264492035,\n",
       "   -0.5233235359191895,\n",
       "   -0.21323147416114807,\n",
       "   0.02826845459640026,\n",
       "   0.550110399723053,\n",
       "   0.6047948598861694,\n",
       "   0.2805611491203308,\n",
       "   0.15005284547805786,\n",
       "   -0.21077589690685272,\n",
       "   -0.4876578748226166,\n",
       "   0.2879696190357208,\n",
       "   -0.09444661438465118,\n",
       "   0.1587056815624237,\n",
       "   -0.35030215978622437,\n",
       "   0.17914964258670807,\n",
       "   -0.1713651865720749,\n",
       "   0.3119801878929138,\n",
       "   0.4843764305114746,\n",
       "   0.12355875223875046,\n",
       "   0.19923952221870422,\n",
       "   0.10623817890882492,\n",
       "   -0.15028773248195648,\n",
       "   -0.4986598789691925,\n",
       "   0.21231725811958313,\n",
       "   -0.4560203552246094,\n",
       "   -0.05247613042593002,\n",
       "   0.00266622519120574,\n",
       "   0.24121502041816711,\n",
       "   -0.38238731026649475,\n",
       "   -0.3125019669532776,\n",
       "   -0.5896747708320618,\n",
       "   -0.21716316044330597,\n",
       "   -0.11549326032400131,\n",
       "   0.21698278188705444,\n",
       "   -0.26323428750038147,\n",
       "   -0.07301389425992966,\n",
       "   -0.06658989936113358,\n",
       "   -0.0639989897608757,\n",
       "   -0.09268243610858917,\n",
       "   -0.5809717178344727,\n",
       "   0.16412891447544098,\n",
       "   -0.08313966542482376,\n",
       "   0.03454255685210228,\n",
       "   -0.12032556533813477,\n",
       "   0.20796075463294983,\n",
       "   -0.028459781780838966,\n",
       "   -0.14473551511764526,\n",
       "   -0.09370698034763336,\n",
       "   -0.3057251572608948,\n",
       "   0.2168678641319275,\n",
       "   0.22809292376041412,\n",
       "   0.15786448121070862,\n",
       "   0.30604082345962524,\n",
       "   -0.2894030213356018,\n",
       "   0.2635148763656616,\n",
       "   -0.35048767924308777,\n",
       "   -0.04635355994105339,\n",
       "   0.18372462689876556,\n",
       "   -0.13899189233779907,\n",
       "   -0.3103478252887726,\n",
       "   0.22996307909488678,\n",
       "   -0.03741785138845444,\n",
       "   -0.19104570150375366,\n",
       "   -0.16201284527778625,\n",
       "   -0.0857795849442482,\n",
       "   -0.16318319737911224,\n",
       "   -0.08298186957836151,\n",
       "   -0.14884763956069946,\n",
       "   0.07608818262815475,\n",
       "   0.4617290794849396,\n",
       "   -0.26464131474494934,\n",
       "   -0.5016830563545227,\n",
       "   -0.3622983992099762,\n",
       "   0.2589702308177948,\n",
       "   0.286943256855011,\n",
       "   0.5618935227394104,\n",
       "   -0.06565484404563904,\n",
       "   -0.18244332075119019,\n",
       "   -0.2258094996213913,\n",
       "   0.3654399514198303,\n",
       "   -0.00600995309650898,\n",
       "   0.13818855583667755,\n",
       "   0.4065084457397461,\n",
       "   0.02354707568883896,\n",
       "   -0.026606179773807526,\n",
       "   -0.1449257731437683,\n",
       "   -0.8568019866943359,\n",
       "   0.1500130593776703,\n",
       "   0.17898748815059662,\n",
       "   -0.13954421877861023,\n",
       "   0.4933997392654419,\n",
       "   -0.09181052446365356,\n",
       "   -0.14665944874286652,\n",
       "   -0.23372389376163483,\n",
       "   0.32071277499198914,\n",
       "   0.38918253779411316,\n",
       "   -0.3181247413158417,\n",
       "   -0.17354287207126617,\n",
       "   -0.3614799976348877,\n",
       "   0.001278498093597591,\n",
       "   -0.4159020781517029,\n",
       "   0.5858231782913208,\n",
       "   -0.14128708839416504,\n",
       "   0.5271647572517395,\n",
       "   0.01672225445508957,\n",
       "   0.2178027182817459,\n",
       "   -0.6339844465255737,\n",
       "   0.20317168533802032,\n",
       "   -0.0716540589928627,\n",
       "   0.005372169893234968,\n",
       "   -0.04978384077548981,\n",
       "   -0.3973105251789093,\n",
       "   -0.2703627645969391,\n",
       "   0.09139532595872879,\n",
       "   0.2877703905105591,\n",
       "   -0.46449124813079834,\n",
       "   0.34780776500701904,\n",
       "   -0.05130695551633835,\n",
       "   -0.22551864385604858,\n",
       "   -0.06966548413038254,\n",
       "   0.15498097240924835,\n",
       "   -0.19721828401088715,\n",
       "   0.07669790089130402,\n",
       "   -0.06048202887177467,\n",
       "   0.0004653561336454004,\n",
       "   -0.45076265931129456,\n",
       "   0.06659824401140213,\n",
       "   0.43525174260139465,\n",
       "   0.420488566160202,\n",
       "   0.3691733181476593,\n",
       "   0.25010496377944946,\n",
       "   -0.5123942494392395,\n",
       "   0.342464417219162,\n",
       "   0.3373982310295105,\n",
       "   0.34797266125679016,\n",
       "   -0.21681323647499084,\n",
       "   -0.2651120722293854,\n",
       "   0.16333390772342682,\n",
       "   0.2580566704273224,\n",
       "   -0.7078707814216614,\n",
       "   0.1640682816505432,\n",
       "   -0.11145336925983429,\n",
       "   -0.010198106989264488,\n",
       "   -0.05493500456213951,\n",
       "   0.36400672793388367,\n",
       "   -0.34447622299194336,\n",
       "   -0.2534969449043274,\n",
       "   -0.37674254179000854,\n",
       "   0.08505510538816452,\n",
       "   0.2562459707260132,\n",
       "   0.1363915354013443,\n",
       "   -0.18084174394607544,\n",
       "   -0.8595173954963684,\n",
       "   0.17695684731006622,\n",
       "   -0.12898647785186768,\n",
       "   -0.06778351217508316,\n",
       "   0.5088979005813599,\n",
       "   0.41431114077568054,\n",
       "   0.22648942470550537,\n",
       "   0.19789592921733856,\n",
       "   -0.3591476082801819,\n",
       "   0.14089632034301758,\n",
       "   0.06850363314151764,\n",
       "   -0.38736021518707275,\n",
       "   -0.5178377628326416,\n",
       "   -0.23305082321166992,\n",
       "   -0.14532150328159332,\n",
       "   0.4971678853034973,\n",
       "   0.5020583271980286,\n",
       "   -0.5886228680610657,\n",
       "   0.19678504765033722,\n",
       "   -0.06515857577323914,\n",
       "   -0.3492593765258789,\n",
       "   0.013919663615524769,\n",
       "   0.32985737919807434,\n",
       "   0.14004021883010864,\n",
       "   -0.19393101334571838,\n",
       "   0.017172889783978462,\n",
       "   0.1824808269739151,\n",
       "   -0.06340695917606354,\n",
       "   -0.5034671425819397,\n",
       "   -0.3956148624420166,\n",
       "   -0.12905912101268768,\n",
       "   -0.2768622636795044,\n",
       "   0.24908867478370667,\n",
       "   -0.11404205113649368,\n",
       "   0.322529137134552,\n",
       "   0.18218442797660828,\n",
       "   -0.008789698593318462,\n",
       "   0.24241064488887787,\n",
       "   -0.2529589533805847,\n",
       "   0.4048522710800171,\n",
       "   -0.17898999154567719,\n",
       "   -0.051805589348077774,\n",
       "   -0.22442440688610077,\n",
       "   -0.06441402435302734,\n",
       "   0.43866515159606934,\n",
       "   0.008010785095393658,\n",
       "   0.08875028789043427,\n",
       "   -0.02350188046693802,\n",
       "   0.07542284578084946,\n",
       "   0.29027891159057617,\n",
       "   0.2501509487628937,\n",
       "   0.020081624388694763,\n",
       "   0.05902758613228798,\n",
       "   0.09603916108608246,\n",
       "   0.2134655863046646,\n",
       "   0.1496388465166092,\n",
       "   -0.14202749729156494,\n",
       "   -0.5195451974868774,\n",
       "   -0.1071552187204361,\n",
       "   0.03891285881400108,\n",
       "   0.04277023673057556,\n",
       "   0.17188319563865662,\n",
       "   -0.001573104877024889,\n",
       "   0.39097025990486145,\n",
       "   -0.23781244456768036,\n",
       "   0.7424632906913757,\n",
       "   -0.33552929759025574,\n",
       "   0.25690028071403503,\n",
       "   -0.2982504665851593,\n",
       "   -0.06990749388933182,\n",
       "   -0.07204810529947281,\n",
       "   -0.29939913749694824,\n",
       "   0.006115742027759552,\n",
       "   0.02781807631254196,\n",
       "   -0.45587408542633057,\n",
       "   -0.09735435247421265,\n",
       "   0.2109513133764267,\n",
       "   0.6865816116333008,\n",
       "   -0.18912461400032043,\n",
       "   0.23749592900276184,\n",
       "   -0.4773626923561096,\n",
       "   0.17059709131717682,\n",
       "   -0.08132605254650116,\n",
       "   0.36546728014945984,\n",
       "   0.1337440311908722,\n",
       "   0.42046284675598145,\n",
       "   -0.023231955245137215,\n",
       "   0.3555740714073181,\n",
       "   0.07926090061664581,\n",
       "   -0.26818832755088806,\n",
       "   -0.15400373935699463,\n",
       "   -0.3109884560108185,\n",
       "   0.1597646176815033,\n",
       "   0.07967297732830048,\n",
       "   -0.19272708892822266,\n",
       "   -0.37937769293785095,\n",
       "   0.19909775257110596,\n",
       "   -0.0699266567826271,\n",
       "   -0.1907435804605484,\n",
       "   -0.12638002634048462,\n",
       "   -0.54676353931427,\n",
       "   0.013998730108141899,\n",
       "   0.18523293733596802,\n",
       "   0.18227839469909668,\n",
       "   0.03600434586405754,\n",
       "   -0.08682475239038467,\n",
       "   -0.3155370056629181,\n",
       "   -0.1881665587425232,\n",
       "   0.5235685706138611,\n",
       "   0.3203895688056946,\n",
       "   -0.39599910378456116,\n",
       "   0.23779213428497314,\n",
       "   -0.12753769755363464,\n",
       "   0.10524673014879227,\n",
       "   0.03633103147149086,\n",
       "   0.0981215089559555,\n",
       "   -0.13765273988246918,\n",
       "   -0.17168353497982025,\n",
       "   -0.15374092757701874,\n",
       "   0.25866422057151794,\n",
       "   -0.16837751865386963,\n",
       "   0.08551983535289764,\n",
       "   -0.31158876419067383,\n",
       "   -0.42880409955978394,\n",
       "   0.5075845718383789,\n",
       "   0.0017555973026901484,\n",
       "   -0.3010878264904022,\n",
       "   0.009026208892464638,\n",
       "   0.27978843450546265,\n",
       "   -0.42398878931999207,\n",
       "   0.5470952391624451,\n",
       "   0.3156229257583618,\n",
       "   0.44325944781303406,\n",
       "   -0.09581679105758667,\n",
       "   0.16891729831695557,\n",
       "   0.17567504942417145,\n",
       "   -0.2544829249382019,\n",
       "   -0.049544401466846466,\n",
       "   -0.46157336235046387,\n",
       "   -0.2321949005126953,\n",
       "   -0.30620694160461426,\n",
       "   0.27848121523857117,\n",
       "   -0.3672807216644287,\n",
       "   0.28287023305892944,\n",
       "   -0.009174582548439503,\n",
       "   0.27189016342163086,\n",
       "   -0.19910217821598053,\n",
       "   -0.05488846078515053,\n",
       "   -0.39756909012794495,\n",
       "   0.37164774537086487,\n",
       "   -0.12217634171247482,\n",
       "   -0.2717757821083069,\n",
       "   -0.2425629198551178,\n",
       "   -0.14728519320487976,\n",
       "   -0.200848788022995,\n",
       "   0.07986043393611908,\n",
       "   0.24943813681602478,\n",
       "   0.33698907494544983,\n",
       "   0.47297921776771545,\n",
       "   -0.03890052065253258,\n",
       "   0.00037913021515123546,\n",
       "   0.23036210238933563,\n",
       "   0.37665218114852905,\n",
       "   0.08140773326158524,\n",
       "   -0.06075780838727951,\n",
       "   0.5382668375968933,\n",
       "   0.06421854346990585,\n",
       "   0.2021329700946808,\n",
       "   0.4138662815093994,\n",
       "   -0.1930159628391266,\n",
       "   -0.21939072012901306,\n",
       "   -0.16473542153835297,\n",
       "   0.48991918563842773,\n",
       "   -0.5034283995628357,\n",
       "   -0.07250401377677917,\n",
       "   -0.1260085105895996,\n",
       "   -0.10426277667284012,\n",
       "   -0.1348491758108139,\n",
       "   -0.1764867901802063,\n",
       "   0.5261843204498291,\n",
       "   -0.29050156474113464,\n",
       "   0.6408199667930603,\n",
       "   -0.05856504663825035,\n",
       "   0.06075034663081169,\n",
       "   0.30638548731803894,\n",
       "   -0.26711714267730713,\n",
       "   -0.2788453698158264,\n",
       "   0.24276180565357208,\n",
       "   0.11250514537096024,\n",
       "   -0.03693297877907753,\n",
       "   0.021093405783176422,\n",
       "   -0.1577620804309845,\n",
       "   -0.1424429714679718,\n",
       "   -0.3042551279067993,\n",
       "   0.12864695489406586,\n",
       "   -0.5330514311790466,\n",
       "   -0.18862153589725494,\n",
       "   0.14282672107219696,\n",
       "   0.0310868788510561,\n",
       "   -0.2284582555294037,\n",
       "   0.8762876987457275,\n",
       "   0.5793570876121521,\n",
       "   -0.432577908039093,\n",
       "   0.18803802132606506,\n",
       "   -0.21602210402488708,\n",
       "   -0.20937348902225494,\n",
       "   -0.6137093305587769,\n",
       "   -0.023942623287439346,\n",
       "   0.03767470270395279,\n",
       "   -0.1254623830318451,\n",
       "   -0.20988593995571136,\n",
       "   0.3158893585205078,\n",
       "   -0.12539814412593842,\n",
       "   -0.009359666146337986,\n",
       "   0.20354561507701874,\n",
       "   0.6587965488433838,\n",
       "   0.24253323674201965,\n",
       "   -0.21142570674419403,\n",
       "   -0.250338613986969,\n",
       "   -0.04492393508553505,\n",
       "   0.2737964391708374,\n",
       "   0.7606790661811829,\n",
       "   0.0924374982714653,\n",
       "   0.0806603729724884,\n",
       "   0.3366347551345825,\n",
       "   -0.3980686664581299,\n",
       "   0.29194024205207825,\n",
       "   -0.0041015734896063805,\n",
       "   0.07623998075723648,\n",
       "   0.8717882633209229,\n",
       "   -0.049867067486047745,\n",
       "   -0.17173418402671814,\n",
       "   0.09651828557252884,\n",
       "   -0.41367819905281067,\n",
       "   0.18127255141735077,\n",
       "   0.05077064037322998,\n",
       "   -0.09530948102474213,\n",
       "   -0.04597415402531624,\n",
       "   0.12423872947692871,\n",
       "   -0.16614402830600739,\n",
       "   0.3890908360481262,\n",
       "   0.2579618990421295,\n",
       "   0.281611829996109,\n",
       "   0.11072959005832672,\n",
       "   0.2618427872657776,\n",
       "   0.0008687299559824169,\n",
       "   -0.06953424960374832,\n",
       "   -0.7481274008750916,\n",
       "   0.15289051830768585,\n",
       "   0.2246326208114624,\n",
       "   -0.0036384633276611567,\n",
       "   -0.12603415548801422,\n",
       "   -8.999353408813477,\n",
       "   0.32170727849006653,\n",
       "   -0.04464646428823471,\n",
       "   0.1683618724346161,\n",
       "   0.30167967081069946,\n",
       "   -0.03146395459771156,\n",
       "   0.029474977403879166,\n",
       "   -0.6308783292770386,\n",
       "   -0.7517763376235962,\n",
       "   0.18763847649097443,\n",
       "   0.010063429363071918,\n",
       "   0.09255576878786087,\n",
       "   0.41599923372268677,\n",
       "   -0.2805348336696625,\n",
       "   0.161655992269516,\n",
       "   -0.26394739747047424,\n",
       "   0.22217752039432526,\n",
       "   -0.5653946995735168,\n",
       "   0.36492210626602173,\n",
       "   -0.08855661004781723,\n",
       "   0.06130916625261307,\n",
       "   0.44148778915405273,\n",
       "   -0.2206120491027832,\n",
       "   0.4304497241973877,\n",
       "   0.12455380707979202,\n",
       "   0.34147775173187256,\n",
       "   -0.01208762638270855,\n",
       "   -0.060788460075855255,\n",
       "   0.27137142419815063,\n",
       "   -0.10276006162166595,\n",
       "   -0.13614436984062195,\n",
       "   0.3643706142902374,\n",
       "   0.002964574610814452,\n",
       "   -0.23015059530735016,\n",
       "   0.4713822901248932,\n",
       "   -0.26556238532066345,\n",
       "   0.23824255168437958,\n",
       "   0.09812325984239578,\n",
       "   0.1551547646522522,\n",
       "   -0.5550958514213562,\n",
       "   -0.20755422115325928,\n",
       "   -0.04118385538458824,\n",
       "   -0.1797877550125122,\n",
       "   0.2723587155342102,\n",
       "   0.13414308428764343,\n",
       "   0.01953940838575363,\n",
       "   0.33020734786987305,\n",
       "   0.4179410934448242,\n",
       "   -0.1428811252117157,\n",
       "   -0.21366271376609802,\n",
       "   0.2619003355503082,\n",
       "   0.4650757610797882,\n",
       "   0.08701497316360474,\n",
       "   0.18946807086467743,\n",
       "   0.3481076657772064,\n",
       "   0.23744046688079834,\n",
       "   -0.048567142337560654,\n",
       "   0.6342605948448181,\n",
       "   -0.2745974361896515,\n",
       "   -0.37474435567855835,\n",
       "   0.3956849277019501,\n",
       "   -0.19444791972637177,\n",
       "   0.8582135438919067,\n",
       "   0.01686846651136875,\n",
       "   0.002172510139644146,\n",
       "   0.2970176339149475,\n",
       "   0.07636325806379318,\n",
       "   -0.08508991450071335,\n",
       "   0.2062511295080185,\n",
       "   -0.41840484738349915,\n",
       "   -0.44551917910575867,\n",
       "   -0.20133721828460693,\n",
       "   0.10678378492593765,\n",
       "   -0.5432325005531311,\n",
       "   -0.28603455424308777,\n",
       "   -0.22817867994308472,\n",
       "   -0.19513840973377228,\n",
       "   -0.007889894768595695,\n",
       "   -0.020244821906089783,\n",
       "   0.017201542854309082,\n",
       "   -0.440136581659317,\n",
       "   0.6403218507766724,\n",
       "   0.7450942993164062,\n",
       "   -0.35215163230895996,\n",
       "   -0.1323423832654953,\n",
       "   -0.22960540652275085,\n",
       "   -0.06280327588319778,\n",
       "   0.5212648510932922,\n",
       "   0.23034299910068512,\n",
       "   -0.14702124893665314,\n",
       "   -0.08800296485424042,\n",
       "   0.24004016816616058,\n",
       "   0.2587001621723175,\n",
       "   0.2455957978963852,\n",
       "   -0.21311579644680023,\n",
       "   0.4161134362220764,\n",
       "   -0.03295006975531578,\n",
       "   0.01695297472178936,\n",
       "   -0.07967513054609299,\n",
       "   0.007561699952930212,\n",
       "   -0.06319007277488708,\n",
       "   -0.14362983405590057,\n",
       "   -0.6314528584480286,\n",
       "   -0.23479823768138885,\n",
       "   -0.030016835778951645,\n",
       "   -0.13884404301643372,\n",
       "   -0.0611313059926033,\n",
       "   -0.37784042954444885,\n",
       "   0.5381004810333252,\n",
       "   0.3161104917526245,\n",
       "   -0.19397962093353271,\n",
       "   0.3004316985607147,\n",
       "   0.5870153903961182,\n",
       "   -0.01577354595065117,\n",
       "   0.27217212319374084,\n",
       "   0.3969939351081848,\n",
       "   -0.20656108856201172,\n",
       "   -0.35150623321533203,\n",
       "   -0.024801179766654968,\n",
       "   -0.07735384255647659,\n",
       "   -0.13374052941799164,\n",
       "   -0.2900846302509308,\n",
       "   -0.3205217719078064,\n",
       "   -0.26359960436820984,\n",
       "   0.2411084920167923,\n",
       "   0.10319621860980988,\n",
       "   -0.2255445122718811,\n",
       "   -0.5072118639945984,\n",
       "   0.0175018347799778,\n",
       "   -0.02851932868361473,\n",
       "   0.641429603099823,\n",
       "   0.3313780426979065,\n",
       "   -0.3362937867641449,\n",
       "   0.39463573694229126,\n",
       "   0.007589632645249367,\n",
       "   -0.052944499999284744,\n",
       "   -0.27476873993873596,\n",
       "   0.16706401109695435,\n",
       "   -0.4558303952217102,\n",
       "   0.011317403055727482,\n",
       "   -0.2533472180366516,\n",
       "   -0.16745756566524506,\n",
       "   0.8323476910591125,\n",
       "   0.06274238973855972,\n",
       "   0.6852890253067017,\n",
       "   0.038306500762701035,\n",
       "   -0.03053845837712288,\n",
       "   -0.2138509452342987,\n",
       "   -0.05297825485467911,\n",
       "   -0.12857571244239807,\n",
       "   -0.07907033711671829,\n",
       "   0.5193734169006348,\n",
       "   0.3684433400630951,\n",
       "   0.08162897825241089,\n",
       "   0.20541243255138397,\n",
       "   -0.4686744213104248,\n",
       "   3.1873074476607144e-05,\n",
       "   0.12128917872905731,\n",
       "   -0.6480854153633118,\n",
       "   0.03944782167673111,\n",
       "   0.17222587764263153,\n",
       "   -0.012855001725256443,\n",
       "   0.03631994128227234,\n",
       "   0.011874410323798656,\n",
       "   -0.33758237957954407,\n",
       "   -0.12635083496570587,\n",
       "   -0.11699503660202026,\n",
       "   0.09132533520460129,\n",
       "   0.03185679018497467,\n",
       "   -0.1854025423526764,\n",
       "   -0.17000237107276917,\n",
       "   -0.033672891557216644,\n",
       "   0.11955345422029495,\n",
       "   0.19259728491306305,\n",
       "   0.06709842383861542,\n",
       "   -0.04844577610492706,\n",
       "   0.25349950790405273,\n",
       "   -0.22531729936599731,\n",
       "   0.3690546452999115,\n",
       "   0.2651667296886444,\n",
       "   0.5691767334938049,\n",
       "   0.28862902522087097,\n",
       "   -0.15276727080345154,\n",
       "   -0.7804216146469116,\n",
       "   -0.27488091588020325,\n",
       "   0.49056100845336914,\n",
       "   0.45452311635017395,\n",
       "   0.05469173938035965,\n",
       "   -0.1303173154592514,\n",
       "   0.38475292921066284,\n",
       "   0.019470909610390663,\n",
       "   0.051459264010190964,\n",
       "   -0.03250508010387421,\n",
       "   0.2289341688156128,\n",
       "   -0.043181054294109344,\n",
       "   0.4371699392795563,\n",
       "   0.08685053139925003,\n",
       "   0.31041646003723145,\n",
       "   -0.3181527853012085,\n",
       "   -0.11609219759702682,\n",
       "   0.1902707815170288,\n",
       "   0.22529126703739166,\n",
       "   -0.022953025996685028,\n",
       "   -0.430454283952713,\n",
       "   0.25192245841026306,\n",
       "   -0.034912820905447006,\n",
       "   -0.08118077367544174,\n",
       "   0.23829412460327148,\n",
       "   0.38286837935447693,\n",
       "   -0.08544862270355225,\n",
       "   0.21378901600837708],\n",
       "  [0.280465692281723,\n",
       "   -0.19070591032505035,\n",
       "   -0.0716124027967453,\n",
       "   0.13770966231822968,\n",
       "   0.04684178903698921,\n",
       "   -0.18801844120025635,\n",
       "   0.24970108270645142,\n",
       "   -0.01801074855029583,\n",
       "   -0.4172280728816986,\n",
       "   0.2166612446308136,\n",
       "   0.24365927278995514,\n",
       "   0.19053487479686737,\n",
       "   -0.5529881119728088,\n",
       "   0.4117095172405243,\n",
       "   -0.3722993731498718,\n",
       "   0.27769607305526733,\n",
       "   0.282917320728302,\n",
       "   -0.20407475531101227,\n",
       "   0.040250346064567566,\n",
       "   0.11470445990562439,\n",
       "   0.08038924634456635,\n",
       "   0.05077841877937317,\n",
       "   -0.30934062600135803,\n",
       "   -0.004049167037010193,\n",
       "   -0.08218858391046524,\n",
       "   -0.5025857090950012,\n",
       "   0.2120143175125122,\n",
       "   0.1579916775226593,\n",
       "   0.5911693572998047,\n",
       "   0.432701051235199,\n",
       "   0.15369084477424622,\n",
       "   -0.2746497392654419,\n",
       "   0.47410890460014343,\n",
       "   0.047394927591085434,\n",
       "   -0.2523055076599121,\n",
       "   0.39995425939559937,\n",
       "   -0.08065527677536011,\n",
       "   0.4121238589286804,\n",
       "   -0.1391267031431198,\n",
       "   0.1427614986896515,\n",
       "   0.2154369205236435,\n",
       "   -0.21752892434597015,\n",
       "   -0.15143762528896332,\n",
       "   -0.5182114839553833,\n",
       "   0.2860051393508911,\n",
       "   -0.4104137122631073,\n",
       "   0.14714474976062775,\n",
       "   -0.31390702724456787,\n",
       "   -0.20179660618305206,\n",
       "   0.035711415112018585,\n",
       "   -0.005969937890768051,\n",
       "   0.37445002794265747,\n",
       "   -0.6001505255699158,\n",
       "   -0.16806018352508545,\n",
       "   -0.09315263479948044,\n",
       "   -0.32471200823783875,\n",
       "   0.15698374807834625,\n",
       "   -0.0006215611938387156,\n",
       "   -0.42996659874916077,\n",
       "   0.6649534106254578,\n",
       "   -0.38317084312438965,\n",
       "   0.41118520498275757,\n",
       "   -0.018294068053364754,\n",
       "   0.25540614128112793,\n",
       "   0.1453138142824173,\n",
       "   0.34802794456481934,\n",
       "   -0.09926345944404602,\n",
       "   0.005301275290548801,\n",
       "   0.24564111232757568,\n",
       "   -0.47221019864082336,\n",
       "   0.1255158931016922,\n",
       "   0.2110871970653534,\n",
       "   -0.3383446931838989,\n",
       "   -0.04597650095820427,\n",
       "   0.2200973927974701,\n",
       "   -0.404193639755249,\n",
       "   0.08835436403751373,\n",
       "   -0.13787730038166046,\n",
       "   0.3614383637905121,\n",
       "   -0.25795978307724,\n",
       "   0.19070202112197876,\n",
       "   0.6091405153274536,\n",
       "   -0.04646339267492294,\n",
       "   0.5479344129562378,\n",
       "   0.08290418237447739,\n",
       "   0.033863797783851624,\n",
       "   -0.05032079666852951,\n",
       "   0.3868897557258606,\n",
       "   0.13794606924057007,\n",
       "   0.3616696298122406,\n",
       "   -0.2446184605360031,\n",
       "   -0.15714022517204285,\n",
       "   0.23653192818164825,\n",
       "   -0.2154836505651474,\n",
       "   -0.21891888976097107,\n",
       "   0.3061588406562805,\n",
       "   -0.25934499502182007,\n",
       "   -0.3885425925254822,\n",
       "   -0.4661172330379486,\n",
       "   0.2384139746427536,\n",
       "   -0.05474326014518738,\n",
       "   0.08159220963716507,\n",
       "   -0.3059580624103546,\n",
       "   -0.1371728479862213,\n",
       "   -0.036437470465898514,\n",
       "   0.14796008169651031,\n",
       "   -0.20264004170894623,\n",
       "   -0.5799482464790344,\n",
       "   -0.25558751821517944,\n",
       "   0.5630894303321838,\n",
       "   0.025393765419721603,\n",
       "   0.1153123676776886,\n",
       "   -0.16514140367507935,\n",
       "   0.07766443490982056,\n",
       "   -0.022659454494714737,\n",
       "   -0.05984148755669594,\n",
       "   -0.15539290010929108,\n",
       "   -0.42498213052749634,\n",
       "   -0.35624682903289795,\n",
       "   0.20757392048835754,\n",
       "   0.17389778792858124,\n",
       "   0.14418786764144897,\n",
       "   -0.07639698684215546,\n",
       "   -0.22204671800136566,\n",
       "   0.10295388847589493,\n",
       "   -0.09547213464975357,\n",
       "   -0.19198834896087646,\n",
       "   -0.34288540482521057,\n",
       "   -0.08687952905893326,\n",
       "   0.4575645923614502,\n",
       "   -0.21950024366378784,\n",
       "   -0.1878894567489624,\n",
       "   0.06836915761232376,\n",
       "   -0.1343105137348175,\n",
       "   0.13328960537910461,\n",
       "   -0.07013251632452011,\n",
       "   -0.3154732286930084,\n",
       "   0.2685326635837555,\n",
       "   -0.9699925780296326,\n",
       "   0.11136527359485626,\n",
       "   -0.2754775285720825,\n",
       "   -0.3714469075202942,\n",
       "   0.49722906947135925,\n",
       "   0.16314570605754852,\n",
       "   0.240342378616333,\n",
       "   -0.18157824873924255,\n",
       "   -0.2793082296848297,\n",
       "   -0.18836423754692078,\n",
       "   -0.057484131306409836,\n",
       "   0.2994611859321594,\n",
       "   -0.6719813346862793,\n",
       "   -0.04477421194314957,\n",
       "   0.07126807421445847,\n",
       "   0.5851691961288452,\n",
       "   -0.24970459938049316,\n",
       "   -0.12436923384666443,\n",
       "   -0.061887677758932114,\n",
       "   0.4425716996192932,\n",
       "   0.6064180135726929,\n",
       "   0.3930950462818146,\n",
       "   0.2247316986322403,\n",
       "   -0.13751597702503204,\n",
       "   -0.3045892119407654,\n",
       "   0.007434772793203592,\n",
       "   0.019765520468354225,\n",
       "   0.043836016207933426,\n",
       "   -0.36319857835769653,\n",
       "   0.20249800384044647,\n",
       "   0.10180532932281494,\n",
       "   0.06803593784570694,\n",
       "   0.33169567584991455,\n",
       "   0.005489954724907875,\n",
       "   -0.02987620234489441,\n",
       "   0.227605402469635,\n",
       "   -0.05782071873545647,\n",
       "   -0.07238656282424927,\n",
       "   0.03959353268146515,\n",
       "   -0.11480819433927536,\n",
       "   0.13026103377342224,\n",
       "   0.11777292937040329,\n",
       "   0.1128547191619873,\n",
       "   -0.07290928065776825,\n",
       "   -0.22361597418785095,\n",
       "   -0.41470855474472046,\n",
       "   0.17712004482746124,\n",
       "   -0.002991994610056281,\n",
       "   0.4152747094631195,\n",
       "   -0.2980605661869049,\n",
       "   0.6583303809165955,\n",
       "   -0.28136587142944336,\n",
       "   -0.06928703933954239,\n",
       "   -0.13537293672561646,\n",
       "   -0.6296663284301758,\n",
       "   0.20903313159942627,\n",
       "   -0.19579990208148956,\n",
       "   -0.22447064518928528,\n",
       "   0.25079894065856934,\n",
       "   0.04155120253562927,\n",
       "   0.20382021367549896,\n",
       "   0.020759342238307,\n",
       "   -0.3248456120491028,\n",
       "   0.1260971873998642,\n",
       "   0.05276310443878174,\n",
       "   -0.057278309017419815,\n",
       "   -0.02086411789059639,\n",
       "   0.2003231793642044,\n",
       "   -0.31260108947753906,\n",
       "   0.21712695062160492,\n",
       "   -0.16525870561599731,\n",
       "   -0.29021844267845154,\n",
       "   0.37143075466156006,\n",
       "   -0.48044121265411377,\n",
       "   -0.33746498823165894,\n",
       "   -0.18354332447052002,\n",
       "   -0.24972519278526306,\n",
       "   -0.28815871477127075,\n",
       "   0.34243640303611755,\n",
       "   -0.05960444360971451,\n",
       "   -0.08653898537158966,\n",
       "   -0.02165566012263298,\n",
       "   0.028078550472855568,\n",
       "   -0.07577262818813324,\n",
       "   0.39770060777664185,\n",
       "   -0.4505546987056732,\n",
       "   -0.1306668221950531,\n",
       "   -0.22321344912052155,\n",
       "   0.15359969437122345,\n",
       "   0.24810078740119934,\n",
       "   0.2492155134677887,\n",
       "   -0.17083047330379486,\n",
       "   -0.1979752779006958,\n",
       "   -0.11478962004184723,\n",
       "   0.17906694114208221,\n",
       "   -0.2998792231082916,\n",
       "   -0.012273944914340973,\n",
       "   0.21376822888851166,\n",
       "   0.12966448068618774,\n",
       "   0.12815652787685394,\n",
       "   -0.16880552470684052,\n",
       "   -0.8358522057533264,\n",
       "   0.033420633524656296,\n",
       "   0.12065456807613373,\n",
       "   0.07747828960418701,\n",
       "   0.07218125462532043,\n",
       "   -0.06992566585540771,\n",
       "   0.0673844963312149,\n",
       "   -0.26335740089416504,\n",
       "   0.37408721446990967,\n",
       "   0.33649981021881104,\n",
       "   -0.08286446332931519,\n",
       "   0.18065769970417023,\n",
       "   -0.1677098125219345,\n",
       "   0.10610987991094589,\n",
       "   -0.279092013835907,\n",
       "   0.594528079032898,\n",
       "   -0.5345038175582886,\n",
       "   0.002979457611218095,\n",
       "   0.2213110625743866,\n",
       "   -0.0053045847453176975,\n",
       "   -0.47401848435401917,\n",
       "   0.1075628250837326,\n",
       "   0.13850966095924377,\n",
       "   0.1655108630657196,\n",
       "   0.1002398282289505,\n",
       "   -0.22550201416015625,\n",
       "   -0.2086576372385025,\n",
       "   0.09439367055892944,\n",
       "   0.351535826921463,\n",
       "   -0.36058303713798523,\n",
       "   0.6094028949737549,\n",
       "   0.20584891736507416,\n",
       "   -0.39897438883781433,\n",
       "   -0.26624810695648193,\n",
       "   0.4169726073741913,\n",
       "   -0.12624725699424744,\n",
       "   0.17250417172908783,\n",
       "   -0.045484352856874466,\n",
       "   -0.35756000876426697,\n",
       "   -0.27623450756073,\n",
       "   -0.04514990746974945,\n",
       "   0.6578980684280396,\n",
       "   0.20727446675300598,\n",
       "   -0.021165115758776665,\n",
       "   0.15300922095775604,\n",
       "   -0.4990735650062561,\n",
       "   -0.22494125366210938,\n",
       "   -0.14443087577819824,\n",
       "   0.32643088698387146,\n",
       "   -0.40263476967811584,\n",
       "   -0.3188909590244293,\n",
       "   -0.18746061623096466,\n",
       "   0.23551775515079498,\n",
       "   -0.2375941127538681,\n",
       "   -0.0864640399813652,\n",
       "   -0.1481184959411621,\n",
       "   -0.04754456877708435,\n",
       "   -0.3354329466819763,\n",
       "   0.37789568305015564,\n",
       "   -0.3946055769920349,\n",
       "   -0.12479211390018463,\n",
       "   -0.10522535443305969,\n",
       "   0.18605615198612213,\n",
       "   0.28010767698287964,\n",
       "   0.30427873134613037,\n",
       "   -0.19937458634376526,\n",
       "   -0.4731128513813019,\n",
       "   0.2818133533000946,\n",
       "   0.4218590259552002,\n",
       "   0.5846205353736877,\n",
       "   0.37713441252708435,\n",
       "   0.5932138562202454,\n",
       "   0.19461528956890106,\n",
       "   -0.3596399426460266,\n",
       "   -0.39208969473838806,\n",
       "   0.15044710040092468,\n",
       "   0.06536274403333664,\n",
       "   -0.07896742224693298,\n",
       "   -0.44647401571273804,\n",
       "   -0.3335626721382141,\n",
       "   0.3704759478569031,\n",
       "   0.4445870816707611,\n",
       "   0.2321305125951767,\n",
       "   -0.16872291266918182,\n",
       "   0.21552979946136475,\n",
       "   0.12799297273159027,\n",
       "   -0.09816665947437286,\n",
       "   0.06661947816610336,\n",
       "   0.10992515087127686,\n",
       "   -0.1855515092611313,\n",
       "   -0.04074077308177948,\n",
       "   0.046661149710416794,\n",
       "   -0.27622970938682556,\n",
       "   0.06632200628519058,\n",
       "   -0.46238091588020325,\n",
       "   -0.19453826546669006,\n",
       "   0.1255943924188614,\n",
       "   -0.3792431652545929,\n",
       "   0.5554712414741516,\n",
       "   -0.36829662322998047,\n",
       "   0.3564099669456482,\n",
       "   0.1262059360742569,\n",
       "   0.1349300593137741,\n",
       "   0.24288222193717957,\n",
       "   -0.372560054063797,\n",
       "   0.28832608461380005,\n",
       "   0.04158507660031319,\n",
       "   -0.12451424449682236,\n",
       "   0.06824339181184769,\n",
       "   0.0007359557785093784,\n",
       "   -0.038114555180072784,\n",
       "   -0.2318277508020401,\n",
       "   -0.11581242829561234,\n",
       "   -0.2038508504629135,\n",
       "   -0.13817717134952545,\n",
       "   0.2427220493555069,\n",
       "   0.33839625120162964,\n",
       "   0.2618464231491089,\n",
       "   0.05679170787334442,\n",
       "   0.2653264105319977,\n",
       "   0.421049028635025,\n",
       "   0.2510709762573242,\n",
       "   0.18245725333690643,\n",
       "   -0.24122275412082672,\n",
       "   -0.18292677402496338,\n",
       "   0.11523284763097763,\n",
       "   -0.11466903239488602,\n",
       "   0.09393589198589325,\n",
       "   0.3563162088394165,\n",
       "   -0.17196179926395416,\n",
       "   0.056330710649490356,\n",
       "   0.7249979972839355,\n",
       "   -0.3890088200569153,\n",
       "   0.5116388201713562,\n",
       "   0.006958389189094305,\n",
       "   0.07264634966850281,\n",
       "   0.11919625848531723,\n",
       "   -0.19112750887870789,\n",
       "   0.015643838793039322,\n",
       "   0.17429904639720917,\n",
       "   -0.6224801540374756,\n",
       "   0.2133440375328064,\n",
       "   0.1548263281583786,\n",
       "   0.7324671745300293,\n",
       "   -0.33075541257858276,\n",
       "   0.09260259568691254,\n",
       "   -0.06254196912050247,\n",
       "   -0.2569939196109772,\n",
       "   -0.09689098596572876,\n",
       "   0.06716366857290268,\n",
       "   0.05510401353240013,\n",
       "   0.5388463139533997,\n",
       "   -0.15824812650680542,\n",
       "   0.22277653217315674,\n",
       "   -0.3230809271335602,\n",
       "   -0.16120517253875732,\n",
       "   -0.26929858326911926,\n",
       "   -0.43106722831726074,\n",
       "   0.09347133338451385,\n",
       "   0.01527394913136959,\n",
       "   -0.3619493544101715,\n",
       "   -0.17242750525474548,\n",
       "   -0.09507401287555695,\n",
       "   -0.16932228207588196,\n",
       "   -0.13111363351345062,\n",
       "   -0.39338281750679016,\n",
       "   -0.5529777407646179,\n",
       "   -0.025292646139860153,\n",
       "   0.4064948260784149,\n",
       "   0.0903262123465538,\n",
       "   0.17174071073532104,\n",
       "   0.1395077407360077,\n",
       "   -0.34078162908554077,\n",
       "   -0.08669831603765488,\n",
       "   0.1390007883310318,\n",
       "   0.42649534344673157,\n",
       "   -0.6357006430625916,\n",
       "   0.25658807158470154,\n",
       "   -0.23800043761730194,\n",
       "   0.16183240711688995,\n",
       "   0.24388496577739716,\n",
       "   -0.03987213596701622,\n",
       "   0.5732671618461609,\n",
       "   -0.17869290709495544,\n",
       "   -0.30883899331092834,\n",
       "   0.1618814319372177,\n",
       "   0.26147037744522095,\n",
       "   0.08322948962450027,\n",
       "   -0.3634510636329651,\n",
       "   -0.4475913345813751,\n",
       "   -0.003007260849699378,\n",
       "   -0.22247016429901123,\n",
       "   0.04303232580423355,\n",
       "   0.18311002850532532,\n",
       "   0.5187604427337646,\n",
       "   -0.6175910234451294,\n",
       "   0.35946547985076904,\n",
       "   0.2451012134552002,\n",
       "   0.23091036081314087,\n",
       "   0.08362836390733719,\n",
       "   0.06908144801855087,\n",
       "   -0.13061067461967468,\n",
       "   -0.24879014492034912,\n",
       "   0.29967302083969116,\n",
       "   -0.41831719875335693,\n",
       "   -0.19879551231861115,\n",
       "   -0.07655659317970276,\n",
       "   0.3115040063858032,\n",
       "   0.05329865589737892,\n",
       "   -0.1432032585144043,\n",
       "   -0.4544605612754822,\n",
       "   0.44997358322143555,\n",
       "   -0.12363798916339874,\n",
       "   -0.11600100994110107,\n",
       "   -0.49027732014656067,\n",
       "   0.49134424328804016,\n",
       "   -0.4879087209701538,\n",
       "   -0.17198656499385834,\n",
       "   -0.020738478749990463,\n",
       "   -0.0973513200879097,\n",
       "   -0.06010102480649948,\n",
       "   -0.20680338144302368,\n",
       "   -0.16154983639717102,\n",
       "   0.32984763383865356,\n",
       "   0.37927788496017456,\n",
       "   -0.0267852321267128,\n",
       "   -0.028745949268341064,\n",
       "   0.5901308059692383,\n",
       "   0.11200918257236481,\n",
       "   -0.20354217290878296,\n",
       "   0.018518909811973572,\n",
       "   0.45638397336006165,\n",
       "   0.31963586807250977,\n",
       "   0.27324721217155457,\n",
       "   0.740415632724762,\n",
       "   0.22739003598690033,\n",
       "   -0.22645637392997742,\n",
       "   0.1136474683880806,\n",
       "   0.19661583006381989,\n",
       "   -0.5145408511161804,\n",
       "   -0.025699343532323837,\n",
       "   -0.2851206362247467,\n",
       "   -0.330100953578949,\n",
       "   -0.32183340191841125,\n",
       "   -0.3202176094055176,\n",
       "   0.35393792390823364,\n",
       "   0.06106818839907646,\n",
       "   0.8587262034416199,\n",
       "   -0.07265779376029968,\n",
       "   -0.29604238271713257,\n",
       "   0.2978823482990265,\n",
       "   0.07465625554323196,\n",
       "   0.130595400929451,\n",
       "   0.09960149228572845,\n",
       "   0.2530165910720825,\n",
       "   -0.30912765860557556,\n",
       "   -0.07268312573432922,\n",
       "   -0.0131094204261899,\n",
       "   0.026924433186650276,\n",
       "   -0.43132543563842773,\n",
       "   -0.09348911046981812,\n",
       "   -0.08733423799276352,\n",
       "   -0.1320161521434784,\n",
       "   -0.17113269865512848,\n",
       "   0.22280259430408478,\n",
       "   -0.13218152523040771,\n",
       "   0.662516176700592,\n",
       "   0.41879943013191223,\n",
       "   0.057638682425022125,\n",
       "   -0.09143880754709244,\n",
       "   -0.04850062355399132,\n",
       "   -0.04883640259504318,\n",
       "   -0.35227328538894653,\n",
       "   0.011073409579694271,\n",
       "   0.3798302412033081,\n",
       "   -0.21283943951129913,\n",
       "   -0.016248522326350212,\n",
       "   0.322691947221756,\n",
       "   0.13656805455684662,\n",
       "   0.02907014638185501,\n",
       "   0.18946842849254608,\n",
       "   0.17845799028873444,\n",
       "   0.1701868176460266,\n",
       "   -0.6106259226799011,\n",
       "   -0.5298745036125183,\n",
       "   -0.042397864162921906,\n",
       "   -0.14070023596286774,\n",
       "   0.8901750445365906,\n",
       "   0.1068003922700882,\n",
       "   -0.20574596524238586,\n",
       "   0.4974723160266876,\n",
       "   -0.0882289931178093,\n",
       "   0.04627576097846031,\n",
       "   0.3583196997642517,\n",
       "   -0.1629454493522644,\n",
       "   0.6583919525146484,\n",
       "   -0.16156253218650818,\n",
       "   -0.011414140462875366,\n",
       "   -0.1668475866317749,\n",
       "   -0.3905300498008728,\n",
       "   0.15429775416851044,\n",
       "   0.004722657147794962,\n",
       "   0.14956757426261902,\n",
       "   0.1385543793439865,\n",
       "   0.172866553068161,\n",
       "   -0.2976164221763611,\n",
       "   0.31668776273727417,\n",
       "   0.02075866609811783,\n",
       "   0.39757591485977173,\n",
       "   0.2575247883796692,\n",
       "   0.43607819080352783,\n",
       "   0.18686190247535706,\n",
       "   -0.18917430937290192,\n",
       "   -0.9979381561279297,\n",
       "   0.2724827229976654,\n",
       "   -0.2310657650232315,\n",
       "   -0.13643866777420044,\n",
       "   -0.11473610997200012,\n",
       "   -9.012744903564453,\n",
       "   0.5318706631660461,\n",
       "   -0.07489780336618423,\n",
       "   0.1264379769563675,\n",
       "   -0.31927111744880676,\n",
       "   0.15777593851089478,\n",
       "   -0.36587759852409363,\n",
       "   -0.28250062465667725,\n",
       "   -0.4359094202518463,\n",
       "   0.23797711730003357,\n",
       "   -0.09316699206829071,\n",
       "   -0.015111548826098442,\n",
       "   0.18104475736618042,\n",
       "   -0.2438148707151413,\n",
       "   0.19390176236629486,\n",
       "   -0.21703225374221802,\n",
       "   0.152400940656662,\n",
       "   -0.2233327478170395,\n",
       "   0.30029019713401794,\n",
       "   -0.2028493732213974,\n",
       "   0.32404080033302307,\n",
       "   0.44773992896080017,\n",
       "   0.1705523133277893,\n",
       "   0.37975603342056274,\n",
       "   -0.17646639049053192,\n",
       "   0.37136518955230713,\n",
       "   -0.1774769127368927,\n",
       "   0.049946319311857224,\n",
       "   0.10881519317626953,\n",
       "   -0.387549489736557,\n",
       "   -0.38509440422058105,\n",
       "   0.23167471587657928,\n",
       "   -0.09221165627241135,\n",
       "   0.09799408912658691,\n",
       "   0.22706238925457,\n",
       "   -0.13233305513858795,\n",
       "   0.05773210525512695,\n",
       "   -0.47539064288139343,\n",
       "   0.06873931735754013,\n",
       "   -0.5873450636863708,\n",
       "   -0.4858870506286621,\n",
       "   -0.3095931112766266,\n",
       "   -0.12415556609630585,\n",
       "   0.2448728382587433,\n",
       "   0.4538542330265045,\n",
       "   0.17694653570652008,\n",
       "   -0.040169425308704376,\n",
       "   0.4536779224872589,\n",
       "   0.04983227699995041,\n",
       "   0.260838121175766,\n",
       "   0.3254072964191437,\n",
       "   0.24420547485351562,\n",
       "   0.3181004822254181,\n",
       "   -0.171652689576149,\n",
       "   0.07670081406831741,\n",
       "   0.06019185855984688,\n",
       "   0.0787636786699295,\n",
       "   0.3510848879814148,\n",
       "   -0.06288009881973267,\n",
       "   -0.1771092712879181,\n",
       "   0.0865149050951004,\n",
       "   0.2034108191728592,\n",
       "   0.8572866320610046,\n",
       "   0.1027376800775528,\n",
       "   -0.27229300141334534,\n",
       "   0.585703432559967,\n",
       "   0.05783211067318916,\n",
       "   0.2381822019815445,\n",
       "   0.12721632421016693,\n",
       "   -0.38824790716171265,\n",
       "   -0.3170221447944641,\n",
       "   -0.2569200098514557,\n",
       "   0.12753327190876007,\n",
       "   -0.45262569189071655,\n",
       "   0.3498850464820862,\n",
       "   -0.6254684925079346,\n",
       "   0.08023446053266525,\n",
       "   0.08813560009002686,\n",
       "   -0.17659907042980194,\n",
       "   0.08983928710222244,\n",
       "   -0.4586465358734131,\n",
       "   0.6088524460792542,\n",
       "   0.5694793462753296,\n",
       "   -0.032406169921159744,\n",
       "   0.2102731466293335,\n",
       "   -0.19903336465358734,\n",
       "   -0.21990324556827545,\n",
       "   0.33829545974731445,\n",
       "   0.3215997815132141,\n",
       "   -0.21131375432014465,\n",
       "   -0.349469929933548,\n",
       "   0.37737807631492615,\n",
       "   -0.05590295419096947,\n",
       "   0.2643144130706787,\n",
       "   0.1250304877758026,\n",
       "   0.5956053137779236,\n",
       "   -0.15501484274864197,\n",
       "   0.11176417768001556,\n",
       "   0.0015486482298001647,\n",
       "   0.008603432215750217,\n",
       "   -0.09948674589395523,\n",
       "   0.002257279586046934,\n",
       "   -0.38198521733283997,\n",
       "   -0.26124948263168335,\n",
       "   -0.2898063063621521,\n",
       "   -0.06655313074588776,\n",
       "   -0.07927422970533371,\n",
       "   -0.24030578136444092,\n",
       "   0.24676451086997986,\n",
       "   -0.09016150236129761,\n",
       "   -0.2637746334075928,\n",
       "   0.2654889225959778,\n",
       "   0.17124930024147034,\n",
       "   -0.4030729830265045,\n",
       "   0.014071689918637276,\n",
       "   0.1771707534790039,\n",
       "   -0.11043943464756012,\n",
       "   -0.11119506508111954,\n",
       "   0.2531803846359253,\n",
       "   0.02209467813372612,\n",
       "   -0.1721775233745575,\n",
       "   -0.235124409198761,\n",
       "   -0.012981382198631763,\n",
       "   -0.2636481523513794,\n",
       "   0.1739787608385086,\n",
       "   0.22605325281620026,\n",
       "   0.16334955394268036,\n",
       "   -0.17419730126857758,\n",
       "   -0.022375434637069702,\n",
       "   0.014007247053086758,\n",
       "   0.21922621130943298,\n",
       "   0.5423416495323181,\n",
       "   -0.43577492237091064,\n",
       "   0.4426119029521942,\n",
       "   -0.12476156651973724,\n",
       "   -0.10771063715219498,\n",
       "   0.052580758929252625,\n",
       "   -0.13532206416130066,\n",
       "   -0.32154297828674316,\n",
       "   0.3903004825115204,\n",
       "   -0.936733067035675,\n",
       "   0.025651775300502777,\n",
       "   0.3990069627761841,\n",
       "   0.29873010516166687,\n",
       "   0.41469064354896545,\n",
       "   0.1380411684513092,\n",
       "   -0.27229633927345276,\n",
       "   0.20985601842403412,\n",
       "   0.12815552949905396,\n",
       "   0.12096110731363297,\n",
       "   -0.4018988013267517,\n",
       "   0.3790360689163208,\n",
       "   0.17520982027053833,\n",
       "   0.1611006110906601,\n",
       "   -0.13688015937805176,\n",
       "   -0.43790388107299805,\n",
       "   -0.008802595548331738,\n",
       "   0.16840708255767822,\n",
       "   -0.28789278864860535,\n",
       "   0.07405614852905273,\n",
       "   0.0023679202422499657,\n",
       "   0.11198633164167404,\n",
       "   0.17422370612621307,\n",
       "   -0.20434699952602386,\n",
       "   -0.2733084559440613,\n",
       "   -0.25948500633239746,\n",
       "   -0.12906919419765472,\n",
       "   -0.18747256696224213,\n",
       "   0.17341367900371552,\n",
       "   -0.05481630936264992,\n",
       "   -0.23183181881904602,\n",
       "   -0.0014099867548793554,\n",
       "   -0.18889831006526947,\n",
       "   0.3089780807495117,\n",
       "   0.07258918136358261,\n",
       "   -0.1413806527853012,\n",
       "   -0.2812403738498688,\n",
       "   -0.018570363521575928,\n",
       "   1.0896397829055786,\n",
       "   -0.10443120449781418,\n",
       "   0.21196840703487396,\n",
       "   0.2341572791337967,\n",
       "   0.12423432618379593,\n",
       "   -0.4747682213783264,\n",
       "   -0.1567743867635727,\n",
       "   0.25994008779525757,\n",
       "   0.6076135635375977,\n",
       "   0.3590201437473297,\n",
       "   -0.38546833395957947,\n",
       "   0.19434241950511932,\n",
       "   0.5166791677474976,\n",
       "   0.027860093861818314,\n",
       "   -0.26105666160583496,\n",
       "   0.43198806047439575,\n",
       "   -0.21414576470851898,\n",
       "   0.4434346556663513,\n",
       "   0.0858568325638771,\n",
       "   0.3925277888774872,\n",
       "   -0.21910831332206726,\n",
       "   -0.14944586157798767,\n",
       "   -0.06552659720182419,\n",
       "   0.2725330889225006,\n",
       "   0.21361547708511353,\n",
       "   -0.16300329566001892,\n",
       "   -0.036653123795986176,\n",
       "   0.16648592054843903,\n",
       "   -0.1487254500389099,\n",
       "   0.21673911809921265,\n",
       "   -0.03554828464984894,\n",
       "   0.3411959111690521,\n",
       "   -0.03323625028133392],\n",
       "  [0.20413948595523834,\n",
       "   -0.35685643553733826,\n",
       "   0.12449612468481064,\n",
       "   0.05401112884283066,\n",
       "   0.06376788020133972,\n",
       "   -0.24773821234703064,\n",
       "   0.5897761583328247,\n",
       "   -0.41930830478668213,\n",
       "   -0.3490210175514221,\n",
       "   0.5029516816139221,\n",
       "   -0.16014358401298523,\n",
       "   -0.06938166916370392,\n",
       "   -0.24661874771118164,\n",
       "   0.38921624422073364,\n",
       "   -0.5427656769752502,\n",
       "   0.12053600698709488,\n",
       "   -0.06470125913619995,\n",
       "   0.27720409631729126,\n",
       "   0.16415280103683472,\n",
       "   0.1580178141593933,\n",
       "   -0.5916623473167419,\n",
       "   -0.08048597723245621,\n",
       "   -0.13380831480026245,\n",
       "   0.08694998919963837,\n",
       "   0.07232560962438583,\n",
       "   0.09370202571153641,\n",
       "   -0.043332021683454514,\n",
       "   0.1970205456018448,\n",
       "   0.3243483603000641,\n",
       "   0.8598071932792664,\n",
       "   0.17782780528068542,\n",
       "   -0.013364979065954685,\n",
       "   -0.1386844515800476,\n",
       "   0.13171294331550598,\n",
       "   -0.12296118587255478,\n",
       "   0.3470032811164856,\n",
       "   -0.10825293511152267,\n",
       "   0.24402491748332977,\n",
       "   -0.2567267417907715,\n",
       "   0.3196113407611847,\n",
       "   -0.08875956386327744,\n",
       "   -0.1807582527399063,\n",
       "   0.21915344893932343,\n",
       "   -0.4836287796497345,\n",
       "   0.10042273998260498,\n",
       "   -0.2928462326526642,\n",
       "   0.4135136902332306,\n",
       "   -0.006401800084859133,\n",
       "   -0.12705862522125244,\n",
       "   0.37664294242858887,\n",
       "   -0.19644220173358917,\n",
       "   0.33143705129623413,\n",
       "   -0.27682965993881226,\n",
       "   -0.22581426799297333,\n",
       "   0.15392817556858063,\n",
       "   -0.14203982055187225,\n",
       "   0.1367611289024353,\n",
       "   -0.13211338222026825,\n",
       "   -0.2241363376379013,\n",
       "   0.31887590885162354,\n",
       "   -0.5182228684425354,\n",
       "   -0.05533113330602646,\n",
       "   0.22974234819412231,\n",
       "   0.09438223391771317,\n",
       "   -0.029836373403668404,\n",
       "   0.08631899952888489,\n",
       "   -0.12409532070159912,\n",
       "   -0.039738886058330536,\n",
       "   -0.08609981834888458,\n",
       "   0.03943747282028198,\n",
       "   -0.04575226455926895,\n",
       "   0.02687576599419117,\n",
       "   -0.44784167408943176,\n",
       "   -0.36511632800102234,\n",
       "   -0.3423396944999695,\n",
       "   -0.28576308488845825,\n",
       "   -0.45278558135032654,\n",
       "   0.1604277640581131,\n",
       "   -0.034816402941942215,\n",
       "   -0.1580209732055664,\n",
       "   -0.003913557156920433,\n",
       "   0.06728538870811462,\n",
       "   0.20450012385845184,\n",
       "   0.1425948143005371,\n",
       "   -0.02484845370054245,\n",
       "   0.15605168044567108,\n",
       "   0.020930401980876923,\n",
       "   0.30117931962013245,\n",
       "   0.09652352333068848,\n",
       "   0.4251945912837982,\n",
       "   0.08147669583559036,\n",
       "   -0.1423412561416626,\n",
       "   0.17658144235610962,\n",
       "   0.18009181320667267,\n",
       "   -0.38452595472335815,\n",
       "   -0.025153987109661102,\n",
       "   -0.11571858078241348,\n",
       "   0.030663957819342613,\n",
       "   -0.8927046656608582,\n",
       "   0.1313207447528839,\n",
       "   -0.2145007997751236,\n",
       "   0.41365769505500793,\n",
       "   -0.10435228049755096,\n",
       "   0.4074256420135498,\n",
       "   -0.21160119771957397,\n",
       "   0.34140002727508545,\n",
       "   0.21361590921878815,\n",
       "   -0.20169547200202942,\n",
       "   -0.19992534816265106,\n",
       "   0.5722947120666504,\n",
       "   -0.00842891726642847,\n",
       "   0.3151066303253174,\n",
       "   0.38656413555145264,\n",
       "   -0.07625582069158554,\n",
       "   -0.24153487384319305,\n",
       "   -0.011608856730163097,\n",
       "   -0.2475465089082718,\n",
       "   -0.1688815951347351,\n",
       "   -0.13505063951015472,\n",
       "   0.5064759850502014,\n",
       "   0.27662205696105957,\n",
       "   0.07928933948278427,\n",
       "   0.22615943849086761,\n",
       "   -0.42538732290267944,\n",
       "   0.2295917570590973,\n",
       "   0.34865233302116394,\n",
       "   -0.18475119769573212,\n",
       "   -0.025077171623706818,\n",
       "   -0.23050807416439056,\n",
       "   0.1653260439634323,\n",
       "   -0.2043347954750061,\n",
       "   0.25575950741767883,\n",
       "   0.5871421694755554,\n",
       "   0.10397371649742126,\n",
       "   0.032402608543634415,\n",
       "   0.2468976527452469,\n",
       "   -0.41678038239479065,\n",
       "   0.7314237356185913,\n",
       "   -0.25835442543029785,\n",
       "   -0.36937156319618225,\n",
       "   -0.3044552206993103,\n",
       "   0.16840815544128418,\n",
       "   0.3246763348579407,\n",
       "   0.10354002565145493,\n",
       "   0.11477277427911758,\n",
       "   -0.28440427780151367,\n",
       "   -0.39313676953315735,\n",
       "   0.011565671302378178,\n",
       "   -0.041056569665670395,\n",
       "   0.16180826723575592,\n",
       "   -0.08739468455314636,\n",
       "   0.18896223604679108,\n",
       "   -0.34952592849731445,\n",
       "   0.055217862129211426,\n",
       "   -0.17611661553382874,\n",
       "   -0.3269854784011841,\n",
       "   -0.1605120301246643,\n",
       "   0.37652337551116943,\n",
       "   0.548005998134613,\n",
       "   0.28314825892448425,\n",
       "   0.17984378337860107,\n",
       "   -0.16101005673408508,\n",
       "   -0.5059978365898132,\n",
       "   -0.23875534534454346,\n",
       "   0.251822292804718,\n",
       "   -0.034148965030908585,\n",
       "   -0.25865641236305237,\n",
       "   0.4247259795665741,\n",
       "   -0.07105746865272522,\n",
       "   0.491060346364975,\n",
       "   0.05745575204491615,\n",
       "   0.058250367641448975,\n",
       "   0.020327940583229065,\n",
       "   0.125510111451149,\n",
       "   -0.2921038866043091,\n",
       "   0.35960623621940613,\n",
       "   0.13779933750629425,\n",
       "   -0.1940128654241562,\n",
       "   0.04549352452158928,\n",
       "   0.13345544040203094,\n",
       "   -0.00953598041087389,\n",
       "   0.2117249220609665,\n",
       "   -0.34051191806793213,\n",
       "   -0.6508041024208069,\n",
       "   -0.24383209645748138,\n",
       "   0.11267994344234467,\n",
       "   0.10437867045402527,\n",
       "   -0.3260402977466583,\n",
       "   0.048887427896261215,\n",
       "   -0.42781907320022583,\n",
       "   -0.08329940587282181,\n",
       "   -0.02332485467195511,\n",
       "   -0.030785134062170982,\n",
       "   0.042940251529216766,\n",
       "   -0.4753344655036926,\n",
       "   0.21344764530658722,\n",
       "   -0.1240643709897995,\n",
       "   0.31512245535850525,\n",
       "   0.17109321057796478,\n",
       "   -0.04064949229359627,\n",
       "   0.1687086671590805,\n",
       "   -0.1999850869178772,\n",
       "   0.20386137068271637,\n",
       "   -0.0074026635847985744,\n",
       "   0.06613162159919739,\n",
       "   0.6220704913139343,\n",
       "   -0.22548900544643402,\n",
       "   0.01034070085734129,\n",
       "   -0.02764366753399372,\n",
       "   -0.3119482398033142,\n",
       "   -0.04112820327281952,\n",
       "   -0.41988590359687805,\n",
       "   -0.36290279030799866,\n",
       "   -0.02689514309167862,\n",
       "   0.04736164212226868,\n",
       "   -0.3396610617637634,\n",
       "   -0.03498241305351257,\n",
       "   0.05400910601019859,\n",
       "   -0.46790215373039246,\n",
       "   0.03892013058066368,\n",
       "   0.1682559847831726,\n",
       "   0.06522762775421143,\n",
       "   0.033154550939798355,\n",
       "   -0.023891005665063858,\n",
       "   -0.8163595199584961,\n",
       "   0.10240871459245682,\n",
       "   0.05948995426297188,\n",
       "   0.14238406717777252,\n",
       "   0.23671025037765503,\n",
       "   0.10433447360992432,\n",
       "   -0.34860512614250183,\n",
       "   0.10580987483263016,\n",
       "   0.03212370350956917,\n",
       "   -0.03559416905045509,\n",
       "   0.02276800386607647,\n",
       "   -0.2773350775241852,\n",
       "   0.030338209122419357,\n",
       "   -0.08208566904067993,\n",
       "   -0.4185676574707031,\n",
       "   -0.3435911536216736,\n",
       "   0.4628039598464966,\n",
       "   0.4831521511077881,\n",
       "   0.1133771538734436,\n",
       "   0.028936559334397316,\n",
       "   0.14542409777641296,\n",
       "   0.004311308730393648,\n",
       "   -0.3877109885215759,\n",
       "   0.2999006509780884,\n",
       "   0.3584798276424408,\n",
       "   -0.28415998816490173,\n",
       "   0.01558314636349678,\n",
       "   -0.2590486705303192,\n",
       "   -0.004991732072085142,\n",
       "   -0.28895291686058044,\n",
       "   -0.04581141471862793,\n",
       "   -0.06114630773663521,\n",
       "   0.15516093373298645,\n",
       "   0.14080455899238586,\n",
       "   0.27539604902267456,\n",
       "   -0.5157511234283447,\n",
       "   0.4342176020145416,\n",
       "   0.3770254850387573,\n",
       "   0.015593629330396652,\n",
       "   -0.1938227266073227,\n",
       "   -0.0632861778140068,\n",
       "   -0.1542852222919464,\n",
       "   -0.23986700177192688,\n",
       "   0.4275956153869629,\n",
       "   -0.41051456332206726,\n",
       "   0.2016790360212326,\n",
       "   -0.05898980051279068,\n",
       "   0.08890870213508606,\n",
       "   -0.07808168977499008,\n",
       "   0.2528347373008728,\n",
       "   0.27163034677505493,\n",
       "   0.520697832107544,\n",
       "   0.1880873292684555,\n",
       "   -0.4510648846626282,\n",
       "   0.06647680699825287,\n",
       "   -0.14869742095470428,\n",
       "   0.037369150668382645,\n",
       "   0.3471038043498993,\n",
       "   0.09061811119318008,\n",
       "   0.6289492249488831,\n",
       "   -0.5643742084503174,\n",
       "   -0.19217726588249207,\n",
       "   0.11113699525594711,\n",
       "   0.524250864982605,\n",
       "   -0.10742601752281189,\n",
       "   -0.20804771780967712,\n",
       "   -0.14202097058296204,\n",
       "   0.05132155120372772,\n",
       "   -0.20669199526309967,\n",
       "   0.2421652376651764,\n",
       "   0.503359317779541,\n",
       "   0.23848532140254974,\n",
       "   -0.08631008863449097,\n",
       "   0.3812197148799896,\n",
       "   -0.4680096507072449,\n",
       "   -0.08760669082403183,\n",
       "   0.11819571256637573,\n",
       "   0.3141050338745117,\n",
       "   -0.01590346358716488,\n",
       "   -0.09388627856969833,\n",
       "   -0.22886961698532104,\n",
       "   0.14701974391937256,\n",
       "   0.205849289894104,\n",
       "   0.2829588055610657,\n",
       "   -0.24028359353542328,\n",
       "   -0.08887583762407303,\n",
       "   0.3137507140636444,\n",
       "   0.026497432962059975,\n",
       "   -0.22250553965568542,\n",
       "   0.12060517817735672,\n",
       "   0.19975945353507996,\n",
       "   -0.18518678843975067,\n",
       "   -0.1601894348859787,\n",
       "   -0.41504791378974915,\n",
       "   0.2623220682144165,\n",
       "   0.05513108894228935,\n",
       "   0.17069998383522034,\n",
       "   0.1208127811551094,\n",
       "   -0.40144190192222595,\n",
       "   0.5467469692230225,\n",
       "   0.18703468143939972,\n",
       "   0.35632503032684326,\n",
       "   -0.02366921491920948,\n",
       "   0.10427063703536987,\n",
       "   -0.17535871267318726,\n",
       "   0.08704525977373123,\n",
       "   0.19808931648731232,\n",
       "   0.2247779816389084,\n",
       "   0.16756229102611542,\n",
       "   -0.5818102359771729,\n",
       "   -0.4519619941711426,\n",
       "   -0.30383583903312683,\n",
       "   0.08357912302017212,\n",
       "   0.6449498534202576,\n",
       "   -0.8334816694259644,\n",
       "   -0.18829897046089172,\n",
       "   0.037401217967271805,\n",
       "   0.1292022317647934,\n",
       "   -0.005194441415369511,\n",
       "   -0.4262649118900299,\n",
       "   0.12813645601272583,\n",
       "   0.21312633156776428,\n",
       "   0.11222324520349503,\n",
       "   0.07903336733579636,\n",
       "   0.18491622805595398,\n",
       "   0.24816207587718964,\n",
       "   -0.3195272982120514,\n",
       "   0.08504927158355713,\n",
       "   0.09663725644350052,\n",
       "   -0.1155182421207428,\n",
       "   0.648393988609314,\n",
       "   0.21044187247753143,\n",
       "   -0.265013188123703,\n",
       "   0.05628204718232155,\n",
       "   0.3742729127407074,\n",
       "   -0.09004149585962296,\n",
       "   -0.13114309310913086,\n",
       "   0.03401844948530197,\n",
       "   -0.001409990363754332,\n",
       "   -0.6454715728759766,\n",
       "   -0.4906444847583771,\n",
       "   0.03945159539580345,\n",
       "   -0.266396164894104,\n",
       "   0.16466139256954193,\n",
       "   -0.021851591765880585,\n",
       "   0.047285083681344986,\n",
       "   0.7948010563850403,\n",
       "   -0.1756386011838913,\n",
       "   -0.1363200545310974,\n",
       "   -0.4299537241458893,\n",
       "   -0.16805994510650635,\n",
       "   -0.09924185276031494,\n",
       "   -0.1990276575088501,\n",
       "   -0.23036949336528778,\n",
       "   0.22935210168361664,\n",
       "   -0.27252233028411865,\n",
       "   0.39323705434799194,\n",
       "   0.23371785879135132,\n",
       "   0.34290626645088196,\n",
       "   -0.32414722442626953,\n",
       "   0.19830138981342316,\n",
       "   -0.19419057667255402,\n",
       "   0.1526927351951599,\n",
       "   0.14078183472156525,\n",
       "   0.4601116478443146,\n",
       "   -0.009904308244585991,\n",
       "   0.5689190030097961,\n",
       "   0.013101974502205849,\n",
       "   0.3602549731731415,\n",
       "   0.36002781987190247,\n",
       "   0.00925901997834444,\n",
       "   0.027540573850274086,\n",
       "   -0.41947507858276367,\n",
       "   0.4250414967536926,\n",
       "   0.2421976774930954,\n",
       "   -0.3798331320285797,\n",
       "   -0.14216265082359314,\n",
       "   -0.17126940190792084,\n",
       "   0.30802327394485474,\n",
       "   -0.39227616786956787,\n",
       "   -0.45599478483200073,\n",
       "   -0.021177541464567184,\n",
       "   -0.33905574679374695,\n",
       "   0.22782814502716064,\n",
       "   0.17317533493041992,\n",
       "   -0.05403831601142883,\n",
       "   0.011919792741537094,\n",
       "   -0.2722279131412506,\n",
       "   -0.1671641618013382,\n",
       "   -0.1084904596209526,\n",
       "   0.17502085864543915,\n",
       "   -0.27260953187942505,\n",
       "   -0.1262589693069458,\n",
       "   -0.28978610038757324,\n",
       "   -0.16771328449249268,\n",
       "   0.33397990465164185,\n",
       "   0.2407514452934265,\n",
       "   -0.22731715440750122,\n",
       "   -0.32676148414611816,\n",
       "   0.06564657390117645,\n",
       "   -0.09753470867872238,\n",
       "   0.35585013031959534,\n",
       "   -0.005295454990118742,\n",
       "   -0.205065056681633,\n",
       "   -0.33144912123680115,\n",
       "   0.4007319211959839,\n",
       "   0.40134289860725403,\n",
       "   -0.1958148330450058,\n",
       "   0.05053027719259262,\n",
       "   0.08564910292625427,\n",
       "   -0.19177642464637756,\n",
       "   0.04129629209637642,\n",
       "   -0.017527548596262932,\n",
       "   0.22203458845615387,\n",
       "   0.24723492562770844,\n",
       "   -0.14993727207183838,\n",
       "   0.12263491004705429,\n",
       "   -0.35518181324005127,\n",
       "   -0.1917121559381485,\n",
       "   -0.43682122230529785,\n",
       "   -0.26596277952194214,\n",
       "   0.20161394774913788,\n",
       "   0.3064132034778595,\n",
       "   -0.25068697333335876,\n",
       "   -0.23162634670734406,\n",
       "   0.07353166490793228,\n",
       "   0.3896807134151459,\n",
       "   -0.24919958412647247,\n",
       "   0.12911075353622437,\n",
       "   -0.15384933352470398,\n",
       "   0.18101732432842255,\n",
       "   0.4239821434020996,\n",
       "   -0.30571725964546204,\n",
       "   -0.3133440315723419,\n",
       "   -0.037433523684740067,\n",
       "   -0.20662198960781097,\n",
       "   -0.3660806119441986,\n",
       "   0.36366522312164307,\n",
       "   0.2008596956729889,\n",
       "   0.11704503744840622,\n",
       "   0.005279046017676592,\n",
       "   -0.3777421712875366,\n",
       "   0.26421603560447693,\n",
       "   0.5350065231323242,\n",
       "   -0.05167805030941963,\n",
       "   0.07734860479831696,\n",
       "   0.407704621553421,\n",
       "   0.1753186583518982,\n",
       "   0.5437819957733154,\n",
       "   0.4609121084213257,\n",
       "   0.11989148706197739,\n",
       "   -0.035993628203868866,\n",
       "   -0.21057911217212677,\n",
       "   0.37365201115608215,\n",
       "   -0.6677944660186768,\n",
       "   0.16323314607143402,\n",
       "   0.0773545354604721,\n",
       "   -0.1888723522424698,\n",
       "   -0.11793272197246552,\n",
       "   -0.271484375,\n",
       "   0.19119510054588318,\n",
       "   0.12069319933652878,\n",
       "   0.2579183280467987,\n",
       "   -0.001899101072922349,\n",
       "   0.09523740410804749,\n",
       "   0.10531594604253769,\n",
       "   -0.28823554515838623,\n",
       "   -0.1267395317554474,\n",
       "   0.2178032547235489,\n",
       "   0.3007882833480835,\n",
       "   -0.14732535183429718,\n",
       "   -0.09554015845060349,\n",
       "   0.08822420239448547,\n",
       "   0.2356766015291214,\n",
       "   -0.26523536443710327,\n",
       "   -0.04557739198207855,\n",
       "   -0.2973255515098572,\n",
       "   0.02609613910317421,\n",
       "   -0.007502174470573664,\n",
       "   -0.10835958272218704,\n",
       "   -0.29483625292778015,\n",
       "   0.6132370233535767,\n",
       "   0.07140583544969559,\n",
       "   -0.2508572041988373,\n",
       "   0.07564754784107208,\n",
       "   -0.12657080590724945,\n",
       "   -0.0464228056371212,\n",
       "   -0.39509254693984985,\n",
       "   0.031644828617572784,\n",
       "   -0.467078298330307,\n",
       "   0.1060180589556694,\n",
       "   -0.21391139924526215,\n",
       "   -0.24836379289627075,\n",
       "   0.09532593190670013,\n",
       "   -0.04805530235171318,\n",
       "   0.5193485021591187,\n",
       "   0.5089396834373474,\n",
       "   -0.054517246782779694,\n",
       "   0.06415554136037827,\n",
       "   -0.16058525443077087,\n",
       "   0.6895291805267334,\n",
       "   -0.3064868152141571,\n",
       "   0.20508486032485962,\n",
       "   0.1580081582069397,\n",
       "   -0.07156456261873245,\n",
       "   -0.035947538912296295,\n",
       "   -0.06488964706659317,\n",
       "   0.12770302593708038,\n",
       "   0.14497166872024536,\n",
       "   -0.035923272371292114,\n",
       "   0.5531871914863586,\n",
       "   0.006970844231545925,\n",
       "   -0.18992431461811066,\n",
       "   -0.0578182153403759,\n",
       "   -0.7433053851127625,\n",
       "   -0.07452134042978287,\n",
       "   0.16472208499908447,\n",
       "   0.008281884714961052,\n",
       "   0.050423793494701385,\n",
       "   0.22148479521274567,\n",
       "   -0.012744608335196972,\n",
       "   -0.03967226296663284,\n",
       "   0.013196323066949844,\n",
       "   0.16274110972881317,\n",
       "   0.3791201114654541,\n",
       "   0.06910286843776703,\n",
       "   0.16025994718074799,\n",
       "   -0.00771943386644125,\n",
       "   -0.17578217387199402,\n",
       "   -0.014501087367534637,\n",
       "   -0.07659558206796646,\n",
       "   0.20302975177764893,\n",
       "   -0.5195913910865784,\n",
       "   -9.118027687072754,\n",
       "   0.28347906470298767,\n",
       "   -0.3070468306541443,\n",
       "   0.47479358315467834,\n",
       "   -0.3556433916091919,\n",
       "   0.13476480543613434,\n",
       "   0.30293160676956177,\n",
       "   -0.04679151624441147,\n",
       "   -0.16644935309886932,\n",
       "   -0.0947626605629921,\n",
       "   0.1279289722442627,\n",
       "   -0.07252562791109085,\n",
       "   -0.02597489207983017,\n",
       "   -0.006741087418049574,\n",
       "   0.17865562438964844,\n",
       "   -0.09961677342653275,\n",
       "   0.1913091242313385,\n",
       "   -0.13887132704257965,\n",
       "   -0.2716745436191559,\n",
       "   -0.009677428752183914,\n",
       "   -0.045785073190927505,\n",
       "   -0.3350384831428528,\n",
       "   -0.2070983350276947,\n",
       "   0.8308444619178772,\n",
       "   0.030986472964286804,\n",
       "   0.28723597526550293,\n",
       "   0.07659127563238144,\n",
       "   -0.2223944514989853,\n",
       "   -0.11558019369840622,\n",
       "   -0.07059017568826675,\n",
       "   -0.052358318120241165,\n",
       "   0.15311293303966522,\n",
       "   -0.14616058766841888,\n",
       "   0.23150649666786194,\n",
       "   0.06975048780441284,\n",
       "   -0.16529947519302368,\n",
       "   -0.09772976487874985,\n",
       "   0.17975692451000214,\n",
       "   -0.01980660669505596,\n",
       "   -0.30826127529144287,\n",
       "   0.20657852292060852,\n",
       "   0.03182322159409523,\n",
       "   -0.07340395450592041,\n",
       "   0.44814857840538025,\n",
       "   -0.06679191440343857,\n",
       "   0.05887163430452347,\n",
       "   0.09893371164798737,\n",
       "   -0.10483032464981079,\n",
       "   0.014865073375403881,\n",
       "   0.03466986119747162,\n",
       "   0.5009470582008362,\n",
       "   0.22170518338680267,\n",
       "   0.6296209692955017,\n",
       "   0.35476407408714294,\n",
       "   0.10546638816595078,\n",
       "   0.20796848833560944,\n",
       "   0.16451609134674072,\n",
       "   0.43887361884117126,\n",
       "   -0.275789737701416,\n",
       "   -0.6501743793487549,\n",
       "   0.031138572841882706,\n",
       "   -0.010247997008264065,\n",
       "   0.3951869606971741,\n",
       "   0.050852492451667786,\n",
       "   -0.2547386884689331,\n",
       "   0.22206765413284302,\n",
       "   -0.3658708930015564,\n",
       "   -0.07988772541284561,\n",
       "   -0.0071699651889503,\n",
       "   -0.255604088306427,\n",
       "   -0.5023273229598999,\n",
       "   -0.3941130042076111,\n",
       "   0.2662331163883209,\n",
       "   -0.2424878478050232,\n",
       "   0.0784069299697876,\n",
       "   -0.11499220132827759,\n",
       "   -0.04550953581929207,\n",
       "   0.2730453908443451,\n",
       "   0.11502443253993988,\n",
       "   0.33978769183158875,\n",
       "   -0.6479334235191345,\n",
       "   0.22305560111999512,\n",
       "   0.43311724066734314,\n",
       "   0.03172324225306511,\n",
       "   0.33025744557380676,\n",
       "   -0.20069445669651031,\n",
       "   -0.22776280343532562,\n",
       "   0.030992846935987473,\n",
       "   -0.05551476031541824,\n",
       "   0.3387663662433624,\n",
       "   -0.3440079689025879,\n",
       "   0.18899232149124146,\n",
       "   -0.05618715286254883,\n",
       "   0.22342583537101746,\n",
       "   -0.3041532337665558,\n",
       "   0.3847416043281555,\n",
       "   -0.0940118357539177,\n",
       "   -0.08709026873111725,\n",
       "   -0.012767622247338295,\n",
       "   0.1341438740491867,\n",
       "   0.2561578154563904,\n",
       "   -0.07115161418914795,\n",
       "   0.09551379084587097,\n",
       "   -0.5203420519828796,\n",
       "   -0.48477786779403687,\n",
       "   -0.030244935303926468,\n",
       "   0.022203894332051277,\n",
       "   -0.05235461890697479,\n",
       "   0.2488836795091629,\n",
       "   0.35359013080596924,\n",
       "   -0.18211553990840912,\n",
       "   0.060222286731004715,\n",
       "   0.05252041667699814,\n",
       "   0.002144052181392908,\n",
       "   0.18971765041351318,\n",
       "   0.2827225923538208,\n",
       "   -0.008366024121642113,\n",
       "   -0.42145878076553345,\n",
       "   -0.006511470302939415,\n",
       "   -0.23788101971149445,\n",
       "   0.09491820633411407,\n",
       "   -0.7180013060569763,\n",
       "   0.11016767472028732,\n",
       "   -0.34327927231788635,\n",
       "   -0.056520674377679825,\n",
       "   0.04703885689377785,\n",
       "   0.006499601528048515,\n",
       "   0.08746583014726639,\n",
       "   0.10795775055885315,\n",
       "   0.03601650893688202,\n",
       "   0.3157738149166107,\n",
       "   0.3419637382030487,\n",
       "   0.5567554831504822,\n",
       "   -0.15163275599479675,\n",
       "   0.1298997551202774,\n",
       "   -0.07309000194072723,\n",
       "   0.15937203168869019,\n",
       "   -0.08595259487628937,\n",
       "   -0.19974102079868317,\n",
       "   0.16645190119743347,\n",
       "   -0.5543860793113708,\n",
       "   -0.040412288159132004,\n",
       "   0.16396939754486084,\n",
       "   0.19336771965026855,\n",
       "   0.3577406406402588,\n",
       "   -0.17118953168392181,\n",
       "   -0.3495045304298401,\n",
       "   0.3526316285133362,\n",
       "   0.03847859427332878,\n",
       "   -0.16705183684825897,\n",
       "   -0.25239285826683044,\n",
       "   0.11041722446680069,\n",
       "   0.2670481204986572,\n",
       "   6.820310954935849e-05,\n",
       "   0.297471284866333,\n",
       "   -0.34629499912261963,\n",
       "   -0.17280833423137665,\n",
       "   0.34860357642173767,\n",
       "   -0.044970281422138214,\n",
       "   0.06309551000595093,\n",
       "   -0.3987112045288086,\n",
       "   -0.08995205163955688,\n",
       "   0.05536274611949921,\n",
       "   -0.5251463055610657,\n",
       "   -0.2129545658826828,\n",
       "   -0.07076628506183624,\n",
       "   -0.2876066267490387,\n",
       "   -0.4250195324420929,\n",
       "   0.132330983877182,\n",
       "   0.17970241606235504,\n",
       "   0.33210423588752747,\n",
       "   0.019392983987927437,\n",
       "   0.30764561891555786,\n",
       "   -0.04990830272436142,\n",
       "   0.18875819444656372,\n",
       "   -0.059084098786115646,\n",
       "   -0.06327164173126221,\n",
       "   -0.3218717575073242,\n",
       "   0.317192018032074,\n",
       "   -0.06633531302213669,\n",
       "   0.09714189916849136,\n",
       "   0.6615355610847473,\n",
       "   0.07924572378396988,\n",
       "   -0.4919227063655853,\n",
       "   -0.33060526847839355,\n",
       "   0.25160691142082214,\n",
       "   0.5288073420524597,\n",
       "   0.1966135948896408,\n",
       "   -0.10171940922737122,\n",
       "   0.005397060886025429,\n",
       "   0.31913673877716064,\n",
       "   0.22122588753700256,\n",
       "   -0.08569313585758209,\n",
       "   -0.05345254763960838,\n",
       "   0.17569606006145477,\n",
       "   0.16806457936763763,\n",
       "   0.11552325636148453,\n",
       "   0.47912007570266724,\n",
       "   -0.2773284912109375,\n",
       "   -0.203982874751091,\n",
       "   -0.24475008249282837,\n",
       "   0.4472590982913971,\n",
       "   0.05991990864276886,\n",
       "   0.14667080342769623,\n",
       "   -0.022956321015954018,\n",
       "   -0.015726929530501366,\n",
       "   -0.11855374276638031,\n",
       "   0.05611748248338699,\n",
       "   -0.025354774668812752,\n",
       "   0.1608792096376419,\n",
       "   0.23543067276477814],\n",
       "  [0.4380306005477905,\n",
       "   0.210900217294693,\n",
       "   -0.052670497447252274,\n",
       "   -0.1262255162000656,\n",
       "   -0.05874508246779442,\n",
       "   0.05492733046412468,\n",
       "   0.3202914595603943,\n",
       "   0.19914427399635315,\n",
       "   -0.17336879670619965,\n",
       "   -0.00048610265366733074,\n",
       "   -0.16174274682998657,\n",
       "   -0.09440380334854126,\n",
       "   -0.19045302271842957,\n",
       "   0.22495269775390625,\n",
       "   -0.36981400847435,\n",
       "   -0.023018188774585724,\n",
       "   0.31124618649482727,\n",
       "   -0.04847848042845726,\n",
       "   0.05093797296285629,\n",
       "   -0.03317840024828911,\n",
       "   -0.24663200974464417,\n",
       "   -0.18018192052841187,\n",
       "   -0.3680281937122345,\n",
       "   0.1983019858598709,\n",
       "   -0.2498985081911087,\n",
       "   -0.0353095568716526,\n",
       "   0.2543593645095825,\n",
       "   0.2446197122335434,\n",
       "   -0.0007477824692614377,\n",
       "   0.5119876861572266,\n",
       "   -0.12161312252283096,\n",
       "   -0.3450152277946472,\n",
       "   0.23305492103099823,\n",
       "   0.14886067807674408,\n",
       "   0.23916558921337128,\n",
       "   -0.04996959865093231,\n",
       "   -0.0712234228849411,\n",
       "   0.07648071646690369,\n",
       "   -0.20264530181884766,\n",
       "   0.23693028092384338,\n",
       "   0.012702065519988537,\n",
       "   -0.021763689815998077,\n",
       "   0.10153042525053024,\n",
       "   0.06759732961654663,\n",
       "   0.6030600666999817,\n",
       "   -0.2958822548389435,\n",
       "   -0.10587042570114136,\n",
       "   -0.007551813963800669,\n",
       "   -0.18611647188663483,\n",
       "   0.19917316734790802,\n",
       "   -0.25504010915756226,\n",
       "   0.07514598965644836,\n",
       "   -0.3627380430698395,\n",
       "   -0.009144959039986134,\n",
       "   0.1708245575428009,\n",
       "   -0.1743583381175995,\n",
       "   0.1461927443742752,\n",
       "   -0.0410955585539341,\n",
       "   -0.21357133984565735,\n",
       "   0.054612766951322556,\n",
       "   -0.27208825945854187,\n",
       "   0.1340969204902649,\n",
       "   0.2232128083705902,\n",
       "   0.031715720891952515,\n",
       "   -0.27917346358299255,\n",
       "   -0.20939365029335022,\n",
       "   -0.007777912076562643,\n",
       "   0.20247092843055725,\n",
       "   -0.19757123291492462,\n",
       "   -0.04267428442835808,\n",
       "   -0.1177755668759346,\n",
       "   -0.1332431584596634,\n",
       "   -0.03235355764627457,\n",
       "   0.018399124965071678,\n",
       "   0.217594712972641,\n",
       "   -0.3008436858654022,\n",
       "   0.13714556396007538,\n",
       "   -0.19882237911224365,\n",
       "   0.0509692057967186,\n",
       "   0.09462472051382065,\n",
       "   0.3047659397125244,\n",
       "   0.3393493890762329,\n",
       "   -0.019480090588331223,\n",
       "   -0.07364865392446518,\n",
       "   -0.05286869779229164,\n",
       "   -0.12930597364902496,\n",
       "   -0.00409229751676321,\n",
       "   0.13941481709480286,\n",
       "   0.21452566981315613,\n",
       "   0.17876935005187988,\n",
       "   0.20739400386810303,\n",
       "   -0.27849850058555603,\n",
       "   0.01780695468187332,\n",
       "   -0.15293103456497192,\n",
       "   -0.4102531969547272,\n",
       "   -0.38746699690818787,\n",
       "   -0.040022436529397964,\n",
       "   -0.10330256074666977,\n",
       "   -0.08789067715406418,\n",
       "   0.2192866951227188,\n",
       "   0.16402216255664825,\n",
       "   -0.06744559854269028,\n",
       "   -0.03665207698941231,\n",
       "   -0.032389163970947266,\n",
       "   0.06484406441450119,\n",
       "   0.373661071062088,\n",
       "   0.019250361248850822,\n",
       "   -0.4348296821117401,\n",
       "   -0.27180010080337524,\n",
       "   0.48058027029037476,\n",
       "   0.2945426404476166,\n",
       "   -0.15215347707271576,\n",
       "   0.2519215941429138,\n",
       "   0.07214263081550598,\n",
       "   -0.06667055189609528,\n",
       "   -0.10644867271184921,\n",
       "   -0.22674474120140076,\n",
       "   -0.046892132610082626,\n",
       "   -0.1651047021150589,\n",
       "   0.006545334588736296,\n",
       "   0.42086756229400635,\n",
       "   0.14052614569664001,\n",
       "   0.3808513283729553,\n",
       "   0.04547526687383652,\n",
       "   0.28689050674438477,\n",
       "   0.1973133534193039,\n",
       "   0.004574491176754236,\n",
       "   -0.15157592296600342,\n",
       "   -0.13122963905334473,\n",
       "   0.1686127483844757,\n",
       "   -0.3945451080799103,\n",
       "   -0.08477753400802612,\n",
       "   0.23227326571941376,\n",
       "   0.020448757335543633,\n",
       "   0.03927678242325783,\n",
       "   0.06817276775836945,\n",
       "   -0.5179181098937988,\n",
       "   0.33212828636169434,\n",
       "   -0.8057745099067688,\n",
       "   -0.015311535447835922,\n",
       "   -0.05798092111945152,\n",
       "   0.15777915716171265,\n",
       "   0.22208158671855927,\n",
       "   -0.008002725429832935,\n",
       "   -0.008230707608163357,\n",
       "   -0.1637250781059265,\n",
       "   -0.4570911228656769,\n",
       "   -0.2643115520477295,\n",
       "   0.3087525963783264,\n",
       "   0.18788638710975647,\n",
       "   -0.40488171577453613,\n",
       "   -0.06350318342447281,\n",
       "   -0.03999937325716019,\n",
       "   0.1918586939573288,\n",
       "   -0.039502136409282684,\n",
       "   0.031954098492860794,\n",
       "   -0.08923674374818802,\n",
       "   0.16681404411792755,\n",
       "   0.354574590921402,\n",
       "   0.18125857412815094,\n",
       "   -0.142375186085701,\n",
       "   -0.4386008381843567,\n",
       "   0.027754036709666252,\n",
       "   -0.06387455016374588,\n",
       "   0.05645071342587471,\n",
       "   0.3692861795425415,\n",
       "   -0.2795341908931732,\n",
       "   0.06912178546190262,\n",
       "   0.1424749195575714,\n",
       "   0.19478259980678558,\n",
       "   0.30899369716644287,\n",
       "   -0.05117279291152954,\n",
       "   -0.16798615455627441,\n",
       "   0.1893695741891861,\n",
       "   -0.10407841205596924,\n",
       "   0.20873074233531952,\n",
       "   0.02473316341638565,\n",
       "   -0.3882734179496765,\n",
       "   -0.035601671785116196,\n",
       "   0.15831775963306427,\n",
       "   0.16325201094150543,\n",
       "   -0.011504348367452621,\n",
       "   -0.05520832911133766,\n",
       "   -0.36992156505584717,\n",
       "   -0.10951141268014908,\n",
       "   -0.3616717457771301,\n",
       "   0.20348604023456573,\n",
       "   -0.39946630597114563,\n",
       "   -0.00221978104673326,\n",
       "   -0.2010265737771988,\n",
       "   0.03275102376937866,\n",
       "   -0.029845761135220528,\n",
       "   -0.11224498599767685,\n",
       "   0.03374443203210831,\n",
       "   -0.35441192984580994,\n",
       "   -0.03676101937890053,\n",
       "   0.06937779486179352,\n",
       "   0.3444731533527374,\n",
       "   0.2585934102535248,\n",
       "   0.15039129555225372,\n",
       "   -0.3442442715167999,\n",
       "   -0.08351229131221771,\n",
       "   0.17500156164169312,\n",
       "   -0.007753829471766949,\n",
       "   0.11199814826250076,\n",
       "   0.2038857340812683,\n",
       "   0.01010423619300127,\n",
       "   -0.05796490237116814,\n",
       "   -0.0870349258184433,\n",
       "   -0.11421174556016922,\n",
       "   0.2956782877445221,\n",
       "   -0.10118887573480606,\n",
       "   0.09944593906402588,\n",
       "   0.25332802534103394,\n",
       "   -0.11213928461074829,\n",
       "   -0.42625662684440613,\n",
       "   0.19721701741218567,\n",
       "   -0.2510586082935333,\n",
       "   -0.3816452622413635,\n",
       "   -0.20430395007133484,\n",
       "   0.2432873249053955,\n",
       "   -0.11683947592973709,\n",
       "   0.41737282276153564,\n",
       "   -0.15499407052993774,\n",
       "   -0.37581437826156616,\n",
       "   0.2192123979330063,\n",
       "   0.3188773989677429,\n",
       "   0.4073411226272583,\n",
       "   0.17026233673095703,\n",
       "   -0.06188400462269783,\n",
       "   0.18801596760749817,\n",
       "   0.13301609456539154,\n",
       "   0.08371852338314056,\n",
       "   -0.3115852177143097,\n",
       "   0.15769745409488678,\n",
       "   -0.3944273293018341,\n",
       "   -0.18282148241996765,\n",
       "   0.10773348063230515,\n",
       "   -0.3037925660610199,\n",
       "   -0.17745675146579742,\n",
       "   0.2519146203994751,\n",
       "   0.20117555558681488,\n",
       "   0.061101753264665604,\n",
       "   -0.022105129435658455,\n",
       "   -0.18423773348331451,\n",
       "   -0.010329443030059338,\n",
       "   -0.21459823846817017,\n",
       "   0.4501461982727051,\n",
       "   0.779100239276886,\n",
       "   0.04754422605037689,\n",
       "   -0.03791997954249382,\n",
       "   -0.41023117303848267,\n",
       "   0.28360652923583984,\n",
       "   -0.3039858043193817,\n",
       "   0.12932901084423065,\n",
       "   -0.08981718122959137,\n",
       "   0.2958919107913971,\n",
       "   -0.05062803626060486,\n",
       "   0.1031932383775711,\n",
       "   -0.1210499033331871,\n",
       "   0.23213958740234375,\n",
       "   0.16863062977790833,\n",
       "   0.2162066549062729,\n",
       "   0.23430509865283966,\n",
       "   -0.1485917717218399,\n",
       "   -0.34676650166511536,\n",
       "   -0.004560909699648619,\n",
       "   0.08379505574703217,\n",
       "   -0.3507564067840576,\n",
       "   -0.19479301571846008,\n",
       "   -0.0693269893527031,\n",
       "   -0.11023733764886856,\n",
       "   0.1813579499721527,\n",
       "   0.4403132200241089,\n",
       "   0.3217198848724365,\n",
       "   0.008397978730499744,\n",
       "   0.30616500973701477,\n",
       "   -0.4940994381904602,\n",
       "   0.03551352396607399,\n",
       "   -0.31654539704322815,\n",
       "   0.11276320368051529,\n",
       "   0.03652320057153702,\n",
       "   0.18995364010334015,\n",
       "   0.43970316648483276,\n",
       "   -0.23830074071884155,\n",
       "   -0.20870934426784515,\n",
       "   0.1596832424402237,\n",
       "   0.11831559985876083,\n",
       "   0.04039258509874344,\n",
       "   -0.21522049605846405,\n",
       "   0.028479808941483498,\n",
       "   0.1811409592628479,\n",
       "   -0.24297626316547394,\n",
       "   0.031362518668174744,\n",
       "   0.29554328322410583,\n",
       "   0.16462896764278412,\n",
       "   -0.11995456367731094,\n",
       "   0.09964805096387863,\n",
       "   -0.0257333405315876,\n",
       "   -0.16909338533878326,\n",
       "   0.03782210126519203,\n",
       "   0.005222099367529154,\n",
       "   0.026718050241470337,\n",
       "   -0.05419585853815079,\n",
       "   -0.26980119943618774,\n",
       "   -0.06399288028478622,\n",
       "   0.0062891775742173195,\n",
       "   0.35960912704467773,\n",
       "   0.18837012350559235,\n",
       "   0.16775916516780853,\n",
       "   0.04863068088889122,\n",
       "   -0.09324606508016586,\n",
       "   -0.07806321233510971,\n",
       "   -0.05535034090280533,\n",
       "   -0.28791919350624084,\n",
       "   -0.3368677794933319,\n",
       "   -0.4295184314250946,\n",
       "   -0.14738109707832336,\n",
       "   -0.14600467681884766,\n",
       "   -0.018825480714440346,\n",
       "   0.29057368636131287,\n",
       "   0.10377629846334457,\n",
       "   -0.4138583838939667,\n",
       "   0.4599875211715698,\n",
       "   0.01580803468823433,\n",
       "   0.4257696270942688,\n",
       "   -0.2217375636100769,\n",
       "   0.25761619210243225,\n",
       "   0.14323239028453827,\n",
       "   -0.16359539330005646,\n",
       "   -0.3165924847126007,\n",
       "   -0.16561703383922577,\n",
       "   0.16619223356246948,\n",
       "   0.04196172580122948,\n",
       "   -0.4118786156177521,\n",
       "   -0.27600279450416565,\n",
       "   0.17035040259361267,\n",
       "   0.6534947752952576,\n",
       "   -0.6824848055839539,\n",
       "   -0.13542480766773224,\n",
       "   0.08721185475587845,\n",
       "   0.2712564766407013,\n",
       "   0.20926247537136078,\n",
       "   -0.12083928287029266,\n",
       "   -0.15765972435474396,\n",
       "   0.03232932463288307,\n",
       "   0.2601531445980072,\n",
       "   0.05075019970536232,\n",
       "   0.08390262722969055,\n",
       "   0.014329029247164726,\n",
       "   -0.21115650236606598,\n",
       "   -0.13296376168727875,\n",
       "   0.02965451590716839,\n",
       "   -0.3067527711391449,\n",
       "   0.35032129287719727,\n",
       "   -0.13680584728717804,\n",
       "   -0.0036114221438765526,\n",
       "   -0.12180431932210922,\n",
       "   0.15057896077632904,\n",
       "   0.11526428908109665,\n",
       "   0.29670459032058716,\n",
       "   -0.15221048891544342,\n",
       "   -0.2055361568927765,\n",
       "   -0.0678924173116684,\n",
       "   -0.11760536581277847,\n",
       "   0.0012066190829500556,\n",
       "   0.026815073564648628,\n",
       "   0.6970221400260925,\n",
       "   -0.19296219944953918,\n",
       "   -0.2010902613401413,\n",
       "   0.5711532235145569,\n",
       "   0.3741534352302551,\n",
       "   -0.03553831949830055,\n",
       "   -0.2838602066040039,\n",
       "   0.13142037391662598,\n",
       "   -0.2202506959438324,\n",
       "   -0.061581339687108994,\n",
       "   0.007960782386362553,\n",
       "   -0.27946802973747253,\n",
       "   -0.2727738320827484,\n",
       "   -0.001176284160465002,\n",
       "   0.24786223471164703,\n",
       "   0.21426552534103394,\n",
       "   -0.18941807746887207,\n",
       "   0.059227354824543,\n",
       "   0.10297957062721252,\n",
       "   -0.078288234770298,\n",
       "   -0.06953837722539902,\n",
       "   0.07679565995931625,\n",
       "   -0.0024040432181209326,\n",
       "   -0.4432339668273926,\n",
       "   0.08208049088716507,\n",
       "   0.3659498691558838,\n",
       "   -0.053423188626766205,\n",
       "   -0.1302853524684906,\n",
       "   -0.053362887352705,\n",
       "   -0.4038729667663574,\n",
       "   -0.054838601499795914,\n",
       "   0.1883615106344223,\n",
       "   -0.3929039239883423,\n",
       "   0.05019304156303406,\n",
       "   0.05341627448797226,\n",
       "   0.029554525390267372,\n",
       "   -0.11488314718008041,\n",
       "   -0.24069172143936157,\n",
       "   -0.4219631254673004,\n",
       "   0.04130954295396805,\n",
       "   -0.048055969178676605,\n",
       "   0.07495458424091339,\n",
       "   0.2477075159549713,\n",
       "   -0.1826329380273819,\n",
       "   -0.24363774061203003,\n",
       "   -0.13797684013843536,\n",
       "   -0.5324180126190186,\n",
       "   -0.0017103818245232105,\n",
       "   -0.47825106978416443,\n",
       "   0.07046844810247421,\n",
       "   -0.17923952639102936,\n",
       "   0.1330128312110901,\n",
       "   0.14417538046836853,\n",
       "   0.19596043229103088,\n",
       "   0.28581443428993225,\n",
       "   0.25897935032844543,\n",
       "   -0.40215998888015747,\n",
       "   -0.12183109670877457,\n",
       "   0.3362332582473755,\n",
       "   0.1774376481771469,\n",
       "   -0.10306418687105179,\n",
       "   -0.04938963055610657,\n",
       "   0.12972936034202576,\n",
       "   -0.07298307120800018,\n",
       "   0.018082253634929657,\n",
       "   -0.1228201761841774,\n",
       "   0.05954325199127197,\n",
       "   -0.27247872948646545,\n",
       "   0.1553383767604828,\n",
       "   0.13148346543312073,\n",
       "   0.07626047730445862,\n",
       "   0.06830253452062607,\n",
       "   0.14919273555278778,\n",
       "   -0.08339192718267441,\n",
       "   -0.3364958167076111,\n",
       "   0.03880216181278229,\n",
       "   -0.15943805873394012,\n",
       "   0.15880011022090912,\n",
       "   -0.046206794679164886,\n",
       "   0.24676655232906342,\n",
       "   0.07561313360929489,\n",
       "   0.1830637902021408,\n",
       "   -0.03845825046300888,\n",
       "   0.07522814720869064,\n",
       "   -0.42648008465766907,\n",
       "   0.5815994739532471,\n",
       "   -0.23995637893676758,\n",
       "   0.48084285855293274,\n",
       "   -0.2913372814655304,\n",
       "   -0.20187894999980927,\n",
       "   -0.12091851979494095,\n",
       "   -0.2218809574842453,\n",
       "   -0.03379948437213898,\n",
       "   -0.32990700006484985,\n",
       "   0.11025435477495193,\n",
       "   0.11386314779520035,\n",
       "   0.03980270400643349,\n",
       "   0.0053191399201750755,\n",
       "   0.13600939512252808,\n",
       "   0.6356155276298523,\n",
       "   0.09758827090263367,\n",
       "   0.009849500842392445,\n",
       "   0.05500701069831848,\n",
       "   0.05220899358391762,\n",
       "   0.24203164875507355,\n",
       "   0.30488139390945435,\n",
       "   0.48102498054504395,\n",
       "   0.30981847643852234,\n",
       "   -0.39013710618019104,\n",
       "   -0.12614582479000092,\n",
       "   0.13722248375415802,\n",
       "   -0.32690995931625366,\n",
       "   0.18290261924266815,\n",
       "   0.1792045384645462,\n",
       "   0.08139144629240036,\n",
       "   -0.3141728639602661,\n",
       "   0.09027010202407837,\n",
       "   0.31507888436317444,\n",
       "   0.1326594054698944,\n",
       "   0.17703261971473694,\n",
       "   0.07696732878684998,\n",
       "   0.08245795220136642,\n",
       "   -0.023447314277291298,\n",
       "   -0.190628319978714,\n",
       "   0.2302422672510147,\n",
       "   0.32595160603523254,\n",
       "   0.13202336430549622,\n",
       "   0.2564108073711395,\n",
       "   -0.011144865304231644,\n",
       "   -0.08377422392368317,\n",
       "   -0.0597519613802433,\n",
       "   0.04745233431458473,\n",
       "   0.210191547870636,\n",
       "   0.07621190696954727,\n",
       "   0.17983238399028778,\n",
       "   0.1168341264128685,\n",
       "   0.26468291878700256,\n",
       "   -0.08653181791305542,\n",
       "   0.4500010907649994,\n",
       "   0.25867608189582825,\n",
       "   0.0017354728188365698,\n",
       "   0.07030820846557617,\n",
       "   -0.08616571873426437,\n",
       "   0.08902314305305481,\n",
       "   -0.4497077465057373,\n",
       "   -0.039603572338819504,\n",
       "   -0.2938869893550873,\n",
       "   -0.05764032155275345,\n",
       "   -0.2789365351200104,\n",
       "   0.1788233071565628,\n",
       "   0.1160009577870369,\n",
       "   -0.15016406774520874,\n",
       "   0.32129690051078796,\n",
       "   0.030591271817684174,\n",
       "   0.3201099634170532,\n",
       "   -0.2542575001716614,\n",
       "   -0.3275570571422577,\n",
       "   0.18034932017326355,\n",
       "   -0.33816319704055786,\n",
       "   0.28386735916137695,\n",
       "   -0.23182271420955658,\n",
       "   -0.1901935338973999,\n",
       "   0.24319876730442047,\n",
       "   0.06920723617076874,\n",
       "   0.24478952586650848,\n",
       "   0.44464626908302307,\n",
       "   -0.2517569363117218,\n",
       "   0.2790299654006958,\n",
       "   -0.07325561344623566,\n",
       "   -0.028923558071255684,\n",
       "   0.09787865728139877,\n",
       "   -0.10459646582603455,\n",
       "   -0.19441698491573334,\n",
       "   0.25143271684646606,\n",
       "   -0.12311779707670212,\n",
       "   0.08852335810661316,\n",
       "   -0.03095248155295849,\n",
       "   -0.39934104681015015,\n",
       "   0.4047291576862335,\n",
       "   0.12288510799407959,\n",
       "   0.45952683687210083,\n",
       "   0.44761958718299866,\n",
       "   0.20812714099884033,\n",
       "   0.0466485396027565,\n",
       "   -0.19744464755058289,\n",
       "   -0.4105338454246521,\n",
       "   0.12212571501731873,\n",
       "   0.16474956274032593,\n",
       "   0.14503316581249237,\n",
       "   -0.34454289078712463,\n",
       "   -9.322747230529785,\n",
       "   0.24078269302845,\n",
       "   -0.44471272826194763,\n",
       "   0.39564135670661926,\n",
       "   -0.3840838074684143,\n",
       "   0.036498088389635086,\n",
       "   -0.05595589429140091,\n",
       "   -0.03218647092580795,\n",
       "   -0.03099445067346096,\n",
       "   0.10308727622032166,\n",
       "   0.09320298582315445,\n",
       "   0.15789751708507538,\n",
       "   0.2067078799009323,\n",
       "   0.016843657940626144,\n",
       "   -0.1295800507068634,\n",
       "   -0.108237124979496,\n",
       "   0.29996100068092346,\n",
       "   -0.48483309149742126,\n",
       "   0.1148216724395752,\n",
       "   0.02141597308218479,\n",
       "   0.02962310053408146,\n",
       "   -0.28744176030158997,\n",
       "   -0.09994498640298843,\n",
       "   0.28398269414901733,\n",
       "   -0.17824189364910126,\n",
       "   -0.01958383619785309,\n",
       "   0.1400882750749588,\n",
       "   -0.01783723011612892,\n",
       "   0.4497254490852356,\n",
       "   0.016170695424079895,\n",
       "   0.1466301679611206,\n",
       "   -0.1302686482667923,\n",
       "   -0.3358596861362457,\n",
       "   0.1471475511789322,\n",
       "   0.11855444312095642,\n",
       "   -0.1868991255760193,\n",
       "   0.11877822875976562,\n",
       "   -0.641497790813446,\n",
       "   0.6867641806602478,\n",
       "   -0.3936837315559387,\n",
       "   0.04680965468287468,\n",
       "   -0.17705675959587097,\n",
       "   0.005523044615983963,\n",
       "   0.05874678120017052,\n",
       "   0.20094239711761475,\n",
       "   0.062096405774354935,\n",
       "   -0.024835286661982536,\n",
       "   0.05178958550095558,\n",
       "   -0.0480714812874794,\n",
       "   0.4306737780570984,\n",
       "   0.13396808505058289,\n",
       "   -0.15484300255775452,\n",
       "   0.01418609730899334,\n",
       "   -0.13235272467136383,\n",
       "   -0.24366740882396698,\n",
       "   0.0006667660782113671,\n",
       "   -0.0028248955495655537,\n",
       "   0.09867911785840988,\n",
       "   0.25335171818733215,\n",
       "   -0.37478214502334595,\n",
       "   -0.009525921195745468,\n",
       "   0.5556350946426392,\n",
       "   0.6756357550621033,\n",
       "   -0.00551441079005599,\n",
       "   -0.1698327660560608,\n",
       "   0.5373854041099548,\n",
       "   -0.2397322952747345,\n",
       "   0.04505375772714615,\n",
       "   0.007625372614711523,\n",
       "   0.2808559536933899,\n",
       "   -0.21621106564998627,\n",
       "   -0.15523914992809296,\n",
       "   0.16535808145999908,\n",
       "   -0.1876118779182434,\n",
       "   -0.0724814236164093,\n",
       "   -0.33887845277786255,\n",
       "   0.03757435083389282,\n",
       "   0.11687035113573074,\n",
       "   -0.02573203481733799,\n",
       "   0.3523544371128082,\n",
       "   -0.11610648781061172,\n",
       "   0.1303560584783554,\n",
       "   0.47674673795700073,\n",
       "   -0.5442946553230286,\n",
       "   0.01888294145464897,\n",
       "   0.2023540884256363,\n",
       "   -0.1582576185464859,\n",
       "   0.049820978194475174,\n",
       "   -0.355574369430542,\n",
       "   0.20818151533603668,\n",
       "   -0.49924370646476746,\n",
       "   0.12082979828119278,\n",
       "   0.116719551384449,\n",
       "   -0.04799891263246536,\n",
       "   -0.008167187683284283,\n",
       "   0.4135652482509613,\n",
       "   0.004664221778512001,\n",
       "   -0.08934314548969269,\n",
       "   0.17423191666603088,\n",
       "   0.14946302771568298,\n",
       "   -0.16346964240074158,\n",
       "   0.20827555656433105,\n",
       "   0.05947339907288551,\n",
       "   -0.40804317593574524,\n",
       "   0.2730099558830261,\n",
       "   0.28856799006462097,\n",
       "   0.21905869245529175,\n",
       "   -0.02791532315313816,\n",
       "   0.035527996718883514,\n",
       "   -0.34404465556144714,\n",
       "   -0.1563568264245987,\n",
       "   0.037879325449466705,\n",
       "   0.27698105573654175,\n",
       "   0.12806962430477142,\n",
       "   -0.07923860847949982,\n",
       "   0.22679205238819122,\n",
       "   0.15942822396755219,\n",
       "   0.23874181509017944,\n",
       "   0.182816281914711,\n",
       "   -0.1755518913269043,\n",
       "   0.25527021288871765,\n",
       "   -0.4928838014602661,\n",
       "   0.23916183412075043,\n",
       "   -0.015249288640916348,\n",
       "   -0.04801681265234947,\n",
       "   0.23036612570285797,\n",
       "   -0.027865950018167496,\n",
       "   0.269221693277359,\n",
       "   0.08893660455942154,\n",
       "   -0.13743863999843597,\n",
       "   0.09915409237146378,\n",
       "   0.4332726299762726,\n",
       "   -0.2160199135541916,\n",
       "   0.10483335703611374,\n",
       "   -0.04306722804903984,\n",
       "   0.10380151867866516,\n",
       "   -0.4468033015727997,\n",
       "   0.019532574340701103,\n",
       "   0.1452455371618271,\n",
       "   0.4038349986076355,\n",
       "   -0.2994629442691803,\n",
       "   0.0982661247253418,\n",
       "   0.07292857766151428,\n",
       "   -0.10378275066614151,\n",
       "   0.3837405741214752,\n",
       "   -0.16728122532367706,\n",
       "   -0.1662827581167221,\n",
       "   0.10470841079950333,\n",
       "   0.3505886197090149,\n",
       "   0.029159804806113243,\n",
       "   -0.1318758875131607,\n",
       "   0.31319671869277954,\n",
       "   0.4171103239059448,\n",
       "   -0.11177968978881836,\n",
       "   0.13979649543762207,\n",
       "   -0.31767117977142334,\n",
       "   0.0738188624382019,\n",
       "   -0.1955603063106537,\n",
       "   0.015735503286123276,\n",
       "   0.19369176030158997,\n",
       "   -0.22244428098201752,\n",
       "   0.0433189682662487,\n",
       "   -0.24917805194854736,\n",
       "   0.047787223011255264,\n",
       "   0.018834110349416733,\n",
       "   -0.08522084355354309,\n",
       "   0.12228284776210785,\n",
       "   -0.28363004326820374,\n",
       "   0.04088720306754112,\n",
       "   -0.3267854154109955,\n",
       "   -0.16632500290870667,\n",
       "   0.20252473652362823,\n",
       "   -0.038067568093538284,\n",
       "   -0.0773315280675888,\n",
       "   -0.14201395213603973,\n",
       "   -0.16003647446632385,\n",
       "   -0.1513928771018982,\n",
       "   -0.18066731095314026,\n",
       "   0.35771194100379944,\n",
       "   -0.06347161531448364,\n",
       "   0.18099811673164368,\n",
       "   0.30264636874198914,\n",
       "   -0.051015328615903854,\n",
       "   -0.4416080117225647,\n",
       "   -0.2733084559440613,\n",
       "   0.0661417692899704,\n",
       "   0.10400627553462982,\n",
       "   0.002087206579744816,\n",
       "   0.19865718483924866,\n",
       "   0.07123604416847229,\n",
       "   0.10624618083238602,\n",
       "   -0.4558655917644501,\n",
       "   -0.13405227661132812,\n",
       "   0.05632396042346954,\n",
       "   0.052114732563495636,\n",
       "   0.41609176993370056,\n",
       "   0.11906835436820984,\n",
       "   0.37769976258277893,\n",
       "   -0.03881416469812393,\n",
       "   0.08613438904285431,\n",
       "   -0.0031885351054370403,\n",
       "   0.17402638494968414,\n",
       "   0.007820953615009785,\n",
       "   0.07826615124940872,\n",
       "   -0.10115692764520645,\n",
       "   0.03784390538930893,\n",
       "   -0.1557052582502365,\n",
       "   -0.4175284206867218,\n",
       "   0.07057558745145798,\n",
       "   0.035553671419620514,\n",
       "   0.1198819950222969],\n",
       "  [-0.16118073463439941,\n",
       "   0.2548774182796478,\n",
       "   -0.3390735983848572,\n",
       "   -0.12996628880500793,\n",
       "   0.5241678357124329,\n",
       "   -0.23995788395404816,\n",
       "   0.39605021476745605,\n",
       "   -0.08184140920639038,\n",
       "   0.09046152234077454,\n",
       "   0.4768110513687134,\n",
       "   0.3645860552787781,\n",
       "   0.08496446162462234,\n",
       "   -0.21909387409687042,\n",
       "   0.20740480720996857,\n",
       "   -0.6709026098251343,\n",
       "   -0.5020634531974792,\n",
       "   0.07063911110162735,\n",
       "   -0.4079700708389282,\n",
       "   -0.34148281812667847,\n",
       "   -0.04170208424329758,\n",
       "   0.2854068875312805,\n",
       "   0.30541566014289856,\n",
       "   -0.09589681774377823,\n",
       "   -0.027226101607084274,\n",
       "   0.16270078718662262,\n",
       "   -0.07065753638744354,\n",
       "   0.3289634883403778,\n",
       "   0.7368723154067993,\n",
       "   0.10515297949314117,\n",
       "   0.39846813678741455,\n",
       "   0.33371463418006897,\n",
       "   0.05020945891737938,\n",
       "   0.01961502991616726,\n",
       "   0.03776181489229202,\n",
       "   0.5269654393196106,\n",
       "   0.577707052230835,\n",
       "   -0.6188895106315613,\n",
       "   0.41022273898124695,\n",
       "   0.022482305765151978,\n",
       "   0.5657496452331543,\n",
       "   -0.015379868447780609,\n",
       "   -0.10566238313913345,\n",
       "   -0.009173219092190266,\n",
       "   -0.03448784723877907,\n",
       "   0.36081764101982117,\n",
       "   0.030257530510425568,\n",
       "   -0.0394144132733345,\n",
       "   -0.12640196084976196,\n",
       "   -0.381638765335083,\n",
       "   -0.016927171498537064,\n",
       "   0.01709684357047081,\n",
       "   -0.3749004900455475,\n",
       "   -0.26225584745407104,\n",
       "   -0.2754160761833191,\n",
       "   0.05243943631649017,\n",
       "   -0.1899704486131668,\n",
       "   0.18090781569480896,\n",
       "   -0.08110460638999939,\n",
       "   -0.2097596377134323,\n",
       "   0.800336480140686,\n",
       "   -0.13251100480556488,\n",
       "   0.2743009328842163,\n",
       "   -0.05620815232396126,\n",
       "   -0.4608657658100128,\n",
       "   0.2837355434894562,\n",
       "   0.20740865170955658,\n",
       "   0.0765501856803894,\n",
       "   -0.1381678432226181,\n",
       "   0.49509215354919434,\n",
       "   0.07816601544618607,\n",
       "   -0.23057416081428528,\n",
       "   0.29548728466033936,\n",
       "   -0.16200454533100128,\n",
       "   -0.259537011384964,\n",
       "   -0.2688930332660675,\n",
       "   -0.3849634528160095,\n",
       "   0.24702493846416473,\n",
       "   0.0551317073404789,\n",
       "   0.024577874690294266,\n",
       "   -0.03938732668757439,\n",
       "   0.14451764523983002,\n",
       "   0.6435462236404419,\n",
       "   -0.10290683060884476,\n",
       "   0.38851532340049744,\n",
       "   -0.5587705373764038,\n",
       "   0.016720520332455635,\n",
       "   -0.5611450672149658,\n",
       "   0.035533323884010315,\n",
       "   0.4389627277851105,\n",
       "   0.7349923849105835,\n",
       "   0.19315238296985626,\n",
       "   -0.5062922239303589,\n",
       "   0.1969098150730133,\n",
       "   -0.3151561915874481,\n",
       "   -0.22122812271118164,\n",
       "   -0.0012899384601041675,\n",
       "   -0.37908509373664856,\n",
       "   0.08574892580509186,\n",
       "   -0.5695251822471619,\n",
       "   -0.09390731900930405,\n",
       "   -0.3369566798210144,\n",
       "   -0.01994013972580433,\n",
       "   -0.06315156072378159,\n",
       "   -0.17143525183200836,\n",
       "   -0.15488559007644653,\n",
       "   0.1207503229379654,\n",
       "   -0.12998220324516296,\n",
       "   -0.7910171151161194,\n",
       "   -0.3719596266746521,\n",
       "   0.37155237793922424,\n",
       "   0.19428664445877075,\n",
       "   0.1816428303718567,\n",
       "   0.3181065618991852,\n",
       "   0.31228911876678467,\n",
       "   -0.3338971734046936,\n",
       "   -0.29936572909355164,\n",
       "   -0.1441803127527237,\n",
       "   0.30526527762413025,\n",
       "   0.2649695873260498,\n",
       "   -0.2768634557723999,\n",
       "   0.18579044938087463,\n",
       "   0.15355287492275238,\n",
       "   -0.23716074228286743,\n",
       "   -0.17795713245868683,\n",
       "   0.10366445779800415,\n",
       "   -0.3558698296546936,\n",
       "   -0.0638435035943985,\n",
       "   -0.3636840581893921,\n",
       "   0.2096608430147171,\n",
       "   -0.17850078642368317,\n",
       "   -0.26084548234939575,\n",
       "   -0.11780261248350143,\n",
       "   -0.18397954106330872,\n",
       "   0.26416805386543274,\n",
       "   0.18772967159748077,\n",
       "   0.2708500027656555,\n",
       "   -0.8909931778907776,\n",
       "   0.5056077837944031,\n",
       "   -0.9116321802139282,\n",
       "   -0.2875382900238037,\n",
       "   0.0341101810336113,\n",
       "   0.4675021171569824,\n",
       "   0.5237367153167725,\n",
       "   -0.3223753571510315,\n",
       "   -0.047804392874240875,\n",
       "   -0.5050091743469238,\n",
       "   -0.37830156087875366,\n",
       "   -0.0677693784236908,\n",
       "   0.35862988233566284,\n",
       "   0.15622828900814056,\n",
       "   -0.47065475583076477,\n",
       "   0.11395478993654251,\n",
       "   0.18747839331626892,\n",
       "   -0.6924287676811218,\n",
       "   -0.1241803839802742,\n",
       "   -0.13461756706237793,\n",
       "   -0.3887908160686493,\n",
       "   0.03179125115275383,\n",
       "   0.6316357851028442,\n",
       "   0.5841692090034485,\n",
       "   -0.049560513347387314,\n",
       "   -0.5320768356323242,\n",
       "   -0.38924509286880493,\n",
       "   0.2933293581008911,\n",
       "   0.163348987698555,\n",
       "   0.09418563544750214,\n",
       "   -0.011522192507982254,\n",
       "   0.176877960562706,\n",
       "   -0.5471404194831848,\n",
       "   0.4997970759868622,\n",
       "   0.5758669376373291,\n",
       "   -0.008185029961168766,\n",
       "   -0.010202936828136444,\n",
       "   0.12731988728046417,\n",
       "   -0.0454912893474102,\n",
       "   -0.054816991090774536,\n",
       "   0.7858953475952148,\n",
       "   0.036762434989213943,\n",
       "   -0.2523183822631836,\n",
       "   -0.05207815393805504,\n",
       "   0.4017382860183716,\n",
       "   0.015422760508954525,\n",
       "   -0.4325024485588074,\n",
       "   -0.630757212638855,\n",
       "   0.10409309715032578,\n",
       "   -0.550857663154602,\n",
       "   0.25061169266700745,\n",
       "   -0.7649785280227661,\n",
       "   0.5634002685546875,\n",
       "   0.2590901255607605,\n",
       "   0.04744146764278412,\n",
       "   0.025842998176813126,\n",
       "   -0.4543450176715851,\n",
       "   0.15765444934368134,\n",
       "   -0.5229416489601135,\n",
       "   0.3572026491165161,\n",
       "   0.08482224494218826,\n",
       "   0.10434741526842117,\n",
       "   -0.17223161458969116,\n",
       "   0.32637181878089905,\n",
       "   -0.07457491755485535,\n",
       "   0.25614985823631287,\n",
       "   -0.20128650963306427,\n",
       "   -0.05744699016213417,\n",
       "   0.18718795478343964,\n",
       "   0.32431185245513916,\n",
       "   -0.29233577847480774,\n",
       "   0.24213223159313202,\n",
       "   -0.3256370425224304,\n",
       "   -0.5171032547950745,\n",
       "   0.5864723920822144,\n",
       "   -0.9867942929267883,\n",
       "   -0.29782411456108093,\n",
       "   -0.2225319743156433,\n",
       "   0.46412453055381775,\n",
       "   -0.36717525124549866,\n",
       "   0.18604426085948944,\n",
       "   -0.0775744765996933,\n",
       "   0.04644857347011566,\n",
       "   0.08029460161924362,\n",
       "   -0.3733525574207306,\n",
       "   -0.0827520489692688,\n",
       "   0.04682158678770065,\n",
       "   -0.48164209723472595,\n",
       "   -0.6080595850944519,\n",
       "   0.7808125019073486,\n",
       "   0.004602822475135326,\n",
       "   0.4872892200946808,\n",
       "   0.07176916301250458,\n",
       "   -0.20548997819423676,\n",
       "   0.23054559528827667,\n",
       "   -0.08261416852474213,\n",
       "   0.2667090892791748,\n",
       "   -0.36862096190452576,\n",
       "   0.20790673792362213,\n",
       "   0.4168505370616913,\n",
       "   -0.12099795043468475,\n",
       "   -0.005754964891821146,\n",
       "   -0.47165223956108093,\n",
       "   -0.6004126071929932,\n",
       "   0.1418517678976059,\n",
       "   0.24783477187156677,\n",
       "   -0.17491188645362854,\n",
       "   0.09398355334997177,\n",
       "   -0.0757443979382515,\n",
       "   -0.18352988362312317,\n",
       "   -0.24794043600559235,\n",
       "   0.19859641790390015,\n",
       "   -0.04839085787534714,\n",
       "   -0.20087826251983643,\n",
       "   -0.2899754047393799,\n",
       "   -0.29209011793136597,\n",
       "   -0.15763351321220398,\n",
       "   -0.17393794655799866,\n",
       "   0.521251380443573,\n",
       "   0.002799214096739888,\n",
       "   -0.11521835625171661,\n",
       "   0.5686091780662537,\n",
       "   0.7254945039749146,\n",
       "   -1.0454331636428833,\n",
       "   0.18580187857151031,\n",
       "   0.5087223649024963,\n",
       "   0.28874972462654114,\n",
       "   -0.12014170736074448,\n",
       "   -0.3337426483631134,\n",
       "   -0.27898040413856506,\n",
       "   -0.3310823440551758,\n",
       "   -0.4359247088432312,\n",
       "   -0.8061231970787048,\n",
       "   0.29678940773010254,\n",
       "   0.17564541101455688,\n",
       "   -0.3080475926399231,\n",
       "   0.10638783872127533,\n",
       "   0.2888071537017822,\n",
       "   -0.03798442333936691,\n",
       "   -0.11371498554944992,\n",
       "   0.18010714650154114,\n",
       "   -0.21715635061264038,\n",
       "   -0.8490080237388611,\n",
       "   -0.5200294256210327,\n",
       "   0.6392515897750854,\n",
       "   0.37216809391975403,\n",
       "   0.4477895498275757,\n",
       "   0.004984165541827679,\n",
       "   -0.5795625448226929,\n",
       "   -0.4607103765010834,\n",
       "   -0.050184302031993866,\n",
       "   0.36190328001976013,\n",
       "   -0.03871099650859833,\n",
       "   -0.5729478001594543,\n",
       "   -0.3277944028377533,\n",
       "   0.01590915024280548,\n",
       "   -0.6231562495231628,\n",
       "   -0.113715261220932,\n",
       "   0.13008244335651398,\n",
       "   0.1216185912489891,\n",
       "   -0.05741766840219498,\n",
       "   0.24719016253948212,\n",
       "   -0.4525631368160248,\n",
       "   -0.1743970811367035,\n",
       "   -0.2690068781375885,\n",
       "   -0.09504774212837219,\n",
       "   0.47320830821990967,\n",
       "   -0.07921605557203293,\n",
       "   -0.1759805828332901,\n",
       "   -0.004227641969919205,\n",
       "   0.39032888412475586,\n",
       "   0.3683950901031494,\n",
       "   0.4590758681297302,\n",
       "   0.3725588619709015,\n",
       "   0.46738097071647644,\n",
       "   -0.09202755987644196,\n",
       "   -0.04713936895132065,\n",
       "   -0.021669814363121986,\n",
       "   -0.01067442912608385,\n",
       "   -0.26285603642463684,\n",
       "   -0.04701525345444679,\n",
       "   -0.1919429898262024,\n",
       "   -0.2101144939661026,\n",
       "   -0.0852481797337532,\n",
       "   -0.32133111357688904,\n",
       "   0.2805408835411072,\n",
       "   -0.3578857183456421,\n",
       "   0.4713608920574188,\n",
       "   0.02095244824886322,\n",
       "   0.2167963683605194,\n",
       "   0.1199127659201622,\n",
       "   0.48704010248184204,\n",
       "   0.015723945572972298,\n",
       "   -0.07474096119403839,\n",
       "   0.17614050209522247,\n",
       "   -0.5614811182022095,\n",
       "   -0.22983399033546448,\n",
       "   0.17573678493499756,\n",
       "   -0.6686015725135803,\n",
       "   -0.25735047459602356,\n",
       "   -0.2801733911037445,\n",
       "   0.32306599617004395,\n",
       "   -0.8662562966346741,\n",
       "   0.6142311096191406,\n",
       "   0.20499272644519806,\n",
       "   0.41967907547950745,\n",
       "   0.2565876841545105,\n",
       "   -0.3012479543685913,\n",
       "   0.5664308667182922,\n",
       "   0.4279715418815613,\n",
       "   -0.062226466834545135,\n",
       "   0.40130361914634705,\n",
       "   -0.11381091177463531,\n",
       "   0.03217336907982826,\n",
       "   -0.473230242729187,\n",
       "   -0.3058851957321167,\n",
       "   -0.032181572169065475,\n",
       "   -0.38531213998794556,\n",
       "   0.6459419131278992,\n",
       "   0.045085590332746506,\n",
       "   -0.06170397624373436,\n",
       "   0.38781142234802246,\n",
       "   0.09355365484952927,\n",
       "   0.6372921466827393,\n",
       "   0.24056565761566162,\n",
       "   -0.0035385824739933014,\n",
       "   0.4737622141838074,\n",
       "   0.04744148626923561,\n",
       "   0.2823474109172821,\n",
       "   -0.1681346893310547,\n",
       "   -0.8622671961784363,\n",
       "   -0.2075333297252655,\n",
       "   -0.8606033325195312,\n",
       "   -0.2791644334793091,\n",
       "   1.182724118232727,\n",
       "   -0.2324555516242981,\n",
       "   0.22582463920116425,\n",
       "   -0.1917213350534439,\n",
       "   0.4585868716239929,\n",
       "   -0.3879396319389343,\n",
       "   -0.6489843130111694,\n",
       "   0.05028432607650757,\n",
       "   0.0007891610148362815,\n",
       "   -0.6255216598510742,\n",
       "   -0.27062922716140747,\n",
       "   0.322523832321167,\n",
       "   0.25221189856529236,\n",
       "   -0.1672642081975937,\n",
       "   0.030149750411510468,\n",
       "   -0.3043549954891205,\n",
       "   0.01907193660736084,\n",
       "   0.4379344880580902,\n",
       "   -0.03912591189146042,\n",
       "   -0.6419826745986938,\n",
       "   0.3682537376880646,\n",
       "   0.32277238368988037,\n",
       "   0.395547479391098,\n",
       "   -0.4164431393146515,\n",
       "   -0.13892892003059387,\n",
       "   0.3417985737323761,\n",
       "   -0.4647062122821808,\n",
       "   0.4481540620326996,\n",
       "   0.16648180782794952,\n",
       "   -0.12956558167934418,\n",
       "   -0.30280426144599915,\n",
       "   0.2549421191215515,\n",
       "   -0.040789730846881866,\n",
       "   -0.3203645944595337,\n",
       "   -0.6121866106987,\n",
       "   -0.39085763692855835,\n",
       "   -0.30959269404411316,\n",
       "   0.2779303789138794,\n",
       "   0.0860968679189682,\n",
       "   0.10871454328298569,\n",
       "   -0.2614288330078125,\n",
       "   -0.40568551421165466,\n",
       "   0.014902557246387005,\n",
       "   0.2827588617801666,\n",
       "   0.47607752680778503,\n",
       "   -0.40821054577827454,\n",
       "   0.060470253229141235,\n",
       "   -0.09930220246315002,\n",
       "   -0.005339318420737982,\n",
       "   0.040561869740486145,\n",
       "   -0.02095985971391201,\n",
       "   0.04225371778011322,\n",
       "   0.19403910636901855,\n",
       "   -0.16551020741462708,\n",
       "   -0.026312537491321564,\n",
       "   0.2763359248638153,\n",
       "   0.0408068485558033,\n",
       "   -0.034035857766866684,\n",
       "   -0.4164750576019287,\n",
       "   0.09210696816444397,\n",
       "   -0.11870863288640976,\n",
       "   0.7137219905853271,\n",
       "   -0.018477603793144226,\n",
       "   0.869632363319397,\n",
       "   -0.0678962841629982,\n",
       "   0.1548815816640854,\n",
       "   -0.19119805097579956,\n",
       "   0.13617166876792908,\n",
       "   -0.2600694000720978,\n",
       "   0.2403062880039215,\n",
       "   -0.2703897953033447,\n",
       "   -0.4293617308139801,\n",
       "   -0.6342846751213074,\n",
       "   -0.7949784398078918,\n",
       "   -0.0339985117316246,\n",
       "   -0.4401686191558838,\n",
       "   -0.42362886667251587,\n",
       "   -0.5722860097885132,\n",
       "   0.028718004003167152,\n",
       "   -0.3551661968231201,\n",
       "   0.2383914738893509,\n",
       "   0.05603959783911705,\n",
       "   -0.35834383964538574,\n",
       "   0.08771766722202301,\n",
       "   0.29782646894454956,\n",
       "   -0.4206429123878479,\n",
       "   -0.6134317517280579,\n",
       "   -0.3106146454811096,\n",
       "   -0.368191659450531,\n",
       "   0.008005979470908642,\n",
       "   -1.1116645336151123,\n",
       "   0.31659209728240967,\n",
       "   0.0418047159910202,\n",
       "   0.5331407189369202,\n",
       "   -0.13732559978961945,\n",
       "   -0.1177656501531601,\n",
       "   0.5778922438621521,\n",
       "   0.4515661597251892,\n",
       "   0.8493822813034058,\n",
       "   0.12393416464328766,\n",
       "   0.7081280946731567,\n",
       "   0.6008553504943848,\n",
       "   0.16986572742462158,\n",
       "   1.044695496559143,\n",
       "   0.04611176997423172,\n",
       "   -0.9326907396316528,\n",
       "   0.028310520574450493,\n",
       "   0.3845919668674469,\n",
       "   -0.17224495112895966,\n",
       "   -0.1473865658044815,\n",
       "   -0.6794402599334717,\n",
       "   0.05287392809987068,\n",
       "   -0.3958607316017151,\n",
       "   -0.23576250672340393,\n",
       "   0.033211600035429,\n",
       "   -0.2599146068096161,\n",
       "   1.113266110420227,\n",
       "   0.15778294205665588,\n",
       "   0.400227427482605,\n",
       "   0.39104315638542175,\n",
       "   0.05080532655119896,\n",
       "   -0.23026372492313385,\n",
       "   -0.02312278002500534,\n",
       "   0.1438980996608734,\n",
       "   -0.6486461162567139,\n",
       "   0.13170570135116577,\n",
       "   -0.0320870541036129,\n",
       "   0.1320532113313675,\n",
       "   0.17821750044822693,\n",
       "   -0.4725234806537628,\n",
       "   -0.06972045451402664,\n",
       "   0.18372948467731476,\n",
       "   0.24692592024803162,\n",
       "   0.31285321712493896,\n",
       "   -0.6673089265823364,\n",
       "   1.0399261713027954,\n",
       "   0.32757696509361267,\n",
       "   0.0540623664855957,\n",
       "   0.4357715845108032,\n",
       "   0.39679914712905884,\n",
       "   -0.07823719084262848,\n",
       "   -0.2924969494342804,\n",
       "   0.03814644366502762,\n",
       "   -0.023923467844724655,\n",
       "   -0.012193912640213966,\n",
       "   -0.10499174147844315,\n",
       "   0.4143116772174835,\n",
       "   0.11188878864049911,\n",
       "   0.15597599744796753,\n",
       "   -0.2224600911140442,\n",
       "   0.4373753070831299,\n",
       "   0.24591423571109772,\n",
       "   0.20151974260807037,\n",
       "   0.36282798647880554,\n",
       "   0.0650181919336319,\n",
       "   -0.05484314635396004,\n",
       "   0.7511821389198303,\n",
       "   0.6218429803848267,\n",
       "   0.08290044218301773,\n",
       "   0.007583341095596552,\n",
       "   -0.1664692461490631,\n",
       "   -0.02026153914630413,\n",
       "   0.26508572697639465,\n",
       "   -0.30006518959999084,\n",
       "   1.0192816257476807,\n",
       "   -0.25232940912246704,\n",
       "   0.5490958094596863,\n",
       "   -0.1917032152414322,\n",
       "   -0.30338191986083984,\n",
       "   -0.2464970499277115,\n",
       "   -0.08461529016494751,\n",
       "   0.02281809411942959,\n",
       "   0.18356791138648987,\n",
       "   0.09961261600255966,\n",
       "   0.15479308366775513,\n",
       "   0.3834020495414734,\n",
       "   -0.20922957360744476,\n",
       "   0.39501580595970154,\n",
       "   0.5017167329788208,\n",
       "   -0.14048147201538086,\n",
       "   0.3479826748371124,\n",
       "   -0.00677545415237546,\n",
       "   -1.290539026260376,\n",
       "   0.2572508454322815,\n",
       "   0.30709871649742126,\n",
       "   -0.2646244168281555,\n",
       "   -0.10250473022460938,\n",
       "   -8.46123218536377,\n",
       "   0.42074084281921387,\n",
       "   -0.06890016794204712,\n",
       "   0.3591923117637634,\n",
       "   0.3437114953994751,\n",
       "   0.6763907670974731,\n",
       "   0.25267913937568665,\n",
       "   -0.6744294762611389,\n",
       "   0.016844823956489563,\n",
       "   -0.038938675075769424,\n",
       "   -0.33121955394744873,\n",
       "   -0.3340364992618561,\n",
       "   0.5931398272514343,\n",
       "   -0.0632513239979744,\n",
       "   0.35472142696380615,\n",
       "   0.11524216830730438,\n",
       "   -0.08524611592292786,\n",
       "   0.23076768219470978,\n",
       "   0.6242290735244751,\n",
       "   -0.37884923815727234,\n",
       "   -0.07490288466215134,\n",
       "   0.456752210855484,\n",
       "   0.2507925033569336,\n",
       "   0.14842303097248077,\n",
       "   -0.08614382147789001,\n",
       "   0.136470764875412,\n",
       "   0.3224853277206421,\n",
       "   0.019159605726599693,\n",
       "   0.12716500461101532,\n",
       "   -0.08718591928482056,\n",
       "   -0.14094434678554535,\n",
       "   0.0700392946600914,\n",
       "   -0.14966751635074615,\n",
       "   -0.16681720316410065,\n",
       "   0.3882993459701538,\n",
       "   -0.33280935883522034,\n",
       "   0.05080309882760048,\n",
       "   -0.18961967527866364,\n",
       "   0.8266004920005798,\n",
       "   -0.47719818353652954,\n",
       "   -0.055155038833618164,\n",
       "   -0.22593015432357788,\n",
       "   -0.32767993211746216,\n",
       "   -0.22564692795276642,\n",
       "   0.26996666193008423,\n",
       "   0.11049834638834,\n",
       "   0.20235401391983032,\n",
       "   -0.18657255172729492,\n",
       "   -0.6951897740364075,\n",
       "   0.39505791664123535,\n",
       "   0.5411190986633301,\n",
       "   0.2780722677707672,\n",
       "   0.43997687101364136,\n",
       "   0.2338048368692398,\n",
       "   -0.6291672587394714,\n",
       "   -0.2125922441482544,\n",
       "   -0.20704716444015503,\n",
       "   0.7813321352005005,\n",
       "   0.06425883620977402,\n",
       "   -0.24195292592048645,\n",
       "   0.1982850581407547,\n",
       "   -0.3871174156665802,\n",
       "   1.0662949085235596,\n",
       "   -0.010818087495863438,\n",
       "   -0.16491729021072388,\n",
       "   0.42062515020370483,\n",
       "   0.0089157335460186,\n",
       "   0.36211639642715454,\n",
       "   0.06667596846818924,\n",
       "   -0.2655068337917328,\n",
       "   -0.1279829442501068,\n",
       "   -0.40621617436408997,\n",
       "   0.15574201941490173,\n",
       "   -0.879731297492981,\n",
       "   0.10944898426532745,\n",
       "   -1.0463292598724365,\n",
       "   -0.6048270463943481,\n",
       "   -0.05775965750217438,\n",
       "   -0.32270190119743347,\n",
       "   0.41138604283332825,\n",
       "   -0.27129366993904114,\n",
       "   0.5715922713279724,\n",
       "   0.27490901947021484,\n",
       "   -0.6586940288543701,\n",
       "   0.03106859140098095,\n",
       "   0.11090236902236938,\n",
       "   -0.2458256036043167,\n",
       "   0.24054737389087677,\n",
       "   -0.14132289588451385,\n",
       "   0.061777494847774506,\n",
       "   0.08935718983411789,\n",
       "   0.4066143333911896,\n",
       "   -0.011214351281523705,\n",
       "   -0.021839186549186707,\n",
       "   0.1442030817270279,\n",
       "   0.2681195139884949,\n",
       "   -0.2094753086566925,\n",
       "   -0.2278686761856079,\n",
       "   0.06600209325551987,\n",
       "   0.00026302330661565065,\n",
       "   -0.6900690793991089,\n",
       "   -0.08256640285253525,\n",
       "   -0.46704408526420593,\n",
       "   0.41129758954048157,\n",
       "   -0.12397667020559311,\n",
       "   -0.30216071009635925,\n",
       "   0.46779966354370117,\n",
       "   -0.634355366230011,\n",
       "   0.4759574830532074,\n",
       "   0.3509061932563782,\n",
       "   -0.41110432147979736,\n",
       "   0.4805918335914612,\n",
       "   0.9691585898399353,\n",
       "   -0.6198939085006714,\n",
       "   0.1756073236465454,\n",
       "   0.4637015163898468,\n",
       "   0.6311014890670776,\n",
       "   -0.11954217404127121,\n",
       "   0.28854304552078247,\n",
       "   -0.6181805729866028,\n",
       "   -0.2643481194972992,\n",
       "   -0.31100642681121826,\n",
       "   0.2058284431695938,\n",
       "   -0.08262571692466736,\n",
       "   0.584730327129364,\n",
       "   -0.34600502252578735,\n",
       "   0.10258397459983826,\n",
       "   -0.08332227915525436,\n",
       "   0.3845910131931305,\n",
       "   0.016180209815502167,\n",
       "   0.20843781530857086,\n",
       "   -0.018626578152179718,\n",
       "   -0.24249927699565887,\n",
       "   0.7938210964202881,\n",
       "   0.2790290415287018,\n",
       "   0.019204962998628616,\n",
       "   -0.17275623977184296,\n",
       "   -0.48921340703964233,\n",
       "   -0.841564953327179,\n",
       "   -0.3021145462989807,\n",
       "   -0.5301535725593567,\n",
       "   -0.3505966067314148,\n",
       "   0.8121249675750732,\n",
       "   0.46847644448280334,\n",
       "   0.7174308896064758,\n",
       "   -0.40908607840538025,\n",
       "   0.15861095488071442,\n",
       "   0.2529900372028351,\n",
       "   0.03455576300621033,\n",
       "   -0.34811991453170776,\n",
       "   -0.3595316410064697,\n",
       "   0.12215223163366318,\n",
       "   0.8088842630386353,\n",
       "   0.6441749334335327,\n",
       "   0.599824845790863,\n",
       "   -0.7174039483070374,\n",
       "   0.5150080919265747,\n",
       "   0.5524469017982483,\n",
       "   0.03858581557869911,\n",
       "   0.09241300076246262,\n",
       "   0.17091429233551025,\n",
       "   -0.08983244746923447,\n",
       "   0.22865702211856842,\n",
       "   1.0073151588439941,\n",
       "   -0.15557776391506195,\n",
       "   0.3272559642791748,\n",
       "   -0.2570045590400696,\n",
       "   0.512413740158081,\n",
       "   0.1297077238559723,\n",
       "   0.4151148200035095,\n",
       "   -0.06049398332834244,\n",
       "   -0.6515087485313416,\n",
       "   0.45937466621398926,\n",
       "   0.31523603200912476,\n",
       "   -0.06234518066048622,\n",
       "   -0.4168168902397156,\n",
       "   0.08525674790143967,\n",
       "   -0.3192557394504547,\n",
       "   0.46335798501968384,\n",
       "   -0.06699097901582718,\n",
       "   0.320034384727478,\n",
       "   0.4879395067691803,\n",
       "   -0.37413710355758667,\n",
       "   -0.18712744116783142,\n",
       "   -0.8024079203605652,\n",
       "   -0.12542985379695892,\n",
       "   0.2942425012588501,\n",
       "   0.2069431096315384,\n",
       "   0.08666452020406723,\n",
       "   -0.2975652515888214,\n",
       "   0.07736869156360626,\n",
       "   0.26998618245124817,\n",
       "   -0.09897097945213318,\n",
       "   0.15428225696086884,\n",
       "   0.06912282109260559,\n",
       "   0.21941202878952026,\n",
       "   -0.02383633516728878,\n",
       "   0.32676413655281067,\n",
       "   -0.0658387765288353,\n",
       "   -0.22021901607513428,\n",
       "   0.10840217024087906,\n",
       "   0.2581137418746948,\n",
       "   -0.3394894003868103,\n",
       "   -0.31116148829460144,\n",
       "   -0.36273330450057983,\n",
       "   -0.1468496024608612,\n",
       "   0.10231974720954895,\n",
       "   0.667039692401886,\n",
       "   -0.07141995429992676,\n",
       "   0.23693785071372986,\n",
       "   0.03896516188979149],\n",
       "  [-0.25803714990615845,\n",
       "   -0.0265810526907444,\n",
       "   -0.00020897174545098096,\n",
       "   0.09458506107330322,\n",
       "   0.16868817806243896,\n",
       "   -0.07082990556955338,\n",
       "   0.13221102952957153,\n",
       "   -0.0899355560541153,\n",
       "   -0.3115690052509308,\n",
       "   0.3077640235424042,\n",
       "   -0.09806567430496216,\n",
       "   0.04183104634284973,\n",
       "   -0.3141549527645111,\n",
       "   -0.15492792427539825,\n",
       "   -0.5019870400428772,\n",
       "   -0.6071441769599915,\n",
       "   0.1316125988960266,\n",
       "   -0.5406484603881836,\n",
       "   -0.05289701744914055,\n",
       "   -0.07065131515264511,\n",
       "   0.05446361005306244,\n",
       "   -0.14147254824638367,\n",
       "   -0.23325785994529724,\n",
       "   -0.15816374123096466,\n",
       "   0.141606867313385,\n",
       "   -0.5551360249519348,\n",
       "   -0.12598863244056702,\n",
       "   0.7447522282600403,\n",
       "   -0.20747996866703033,\n",
       "   0.45832914113998413,\n",
       "   0.28302517533302307,\n",
       "   -0.2252042442560196,\n",
       "   0.4948596954345703,\n",
       "   0.023368338122963905,\n",
       "   0.038054876029491425,\n",
       "   0.24787358939647675,\n",
       "   -0.28949329257011414,\n",
       "   0.5485417246818542,\n",
       "   0.07099629193544388,\n",
       "   0.39270997047424316,\n",
       "   0.16960187256336212,\n",
       "   0.2640783190727234,\n",
       "   0.05051075667142868,\n",
       "   0.31199339032173157,\n",
       "   -0.7102119326591492,\n",
       "   -0.17547526955604553,\n",
       "   -0.06917273253202438,\n",
       "   -0.5312821865081787,\n",
       "   -0.279775470495224,\n",
       "   0.10280601680278778,\n",
       "   0.5880903005599976,\n",
       "   -0.13363134860992432,\n",
       "   -0.20380611717700958,\n",
       "   -0.10658526420593262,\n",
       "   0.11379894614219666,\n",
       "   -0.48533186316490173,\n",
       "   0.15005235373973846,\n",
       "   0.1257888823747635,\n",
       "   -0.2478102445602417,\n",
       "   0.374017596244812,\n",
       "   -0.06073310598731041,\n",
       "   -0.15053251385688782,\n",
       "   0.009287974797189236,\n",
       "   -0.05344802141189575,\n",
       "   0.056370124220848083,\n",
       "   0.044514358043670654,\n",
       "   0.32063573598861694,\n",
       "   -0.4170417785644531,\n",
       "   0.20449037849903107,\n",
       "   0.20016175508499146,\n",
       "   -0.19750799238681793,\n",
       "   -0.10165729373693466,\n",
       "   -0.27798131108283997,\n",
       "   -0.2658194899559021,\n",
       "   0.06533128768205643,\n",
       "   -0.08551554381847382,\n",
       "   -0.3567045331001282,\n",
       "   0.06505706161260605,\n",
       "   0.14206022024154663,\n",
       "   0.17886494100093842,\n",
       "   0.26966947317123413,\n",
       "   0.14091049134731293,\n",
       "   -0.20916755497455597,\n",
       "   0.5363060235977173,\n",
       "   0.1024615690112114,\n",
       "   0.15151521563529968,\n",
       "   -0.2999197840690613,\n",
       "   -0.013457663357257843,\n",
       "   -0.1186637282371521,\n",
       "   0.2297438085079193,\n",
       "   0.3306187391281128,\n",
       "   -0.5195228457450867,\n",
       "   0.5238832235336304,\n",
       "   -0.055313173681497574,\n",
       "   -0.1254166066646576,\n",
       "   -0.14465810358524323,\n",
       "   -0.4831167161464691,\n",
       "   -0.36931172013282776,\n",
       "   -0.9509906768798828,\n",
       "   0.08079375326633453,\n",
       "   -0.11169762164354324,\n",
       "   0.46738144755363464,\n",
       "   -0.047343891113996506,\n",
       "   -0.20623484253883362,\n",
       "   0.06429433077573776,\n",
       "   0.1678917557001114,\n",
       "   -0.591273844242096,\n",
       "   -0.8214201927185059,\n",
       "   -0.38725534081459045,\n",
       "   0.6589660048484802,\n",
       "   0.036764051765203476,\n",
       "   0.33561888337135315,\n",
       "   -0.31255000829696655,\n",
       "   0.28288334608078003,\n",
       "   -0.4104164242744446,\n",
       "   -0.7497534155845642,\n",
       "   -0.0038992546033114195,\n",
       "   0.14202988147735596,\n",
       "   0.11027470976114273,\n",
       "   0.16176743805408478,\n",
       "   0.2506232261657715,\n",
       "   -0.050049953162670135,\n",
       "   -0.15158088505268097,\n",
       "   0.1035231500864029,\n",
       "   0.0014173256931826472,\n",
       "   -0.12921521067619324,\n",
       "   -0.2123311161994934,\n",
       "   -0.47230926156044006,\n",
       "   0.2650698125362396,\n",
       "   -0.16920658946037292,\n",
       "   -0.2161271572113037,\n",
       "   0.11385975778102875,\n",
       "   -0.10639365017414093,\n",
       "   -0.23700076341629028,\n",
       "   0.23395128548145294,\n",
       "   0.13658872246742249,\n",
       "   -0.5958523154258728,\n",
       "   -0.09192998707294464,\n",
       "   -1.182631015777588,\n",
       "   -0.4507283568382263,\n",
       "   -0.2806835472583771,\n",
       "   0.3938457667827606,\n",
       "   0.6745367646217346,\n",
       "   -0.21526098251342773,\n",
       "   0.1413029432296753,\n",
       "   -0.13156066834926605,\n",
       "   -0.519188404083252,\n",
       "   0.0002762237854767591,\n",
       "   0.29900795221328735,\n",
       "   0.112957663834095,\n",
       "   -0.16458886861801147,\n",
       "   -0.25166887044906616,\n",
       "   0.21291135251522064,\n",
       "   -0.03473478555679321,\n",
       "   -0.32404625415802,\n",
       "   -0.03754853829741478,\n",
       "   -0.16651833057403564,\n",
       "   0.0060563418082892895,\n",
       "   0.5024867057800293,\n",
       "   0.6025907397270203,\n",
       "   0.1768023818731308,\n",
       "   -0.8164893388748169,\n",
       "   -0.1373092085123062,\n",
       "   0.4232575595378876,\n",
       "   0.1715061068534851,\n",
       "   -0.24418826401233673,\n",
       "   -0.06814243644475937,\n",
       "   0.4653151035308838,\n",
       "   -0.38659676909446716,\n",
       "   0.11799412220716476,\n",
       "   0.332793653011322,\n",
       "   -0.23555715382099152,\n",
       "   0.5027730464935303,\n",
       "   0.35157012939453125,\n",
       "   0.1005173772573471,\n",
       "   -0.09703228622674942,\n",
       "   0.3153653144836426,\n",
       "   -0.02800494059920311,\n",
       "   -0.04500807076692581,\n",
       "   -0.24231819808483124,\n",
       "   0.23741871118545532,\n",
       "   -0.49472925066947937,\n",
       "   0.17074550688266754,\n",
       "   -0.5892704725265503,\n",
       "   0.3051777184009552,\n",
       "   -0.43087005615234375,\n",
       "   -0.029568640515208244,\n",
       "   -0.6974031925201416,\n",
       "   0.16119936108589172,\n",
       "   0.06513392180204391,\n",
       "   0.07700634002685547,\n",
       "   -0.32202574610710144,\n",
       "   -0.5001997351646423,\n",
       "   -0.06798636168241501,\n",
       "   -0.988978385925293,\n",
       "   0.3637491464614868,\n",
       "   -0.057732149958610535,\n",
       "   0.2952461242675781,\n",
       "   0.0727556124329567,\n",
       "   0.2016189694404602,\n",
       "   -0.005036074202507734,\n",
       "   0.15082699060440063,\n",
       "   -0.3018663227558136,\n",
       "   -0.03103640489280224,\n",
       "   -0.13230644166469574,\n",
       "   0.21461236476898193,\n",
       "   -0.7408348917961121,\n",
       "   0.42254742980003357,\n",
       "   -0.17302951216697693,\n",
       "   -0.6659635305404663,\n",
       "   -0.15661171078681946,\n",
       "   -0.6492679715156555,\n",
       "   0.09099531918764114,\n",
       "   -0.012517480179667473,\n",
       "   -0.5251418352127075,\n",
       "   -0.2879222333431244,\n",
       "   -0.22010566294193268,\n",
       "   0.14300186932086945,\n",
       "   -0.048124413937330246,\n",
       "   -0.13265088200569153,\n",
       "   0.48282527923583984,\n",
       "   0.22073163092136383,\n",
       "   0.38687485456466675,\n",
       "   -0.009591281414031982,\n",
       "   -0.3682834208011627,\n",
       "   0.333744078874588,\n",
       "   0.3418099880218506,\n",
       "   0.5150434374809265,\n",
       "   0.4715134799480438,\n",
       "   -0.1770418882369995,\n",
       "   -0.16076083481311798,\n",
       "   0.13847434520721436,\n",
       "   -0.008947677910327911,\n",
       "   -0.4163864552974701,\n",
       "   0.26521891355514526,\n",
       "   0.06732258200645447,\n",
       "   -0.06145767867565155,\n",
       "   0.1144726350903511,\n",
       "   -0.7358319163322449,\n",
       "   -0.584770679473877,\n",
       "   -0.04286666959524155,\n",
       "   -0.2064393311738968,\n",
       "   0.29114946722984314,\n",
       "   0.44633254408836365,\n",
       "   -0.1090950146317482,\n",
       "   0.03071620501577854,\n",
       "   -0.40206706523895264,\n",
       "   0.7018203139305115,\n",
       "   0.17239882051944733,\n",
       "   0.17631088197231293,\n",
       "   -0.35870376229286194,\n",
       "   -0.17862990498542786,\n",
       "   -0.25562846660614014,\n",
       "   0.19948823750019073,\n",
       "   -0.39294442534446716,\n",
       "   -0.4331342279911041,\n",
       "   -0.4303445816040039,\n",
       "   0.5622186064720154,\n",
       "   0.5689758062362671,\n",
       "   -0.7415937781333923,\n",
       "   0.47149303555488586,\n",
       "   0.08371046185493469,\n",
       "   0.5340217351913452,\n",
       "   0.2788611352443695,\n",
       "   -0.09496866911649704,\n",
       "   -0.11151701211929321,\n",
       "   -0.15514247119426727,\n",
       "   -0.43747401237487793,\n",
       "   -0.8668137192726135,\n",
       "   0.7113155126571655,\n",
       "   0.23860448598861694,\n",
       "   0.22847680747509003,\n",
       "   0.5276830196380615,\n",
       "   0.5207661390304565,\n",
       "   -0.1439610719680786,\n",
       "   0.17295025289058685,\n",
       "   0.10046222805976868,\n",
       "   -0.21848662197589874,\n",
       "   -0.052546869963407516,\n",
       "   -0.22555430233478546,\n",
       "   0.28051114082336426,\n",
       "   0.5794097185134888,\n",
       "   0.0998477190732956,\n",
       "   0.22698980569839478,\n",
       "   -0.6186580657958984,\n",
       "   0.2805478870868683,\n",
       "   0.06699599325656891,\n",
       "   0.6560162901878357,\n",
       "   -0.10874070972204208,\n",
       "   0.2053755670785904,\n",
       "   -0.3896101713180542,\n",
       "   -0.0402962788939476,\n",
       "   -0.42956823110580444,\n",
       "   0.018299683928489685,\n",
       "   -0.015278697945177555,\n",
       "   0.06062784418463707,\n",
       "   -0.10542839020490646,\n",
       "   0.3536381125450134,\n",
       "   -0.4453757107257843,\n",
       "   -0.3260812759399414,\n",
       "   0.19720487296581268,\n",
       "   0.1197124719619751,\n",
       "   0.09464415162801743,\n",
       "   -0.016126351431012154,\n",
       "   -0.1723378300666809,\n",
       "   -0.0681115984916687,\n",
       "   0.20292840898036957,\n",
       "   -0.01877520978450775,\n",
       "   -0.4276677966117859,\n",
       "   0.7741921544075012,\n",
       "   0.4600662887096405,\n",
       "   -0.07403792440891266,\n",
       "   -0.3844037652015686,\n",
       "   -0.6232851147651672,\n",
       "   0.4076051712036133,\n",
       "   -0.4934273958206177,\n",
       "   -0.2842552959918976,\n",
       "   -0.3611731231212616,\n",
       "   -0.05005578324198723,\n",
       "   -0.11414413154125214,\n",
       "   -0.2465551495552063,\n",
       "   0.11512147635221481,\n",
       "   -0.4298475682735443,\n",
       "   0.529678463935852,\n",
       "   0.04383759945631027,\n",
       "   -0.06048700213432312,\n",
       "   -0.019612090662121773,\n",
       "   0.3970106542110443,\n",
       "   0.43416908383369446,\n",
       "   -0.17765632271766663,\n",
       "   -0.4391702711582184,\n",
       "   -0.3043602406978607,\n",
       "   0.32540634274482727,\n",
       "   0.5063287615776062,\n",
       "   0.15033049881458282,\n",
       "   0.6072776317596436,\n",
       "   0.011618806049227715,\n",
       "   0.030567370355129242,\n",
       "   -0.5295039415359497,\n",
       "   0.5136765837669373,\n",
       "   0.28676021099090576,\n",
       "   0.845704197883606,\n",
       "   0.4255841076374054,\n",
       "   -0.12779255211353302,\n",
       "   0.32173243165016174,\n",
       "   0.1889229714870453,\n",
       "   -0.11684355139732361,\n",
       "   -0.21690356731414795,\n",
       "   -0.1660289317369461,\n",
       "   -0.3157050907611847,\n",
       "   -0.47001922130584717,\n",
       "   -0.47443675994873047,\n",
       "   -0.3219752013683319,\n",
       "   0.2920232117176056,\n",
       "   0.9017189145088196,\n",
       "   0.12796005606651306,\n",
       "   0.12219741195440292,\n",
       "   0.20404089987277985,\n",
       "   0.4134376347064972,\n",
       "   -0.05320663005113602,\n",
       "   0.3226369321346283,\n",
       "   0.17612668871879578,\n",
       "   -0.14216075837612152,\n",
       "   0.4350404441356659,\n",
       "   0.004194941837340593,\n",
       "   0.06537985801696777,\n",
       "   -0.110076904296875,\n",
       "   0.1353948563337326,\n",
       "   -0.030017895624041557,\n",
       "   -0.10725226253271103,\n",
       "   0.9718601107597351,\n",
       "   -0.060901619493961334,\n",
       "   0.21223518252372742,\n",
       "   0.18304267525672913,\n",
       "   0.16434596478939056,\n",
       "   -0.15211188793182373,\n",
       "   -0.28180694580078125,\n",
       "   0.12234915792942047,\n",
       "   -0.6682170629501343,\n",
       "   -0.5247051119804382,\n",
       "   -0.06638255715370178,\n",
       "   -0.11201822012662888,\n",
       "   0.3874099552631378,\n",
       "   0.21633346378803253,\n",
       "   0.36544862389564514,\n",
       "   0.0086201261729002,\n",
       "   -0.3068380057811737,\n",
       "   0.27099815011024475,\n",
       "   -0.3368167281150818,\n",
       "   -0.3318994641304016,\n",
       "   -0.03968732804059982,\n",
       "   -0.09623866528272629,\n",
       "   0.4754258990287781,\n",
       "   -0.24192777276039124,\n",
       "   -0.02896881476044655,\n",
       "   -0.25039976835250854,\n",
       "   -0.519338071346283,\n",
       "   -0.09433294832706451,\n",
       "   0.08773963898420334,\n",
       "   -0.14761321246623993,\n",
       "   -0.1388455182313919,\n",
       "   0.15040245652198792,\n",
       "   -0.20549257099628448,\n",
       "   -0.15395481884479523,\n",
       "   -0.5282682776451111,\n",
       "   -0.3748660683631897,\n",
       "   -0.020211659371852875,\n",
       "   0.5675986409187317,\n",
       "   -0.2096235454082489,\n",
       "   -0.17298907041549683,\n",
       "   -0.2957685887813568,\n",
       "   -0.5375962257385254,\n",
       "   0.1096683144569397,\n",
       "   0.3378791809082031,\n",
       "   0.41288357973098755,\n",
       "   -0.38690808415412903,\n",
       "   -0.38926172256469727,\n",
       "   -0.4302489459514618,\n",
       "   0.2850136160850525,\n",
       "   -0.11109953373670578,\n",
       "   -0.180365189909935,\n",
       "   0.14165160059928894,\n",
       "   0.13072507083415985,\n",
       "   0.008704173378646374,\n",
       "   0.1414642035961151,\n",
       "   0.3455688953399658,\n",
       "   -0.0028864929918199778,\n",
       "   0.07913029938936234,\n",
       "   -0.8071612119674683,\n",
       "   0.13351579010486603,\n",
       "   0.3262891173362732,\n",
       "   0.009851692244410515,\n",
       "   0.1400601714849472,\n",
       "   0.22691918909549713,\n",
       "   0.026896938681602478,\n",
       "   -0.07606089860200882,\n",
       "   -0.2973065674304962,\n",
       "   0.26456475257873535,\n",
       "   0.37234485149383545,\n",
       "   0.15931253135204315,\n",
       "   0.2342642992734909,\n",
       "   -0.10927195847034454,\n",
       "   -0.179133802652359,\n",
       "   -1.0576956272125244,\n",
       "   -0.201064333319664,\n",
       "   0.16288097202777863,\n",
       "   0.2247885912656784,\n",
       "   0.2396276593208313,\n",
       "   0.4226126968860626,\n",
       "   -0.05976902320981026,\n",
       "   0.03742929548025131,\n",
       "   -0.14040400087833405,\n",
       "   0.4226800799369812,\n",
       "   0.0120309479534626,\n",
       "   0.06350351870059967,\n",
       "   -0.6553404331207275,\n",
       "   -0.21470907330513,\n",
       "   -0.09395330399274826,\n",
       "   0.19316478073596954,\n",
       "   -0.1602218747138977,\n",
       "   -0.9528687596321106,\n",
       "   0.3430088758468628,\n",
       "   -0.10820529609918594,\n",
       "   0.19179250299930573,\n",
       "   0.2833159267902374,\n",
       "   0.14835888147354126,\n",
       "   0.3826119899749756,\n",
       "   -0.04849348962306976,\n",
       "   0.0996408686041832,\n",
       "   0.23071043193340302,\n",
       "   0.318734347820282,\n",
       "   0.02529110200703144,\n",
       "   0.7659029364585876,\n",
       "   0.346758633852005,\n",
       "   0.17333132028579712,\n",
       "   -0.44996777176856995,\n",
       "   0.00789703894406557,\n",
       "   0.6057376265525818,\n",
       "   -0.43923917412757874,\n",
       "   -0.6469764113426208,\n",
       "   -0.721840500831604,\n",
       "   0.24914847314357758,\n",
       "   -0.28670427203178406,\n",
       "   -0.41735023260116577,\n",
       "   0.5088402628898621,\n",
       "   0.09954112768173218,\n",
       "   1.201871395111084,\n",
       "   -0.05984177812933922,\n",
       "   -0.45122525095939636,\n",
       "   0.4939950406551361,\n",
       "   -0.149321511387825,\n",
       "   0.48132258653640747,\n",
       "   0.12358349561691284,\n",
       "   0.1816801279783249,\n",
       "   -0.598360538482666,\n",
       "   -0.1410580277442932,\n",
       "   -0.2880706191062927,\n",
       "   -0.07949594408273697,\n",
       "   -0.17483210563659668,\n",
       "   0.008431315422058105,\n",
       "   -0.15261732041835785,\n",
       "   -0.3572880029678345,\n",
       "   -0.13240264356136322,\n",
       "   0.3406860828399658,\n",
       "   -0.3324263393878937,\n",
       "   0.5240832567214966,\n",
       "   -0.4918457567691803,\n",
       "   0.2479841411113739,\n",
       "   0.13559788465499878,\n",
       "   -0.3383296728134155,\n",
       "   0.24623920023441315,\n",
       "   -0.6132378578186035,\n",
       "   0.29576754570007324,\n",
       "   0.20410628616809845,\n",
       "   0.4670618176460266,\n",
       "   -0.1998487114906311,\n",
       "   0.5938974022865295,\n",
       "   0.31613659858703613,\n",
       "   -0.11597725003957748,\n",
       "   -0.06220335513353348,\n",
       "   0.5259625315666199,\n",
       "   0.044962070882320404,\n",
       "   0.11060699075460434,\n",
       "   -0.4093523621559143,\n",
       "   0.1071198508143425,\n",
       "   0.39742183685302734,\n",
       "   0.8131209015846252,\n",
       "   0.06368783861398697,\n",
       "   0.01479463279247284,\n",
       "   0.1465567648410797,\n",
       "   -0.20495332777500153,\n",
       "   0.5492190718650818,\n",
       "   0.16612784564495087,\n",
       "   -0.379502534866333,\n",
       "   0.9487369060516357,\n",
       "   -0.3462057411670685,\n",
       "   -0.12041090428829193,\n",
       "   0.17074407637119293,\n",
       "   -0.10987558215856552,\n",
       "   0.43470773100852966,\n",
       "   -0.3553902208805084,\n",
       "   0.353230744600296,\n",
       "   0.16970567405223846,\n",
       "   0.23507067561149597,\n",
       "   0.038134437054395676,\n",
       "   1.0640106201171875,\n",
       "   -0.3930937647819519,\n",
       "   0.25800225138664246,\n",
       "   0.08430956304073334,\n",
       "   0.20090585947036743,\n",
       "   0.6940434575080872,\n",
       "   0.20404444634914398,\n",
       "   -0.4077291786670685,\n",
       "   -0.09767670929431915,\n",
       "   0.32291004061698914,\n",
       "   -0.00551535515114665,\n",
       "   -0.44827771186828613,\n",
       "   -8.640649795532227,\n",
       "   0.6765817999839783,\n",
       "   -0.10054373741149902,\n",
       "   -0.02622832916676998,\n",
       "   0.1348450779914856,\n",
       "   0.24147169291973114,\n",
       "   0.17126502096652985,\n",
       "   -0.4797508418560028,\n",
       "   -0.21327760815620422,\n",
       "   -0.3225754201412201,\n",
       "   -0.12264926731586456,\n",
       "   -0.017789632081985474,\n",
       "   0.05406726151704788,\n",
       "   -0.08259548246860504,\n",
       "   0.40824276208877563,\n",
       "   -0.34142303466796875,\n",
       "   0.13753478229045868,\n",
       "   0.09191054105758667,\n",
       "   0.6397037506103516,\n",
       "   -0.36878058314323425,\n",
       "   -0.16832710802555084,\n",
       "   0.029923100024461746,\n",
       "   -0.059797078371047974,\n",
       "   0.7439450621604919,\n",
       "   -0.20347002148628235,\n",
       "   0.870540976524353,\n",
       "   0.355169415473938,\n",
       "   0.140893816947937,\n",
       "   0.44728124141693115,\n",
       "   -0.12661123275756836,\n",
       "   -0.2678867280483246,\n",
       "   0.30265265703201294,\n",
       "   -0.07628726214170456,\n",
       "   -0.1714264303445816,\n",
       "   -0.01335134170949459,\n",
       "   0.05043742060661316,\n",
       "   0.08545892685651779,\n",
       "   -0.21878385543823242,\n",
       "   -0.3192089796066284,\n",
       "   -0.8778052926063538,\n",
       "   0.2384042888879776,\n",
       "   -0.3974699378013611,\n",
       "   -0.09175174683332443,\n",
       "   0.1525871604681015,\n",
       "   0.5591398477554321,\n",
       "   0.3125270903110504,\n",
       "   -0.0305184293538332,\n",
       "   0.6418732404708862,\n",
       "   0.1253783404827118,\n",
       "   0.253182977437973,\n",
       "   0.4925342798233032,\n",
       "   0.40976619720458984,\n",
       "   0.20669744908809662,\n",
       "   0.34848085045814514,\n",
       "   -0.12764936685562134,\n",
       "   0.15885600447654724,\n",
       "   -0.39796510338783264,\n",
       "   0.6651548743247986,\n",
       "   0.08404447138309479,\n",
       "   -0.805137574672699,\n",
       "   -0.15659824013710022,\n",
       "   -0.1741270124912262,\n",
       "   0.6929263472557068,\n",
       "   0.077647365629673,\n",
       "   -0.4472368359565735,\n",
       "   -0.11799940466880798,\n",
       "   -0.42847341299057007,\n",
       "   -0.0752311497926712,\n",
       "   0.0414845310151577,\n",
       "   -0.5630866885185242,\n",
       "   -0.5472521781921387,\n",
       "   -0.2646228075027466,\n",
       "   0.04557161405682564,\n",
       "   -0.3560696244239807,\n",
       "   0.2925111651420593,\n",
       "   -0.6638211011886597,\n",
       "   -0.13172751665115356,\n",
       "   0.10895024985074997,\n",
       "   -0.3436436951160431,\n",
       "   0.3819117844104767,\n",
       "   -0.1835440844297409,\n",
       "   0.34074291586875916,\n",
       "   0.43707847595214844,\n",
       "   -0.6676293611526489,\n",
       "   -0.10450479388237,\n",
       "   0.010519164614379406,\n",
       "   -0.4491729140281677,\n",
       "   0.43528059124946594,\n",
       "   0.25668060779571533,\n",
       "   -0.09171658754348755,\n",
       "   -0.267466276884079,\n",
       "   0.09788937866687775,\n",
       "   -0.0500403456389904,\n",
       "   -0.035083968192338943,\n",
       "   0.07424052804708481,\n",
       "   -0.047038812190294266,\n",
       "   -0.14557476341724396,\n",
       "   0.19091585278511047,\n",
       "   -0.004856735933572054,\n",
       "   0.9004971385002136,\n",
       "   -0.39042842388153076,\n",
       "   -0.20269905030727386,\n",
       "   -0.4289332926273346,\n",
       "   0.0975673496723175,\n",
       "   -0.08464016765356064,\n",
       "   -0.1271873414516449,\n",
       "   0.1118861585855484,\n",
       "   -0.24016354978084564,\n",
       "   -0.0112907188013196,\n",
       "   -0.030286770313978195,\n",
       "   -0.17167574167251587,\n",
       "   0.43239524960517883,\n",
       "   0.7464099526405334,\n",
       "   -0.0299806110560894,\n",
       "   -0.09442992508411407,\n",
       "   0.4587254822254181,\n",
       "   0.38750821352005005,\n",
       "   -0.2126518338918686,\n",
       "   -0.07020820677280426,\n",
       "   -0.2500969171524048,\n",
       "   -0.3435053825378418,\n",
       "   -0.6404616832733154,\n",
       "   0.43854352831840515,\n",
       "   -0.32258498668670654,\n",
       "   0.3257302939891815,\n",
       "   0.4143332540988922,\n",
       "   -0.07656656205654144,\n",
       "   0.1782960444688797,\n",
       "   0.7073308825492859,\n",
       "   -0.04004275053739548,\n",
       "   0.6580305099487305,\n",
       "   0.1721462607383728,\n",
       "   -0.42926833033561707,\n",
       "   0.13447006046772003,\n",
       "   0.17528897523880005,\n",
       "   -0.21409034729003906,\n",
       "   0.0015287246787920594,\n",
       "   -0.1957925409078598,\n",
       "   -0.35962218046188354,\n",
       "   -0.12266480177640915,\n",
       "   -0.6709006428718567,\n",
       "   -0.01388273574411869,\n",
       "   0.01859442889690399,\n",
       "   0.35368701815605164,\n",
       "   0.6433121562004089,\n",
       "   -0.25192341208457947,\n",
       "   0.06898936629295349,\n",
       "   -0.14182935655117035,\n",
       "   -0.5137296915054321,\n",
       "   -0.4543920159339905,\n",
       "   -0.16873805224895477,\n",
       "   0.18941240012645721,\n",
       "   0.6594249606132507,\n",
       "   0.24381373822689056,\n",
       "   0.610326886177063,\n",
       "   -0.29378876090049744,\n",
       "   0.3371628522872925,\n",
       "   1.062077522277832,\n",
       "   -0.3390972316265106,\n",
       "   -0.07685016840696335,\n",
       "   -0.2047090083360672,\n",
       "   -0.09979230910539627,\n",
       "   0.2805849611759186,\n",
       "   0.24900421500205994,\n",
       "   -0.14818871021270752,\n",
       "   0.27832865715026855,\n",
       "   -0.15085239708423615,\n",
       "   0.16574512422084808,\n",
       "   0.10170694440603256,\n",
       "   0.25096434354782104,\n",
       "   0.05835023149847984,\n",
       "   -0.24396756291389465,\n",
       "   0.30049949884414673,\n",
       "   0.17075973749160767,\n",
       "   0.1609552651643753,\n",
       "   -0.25607553124427795,\n",
       "   0.46377459168434143,\n",
       "   0.05866336077451706,\n",
       "   0.2018604874610901,\n",
       "   0.09791447967290878,\n",
       "   0.9664280414581299,\n",
       "   0.5513370037078857,\n",
       "   -0.07509928196668625,\n",
       "   0.029835203662514687,\n",
       "   -0.49791499972343445,\n",
       "   0.034591298550367355,\n",
       "   0.42882248759269714,\n",
       "   0.26169899106025696,\n",
       "   -0.04570026323199272,\n",
       "   -0.10850857198238373,\n",
       "   0.24152621626853943,\n",
       "   0.20134223997592926,\n",
       "   -0.360368549823761,\n",
       "   0.48495882749557495,\n",
       "   -0.128645658493042,\n",
       "   0.2094232589006424,\n",
       "   -0.2292463332414627,\n",
       "   -0.35657358169555664,\n",
       "   0.14567652344703674,\n",
       "   -0.014473509974777699,\n",
       "   -0.08789680898189545,\n",
       "   -0.14937171339988708,\n",
       "   0.3680245280265808,\n",
       "   -0.32440996170043945,\n",
       "   -0.32569894194602966,\n",
       "   0.4131490886211395,\n",
       "   0.2458488494157791,\n",
       "   -0.16232828795909882,\n",
       "   0.44301021099090576,\n",
       "   0.21592524647712708,\n",
       "   0.28708648681640625],\n",
       "  [0.2704889476299286,\n",
       "   0.15668247640132904,\n",
       "   -0.2153887152671814,\n",
       "   -0.16829915344715118,\n",
       "   0.6598246097564697,\n",
       "   -0.24779370427131653,\n",
       "   0.3411738872528076,\n",
       "   0.015212522819638252,\n",
       "   -0.1518065184354782,\n",
       "   0.26221147179603577,\n",
       "   0.005026241764426231,\n",
       "   0.3260757029056549,\n",
       "   -0.17043791711330414,\n",
       "   0.139326810836792,\n",
       "   -0.8387947678565979,\n",
       "   0.037361990660429,\n",
       "   0.28407010436058044,\n",
       "   0.00560144055634737,\n",
       "   -0.025563346222043037,\n",
       "   0.15819956362247467,\n",
       "   0.07068715989589691,\n",
       "   -0.15886607766151428,\n",
       "   0.24196262657642365,\n",
       "   0.05266612395644188,\n",
       "   0.18224596977233887,\n",
       "   0.2875429391860962,\n",
       "   0.32672011852264404,\n",
       "   0.8491678237915039,\n",
       "   -0.17913727462291718,\n",
       "   0.5304561853408813,\n",
       "   0.3548372983932495,\n",
       "   0.2133607119321823,\n",
       "   0.31977447867393494,\n",
       "   -0.02487308159470558,\n",
       "   -0.11033979058265686,\n",
       "   0.038182344287633896,\n",
       "   -0.19125668704509735,\n",
       "   0.29719629883766174,\n",
       "   -0.23124344646930695,\n",
       "   0.12048658728599548,\n",
       "   -0.040319304913282394,\n",
       "   -0.11449286341667175,\n",
       "   -0.023995760828256607,\n",
       "   0.0030198157764971256,\n",
       "   -0.01795741729438305,\n",
       "   -0.1335994005203247,\n",
       "   -0.32513466477394104,\n",
       "   -0.1704348474740982,\n",
       "   0.15771712362766266,\n",
       "   0.326756089925766,\n",
       "   -0.008189165964722633,\n",
       "   -0.5808809399604797,\n",
       "   -0.24822139739990234,\n",
       "   -0.40274736285209656,\n",
       "   -0.06865229457616806,\n",
       "   -0.16874296963214874,\n",
       "   -0.0412340983748436,\n",
       "   -0.17978893220424652,\n",
       "   -0.5146219730377197,\n",
       "   0.3718053102493286,\n",
       "   0.1245669424533844,\n",
       "   0.1685236543416977,\n",
       "   0.6325781345367432,\n",
       "   0.1171962171792984,\n",
       "   -0.18686485290527344,\n",
       "   0.2513580322265625,\n",
       "   0.0014545372687280178,\n",
       "   -0.12507806718349457,\n",
       "   0.19940754771232605,\n",
       "   -0.12600548565387726,\n",
       "   0.00952825602144003,\n",
       "   0.021739287301898003,\n",
       "   -0.03098280355334282,\n",
       "   -0.3972287178039551,\n",
       "   0.4722898304462433,\n",
       "   0.0005989605560898781,\n",
       "   0.28818100690841675,\n",
       "   -0.34668952226638794,\n",
       "   -0.35326316952705383,\n",
       "   -0.03210323676466942,\n",
       "   0.4062969982624054,\n",
       "   0.270419180393219,\n",
       "   0.09890886396169662,\n",
       "   0.31415268778800964,\n",
       "   0.25025415420532227,\n",
       "   -0.010308188386261463,\n",
       "   0.16103729605674744,\n",
       "   0.198465034365654,\n",
       "   0.15697549283504486,\n",
       "   0.34240204095840454,\n",
       "   -0.098833829164505,\n",
       "   -0.24419917166233063,\n",
       "   0.06396036595106125,\n",
       "   0.16316260397434235,\n",
       "   -0.4174017608165741,\n",
       "   0.2406662404537201,\n",
       "   -0.42625200748443604,\n",
       "   0.2227400839328766,\n",
       "   -0.037530567497015,\n",
       "   0.04393408074975014,\n",
       "   -0.13467468321323395,\n",
       "   -0.19136430323123932,\n",
       "   0.27775463461875916,\n",
       "   -0.13864590227603912,\n",
       "   0.15246300399303436,\n",
       "   -0.27373993396759033,\n",
       "   -0.506130039691925,\n",
       "   -0.7519996762275696,\n",
       "   -0.5665064454078674,\n",
       "   0.3571268618106842,\n",
       "   0.053915489464998245,\n",
       "   0.6686203479766846,\n",
       "   0.4017813503742218,\n",
       "   -0.03640752285718918,\n",
       "   -0.49917852878570557,\n",
       "   -0.12251254916191101,\n",
       "   0.1518193930387497,\n",
       "   0.3484559655189514,\n",
       "   -0.0402364544570446,\n",
       "   -0.39091309905052185,\n",
       "   0.41839998960494995,\n",
       "   0.3448886275291443,\n",
       "   -0.025989562273025513,\n",
       "   -0.22647346556186676,\n",
       "   0.02788335643708706,\n",
       "   -0.0025204848498106003,\n",
       "   -0.1804356873035431,\n",
       "   -0.14192986488342285,\n",
       "   0.23908871412277222,\n",
       "   0.1396646350622177,\n",
       "   -0.2786042392253876,\n",
       "   -0.41809341311454773,\n",
       "   -0.005834176670759916,\n",
       "   0.45473477244377136,\n",
       "   0.048731546849012375,\n",
       "   0.12085478007793427,\n",
       "   -0.3297216594219208,\n",
       "   0.03415306657552719,\n",
       "   -0.9670760035514832,\n",
       "   0.0758485496044159,\n",
       "   -0.44971489906311035,\n",
       "   0.10873784124851227,\n",
       "   0.4678407907485962,\n",
       "   -0.16284000873565674,\n",
       "   -0.04060185328125954,\n",
       "   0.08491221815347672,\n",
       "   -0.20084325969219208,\n",
       "   -0.09557747840881348,\n",
       "   0.39163705706596375,\n",
       "   0.29938575625419617,\n",
       "   -0.3853534162044525,\n",
       "   0.18919925391674042,\n",
       "   0.04875367134809494,\n",
       "   0.06813329458236694,\n",
       "   -0.29731571674346924,\n",
       "   -0.11096112430095673,\n",
       "   0.2712804675102234,\n",
       "   -0.7036794424057007,\n",
       "   0.8165761828422546,\n",
       "   0.24340419471263885,\n",
       "   -0.2943013906478882,\n",
       "   -0.45955392718315125,\n",
       "   -0.4243870675563812,\n",
       "   0.36391106247901917,\n",
       "   0.2537536323070526,\n",
       "   -0.6121054291725159,\n",
       "   -0.18234913051128387,\n",
       "   0.2944420874118805,\n",
       "   -0.11774710565805435,\n",
       "   0.6756159067153931,\n",
       "   0.48746389150619507,\n",
       "   0.23089942336082458,\n",
       "   -0.2377322018146515,\n",
       "   -0.0625142902135849,\n",
       "   0.16241177916526794,\n",
       "   0.3346039652824402,\n",
       "   0.25556766986846924,\n",
       "   -0.24654445052146912,\n",
       "   0.3070126473903656,\n",
       "   0.049567487090826035,\n",
       "   0.050376735627651215,\n",
       "   -0.04486865550279617,\n",
       "   -0.27035513520240784,\n",
       "   -0.21052207052707672,\n",
       "   -0.13134127855300903,\n",
       "   -0.23428784310817719,\n",
       "   0.14279904961585999,\n",
       "   -0.7091220617294312,\n",
       "   0.6887485384941101,\n",
       "   -0.07793447375297546,\n",
       "   0.07293085753917694,\n",
       "   -0.3115408718585968,\n",
       "   -0.717670738697052,\n",
       "   -0.10344763100147247,\n",
       "   -0.8400517106056213,\n",
       "   0.17481452226638794,\n",
       "   0.3153996765613556,\n",
       "   0.20919623970985413,\n",
       "   0.07392917573451996,\n",
       "   0.7409327626228333,\n",
       "   -0.16798056662082672,\n",
       "   0.11457144469022751,\n",
       "   -0.06842231005430222,\n",
       "   0.0718371570110321,\n",
       "   0.05610329285264015,\n",
       "   0.021548934280872345,\n",
       "   -0.360417902469635,\n",
       "   0.5176117420196533,\n",
       "   -0.18345887959003448,\n",
       "   -0.41684290766716003,\n",
       "   -0.1785299926996231,\n",
       "   -0.3432013690471649,\n",
       "   0.10425794869661331,\n",
       "   0.0008237175643444061,\n",
       "   0.3267163038253784,\n",
       "   -0.3826325833797455,\n",
       "   0.179361954331398,\n",
       "   -0.3918711245059967,\n",
       "   0.17378774285316467,\n",
       "   0.08455339819192886,\n",
       "   -0.38394588232040405,\n",
       "   -0.02222101017832756,\n",
       "   0.6806154251098633,\n",
       "   -0.1360493153333664,\n",
       "   -0.0350000374019146,\n",
       "   0.7893771529197693,\n",
       "   -0.4820170998573303,\n",
       "   0.44465172290802,\n",
       "   0.46896684169769287,\n",
       "   -0.015163595788180828,\n",
       "   0.19094829261302948,\n",
       "   0.2803399860858917,\n",
       "   -0.026473771780729294,\n",
       "   -0.3884856104850769,\n",
       "   -0.28053560853004456,\n",
       "   0.5652730464935303,\n",
       "   -0.1907442957162857,\n",
       "   -0.1835092157125473,\n",
       "   -0.26253101229667664,\n",
       "   -0.5773026347160339,\n",
       "   0.17415229976177216,\n",
       "   -0.005111666861921549,\n",
       "   0.6079858541488647,\n",
       "   -0.05640947446227074,\n",
       "   -0.17008338868618011,\n",
       "   -0.11082789301872253,\n",
       "   -0.11141204088926315,\n",
       "   0.4051688015460968,\n",
       "   -0.11774919182062149,\n",
       "   0.24378569424152374,\n",
       "   -0.32409197092056274,\n",
       "   -0.07126985490322113,\n",
       "   -0.38785940408706665,\n",
       "   -0.054103437811136246,\n",
       "   0.23277157545089722,\n",
       "   -0.2181437462568283,\n",
       "   0.5615978240966797,\n",
       "   -0.1110885813832283,\n",
       "   0.05532711371779442,\n",
       "   -0.5107294917106628,\n",
       "   0.5725885629653931,\n",
       "   0.013936566188931465,\n",
       "   0.2869586944580078,\n",
       "   -0.16210471093654633,\n",
       "   -0.07881994545459747,\n",
       "   -0.11265065521001816,\n",
       "   -0.48599889874458313,\n",
       "   -0.04930417239665985,\n",
       "   -0.5126753449440002,\n",
       "   0.4313804507255554,\n",
       "   0.0945461243391037,\n",
       "   -0.09012671560049057,\n",
       "   0.3865426480770111,\n",
       "   0.1449030041694641,\n",
       "   -0.4029700756072998,\n",
       "   0.5476502776145935,\n",
       "   0.3105768859386444,\n",
       "   -0.4348994493484497,\n",
       "   0.1393844336271286,\n",
       "   -0.10260443389415741,\n",
       "   0.35837996006011963,\n",
       "   0.6478508114814758,\n",
       "   -0.3321205675601959,\n",
       "   0.5317214131355286,\n",
       "   -0.5623787641525269,\n",
       "   -0.5293629169464111,\n",
       "   0.1615428775548935,\n",
       "   0.7328404784202576,\n",
       "   0.14152486622333527,\n",
       "   -0.11595278978347778,\n",
       "   0.04774980619549751,\n",
       "   -0.33966293931007385,\n",
       "   -0.2776387333869934,\n",
       "   0.5422871112823486,\n",
       "   0.3527224361896515,\n",
       "   0.0642392635345459,\n",
       "   -0.32937929034233093,\n",
       "   0.5074755549430847,\n",
       "   -0.4263397753238678,\n",
       "   -0.27202558517456055,\n",
       "   0.20316500961780548,\n",
       "   -0.025758793577551842,\n",
       "   0.3303124010562897,\n",
       "   0.04340140521526337,\n",
       "   -0.6946113705635071,\n",
       "   0.6312724351882935,\n",
       "   0.10478426516056061,\n",
       "   0.22381411492824554,\n",
       "   0.2445371448993683,\n",
       "   0.49006298184394836,\n",
       "   0.4695615768432617,\n",
       "   0.22519302368164062,\n",
       "   -0.1017337441444397,\n",
       "   -0.0334504209458828,\n",
       "   -0.14090320467948914,\n",
       "   -0.3817378580570221,\n",
       "   -0.049033552408218384,\n",
       "   0.012665275484323502,\n",
       "   0.000960549630690366,\n",
       "   0.030626872554421425,\n",
       "   -0.17692925035953522,\n",
       "   0.4990820288658142,\n",
       "   -0.4158220887184143,\n",
       "   0.2243415117263794,\n",
       "   -0.056174103170633316,\n",
       "   0.13888537883758545,\n",
       "   -0.09369771182537079,\n",
       "   0.487576425075531,\n",
       "   -0.1011454313993454,\n",
       "   -0.22522181272506714,\n",
       "   -0.10824248194694519,\n",
       "   -0.1073322743177414,\n",
       "   -0.10540497303009033,\n",
       "   0.3956376016139984,\n",
       "   -0.11213947087526321,\n",
       "   0.06164199858903885,\n",
       "   0.024327153339982033,\n",
       "   0.47014349699020386,\n",
       "   -0.54887455701828,\n",
       "   0.25769609212875366,\n",
       "   0.015597088262438774,\n",
       "   -0.3209717869758606,\n",
       "   0.11687382310628891,\n",
       "   -0.3522024154663086,\n",
       "   -0.11630947887897491,\n",
       "   0.3112383782863617,\n",
       "   0.392522394657135,\n",
       "   0.036317128688097,\n",
       "   0.30670297145843506,\n",
       "   -0.23669525980949402,\n",
       "   -0.3549075126647949,\n",
       "   0.08126794546842575,\n",
       "   -0.17264558374881744,\n",
       "   -0.2919340133666992,\n",
       "   0.3777898848056793,\n",
       "   0.4306502640247345,\n",
       "   0.06798199564218521,\n",
       "   0.09067801386117935,\n",
       "   0.18065939843654633,\n",
       "   -0.12497560679912567,\n",
       "   -0.18020503222942352,\n",
       "   -0.2707485556602478,\n",
       "   0.05156555399298668,\n",
       "   -0.09962451457977295,\n",
       "   -0.20803344249725342,\n",
       "   -0.057090021669864655,\n",
       "   -0.5023213028907776,\n",
       "   -0.29205816984176636,\n",
       "   0.06015939265489578,\n",
       "   0.234599307179451,\n",
       "   0.5431785583496094,\n",
       "   -0.36660662293434143,\n",
       "   0.5095207691192627,\n",
       "   0.32620367407798767,\n",
       "   0.3090536296367645,\n",
       "   -0.036784738302230835,\n",
       "   -0.02421312965452671,\n",
       "   0.26306846737861633,\n",
       "   0.1829446256160736,\n",
       "   -0.8472190499305725,\n",
       "   0.186133474111557,\n",
       "   0.010790040716528893,\n",
       "   -0.19137948751449585,\n",
       "   0.0486258938908577,\n",
       "   0.220327228307724,\n",
       "   -0.07183411717414856,\n",
       "   0.05826638638973236,\n",
       "   -0.30758601427078247,\n",
       "   0.5217297077178955,\n",
       "   -0.29008084535598755,\n",
       "   0.2820420265197754,\n",
       "   -0.05290282517671585,\n",
       "   0.2970668077468872,\n",
       "   -0.3682681918144226,\n",
       "   0.17399181425571442,\n",
       "   0.03266333043575287,\n",
       "   -0.6018847227096558,\n",
       "   0.5473198890686035,\n",
       "   0.2560289800167084,\n",
       "   -0.34285053610801697,\n",
       "   0.16273319721221924,\n",
       "   0.25029075145721436,\n",
       "   -0.21448509395122528,\n",
       "   0.2665659487247467,\n",
       "   -0.6228357553482056,\n",
       "   -0.32255426049232483,\n",
       "   -0.016143715009093285,\n",
       "   0.4270680248737335,\n",
       "   0.01388412807136774,\n",
       "   -0.9283338785171509,\n",
       "   0.054453130811452866,\n",
       "   -0.44055864214897156,\n",
       "   0.28376704454421997,\n",
       "   0.8677456974983215,\n",
       "   0.18354617059230804,\n",
       "   -0.1533302217721939,\n",
       "   0.1767892837524414,\n",
       "   -0.2131185382604599,\n",
       "   0.02845628745853901,\n",
       "   0.22995446622371674,\n",
       "   -0.16822481155395508,\n",
       "   -0.4777422845363617,\n",
       "   -0.250520795583725,\n",
       "   -0.24041494727134705,\n",
       "   0.44111722707748413,\n",
       "   0.20260195434093475,\n",
       "   -0.06036439165472984,\n",
       "   -0.24747222661972046,\n",
       "   -0.21246154606342316,\n",
       "   -0.003535506082698703,\n",
       "   -0.06224381923675537,\n",
       "   -0.07603633403778076,\n",
       "   0.06815021485090256,\n",
       "   0.36106300354003906,\n",
       "   -0.019025688990950584,\n",
       "   0.23208877444267273,\n",
       "   -0.05716850608587265,\n",
       "   0.18874676525592804,\n",
       "   0.05222014710307121,\n",
       "   -0.28047236800193787,\n",
       "   0.18863433599472046,\n",
       "   -0.549224853515625,\n",
       "   0.053044483065605164,\n",
       "   -0.8842458128929138,\n",
       "   0.22142447531223297,\n",
       "   -0.26619693636894226,\n",
       "   -0.17663221061229706,\n",
       "   0.24325689673423767,\n",
       "   -0.1293054074048996,\n",
       "   -0.11929602921009064,\n",
       "   0.4341115355491638,\n",
       "   0.0916479080915451,\n",
       "   0.3453882336616516,\n",
       "   0.20361311733722687,\n",
       "   0.3434421122074127,\n",
       "   0.0810461938381195,\n",
       "   -0.36214157938957214,\n",
       "   -0.1348237693309784,\n",
       "   -0.6124112606048584,\n",
       "   0.167785182595253,\n",
       "   -0.24922029674053192,\n",
       "   0.45567265152931213,\n",
       "   -2.0868703359155916e-05,\n",
       "   0.26468098163604736,\n",
       "   0.2109948694705963,\n",
       "   0.32898786664009094,\n",
       "   -0.29379820823669434,\n",
       "   0.7859092354774475,\n",
       "   -0.06815652549266815,\n",
       "   0.26211485266685486,\n",
       "   0.0964188501238823,\n",
       "   0.367999404668808,\n",
       "   0.1328367292881012,\n",
       "   0.9933961033821106,\n",
       "   -0.037785325199365616,\n",
       "   0.3191249370574951,\n",
       "   0.17931856215000153,\n",
       "   -0.19380763173103333,\n",
       "   -0.3432970345020294,\n",
       "   0.22006341814994812,\n",
       "   -0.3294247090816498,\n",
       "   0.16755303740501404,\n",
       "   -0.3149675726890564,\n",
       "   0.03364807739853859,\n",
       "   0.09576788544654846,\n",
       "   0.2709934413433075,\n",
       "   0.9067677855491638,\n",
       "   0.24268923699855804,\n",
       "   0.11451949179172516,\n",
       "   0.41060933470726013,\n",
       "   0.2744537889957428,\n",
       "   0.10705813765525818,\n",
       "   -0.03656598553061485,\n",
       "   -0.22545558214187622,\n",
       "   0.20897848904132843,\n",
       "   0.49156850576400757,\n",
       "   0.08622284233570099,\n",
       "   0.22100041806697845,\n",
       "   0.4244522750377655,\n",
       "   0.138108029961586,\n",
       "   -0.29599282145500183,\n",
       "   0.0407148115336895,\n",
       "   -0.13757918775081635,\n",
       "   0.16460302472114563,\n",
       "   -0.6933708786964417,\n",
       "   0.46627819538116455,\n",
       "   -0.02891026809811592,\n",
       "   0.06575154513120651,\n",
       "   -0.032426830381155014,\n",
       "   0.0625639334321022,\n",
       "   -0.1314079314470291,\n",
       "   -0.5594700574874878,\n",
       "   -0.3002561926841736,\n",
       "   -0.19596563279628754,\n",
       "   -0.13473699986934662,\n",
       "   0.1879432201385498,\n",
       "   0.4960442781448364,\n",
       "   0.10643762350082397,\n",
       "   0.1251993179321289,\n",
       "   -0.03961003199219704,\n",
       "   0.39036285877227783,\n",
       "   0.10821372270584106,\n",
       "   -0.17056860029697418,\n",
       "   0.08031828701496124,\n",
       "   0.6037555932998657,\n",
       "   -0.25391656160354614,\n",
       "   0.4437231421470642,\n",
       "   0.26769310235977173,\n",
       "   -0.16261059045791626,\n",
       "   -0.018006790429353714,\n",
       "   -0.31490811705589294,\n",
       "   -0.09894699603319168,\n",
       "   0.13392440974712372,\n",
       "   -0.37458536028862,\n",
       "   1.1765953302383423,\n",
       "   0.22717474400997162,\n",
       "   0.13422493636608124,\n",
       "   -0.14145690202713013,\n",
       "   -0.05257532745599747,\n",
       "   -0.12496048957109451,\n",
       "   0.0035108451265841722,\n",
       "   0.7553290724754333,\n",
       "   0.10820457339286804,\n",
       "   0.31130850315093994,\n",
       "   0.007498952094465494,\n",
       "   0.4806959629058838,\n",
       "   0.10182805359363556,\n",
       "   0.5311691164970398,\n",
       "   0.06917557865381241,\n",
       "   0.13883495330810547,\n",
       "   0.35668236017227173,\n",
       "   0.40600594878196716,\n",
       "   -0.770128607749939,\n",
       "   0.43623143434524536,\n",
       "   0.6099736094474792,\n",
       "   0.5601495504379272,\n",
       "   -0.5176160931587219,\n",
       "   -8.882248878479004,\n",
       "   0.08591821044683456,\n",
       "   0.19002850353717804,\n",
       "   0.0005194841069169343,\n",
       "   0.06014154478907585,\n",
       "   0.1513289362192154,\n",
       "   0.3351224362850189,\n",
       "   -0.2123318463563919,\n",
       "   0.3100106418132782,\n",
       "   0.16984961926937103,\n",
       "   -0.09123275429010391,\n",
       "   -0.16460010409355164,\n",
       "   0.49757620692253113,\n",
       "   -0.5497261881828308,\n",
       "   0.03979503735899925,\n",
       "   0.0012767650187015533,\n",
       "   -0.14203664660453796,\n",
       "   -0.06944096088409424,\n",
       "   0.16114649176597595,\n",
       "   -0.2232377827167511,\n",
       "   -0.6192087531089783,\n",
       "   -0.029661640524864197,\n",
       "   0.1993042528629303,\n",
       "   0.2560461163520813,\n",
       "   0.0004924911772832274,\n",
       "   0.23917840421199799,\n",
       "   -0.2522030174732208,\n",
       "   -0.1969243884086609,\n",
       "   -0.06634727120399475,\n",
       "   -0.3731299042701721,\n",
       "   -0.2408735603094101,\n",
       "   -0.2192092090845108,\n",
       "   -0.1971619725227356,\n",
       "   -0.015186400152742863,\n",
       "   0.04840531945228577,\n",
       "   -0.2503749132156372,\n",
       "   0.10146410018205643,\n",
       "   0.1702396124601364,\n",
       "   -0.1420924961566925,\n",
       "   -0.3756294250488281,\n",
       "   -0.23042598366737366,\n",
       "   0.16234353184700012,\n",
       "   0.43024885654449463,\n",
       "   -0.10344850271940231,\n",
       "   0.19775846600532532,\n",
       "   -0.1123105064034462,\n",
       "   0.4223661422729492,\n",
       "   0.23827427625656128,\n",
       "   0.08778656274080276,\n",
       "   0.4836828112602234,\n",
       "   0.5206186175346375,\n",
       "   0.3055424392223358,\n",
       "   -0.06565047800540924,\n",
       "   -0.16468872129917145,\n",
       "   -0.5692195892333984,\n",
       "   -0.1138538047671318,\n",
       "   -0.13268141448497772,\n",
       "   0.38718849420547485,\n",
       "   -0.3004985451698303,\n",
       "   0.0009194446029141545,\n",
       "   -0.1831485629081726,\n",
       "   -0.40699461102485657,\n",
       "   0.40062132477760315,\n",
       "   -0.328035831451416,\n",
       "   -0.3620864748954773,\n",
       "   0.06971778720617294,\n",
       "   -0.6421963572502136,\n",
       "   0.14855162799358368,\n",
       "   0.06987299025058746,\n",
       "   -0.08107948303222656,\n",
       "   -0.5309675335884094,\n",
       "   -0.5628449320793152,\n",
       "   -0.17616555094718933,\n",
       "   0.2976261377334595,\n",
       "   -0.04058808088302612,\n",
       "   -0.48409074544906616,\n",
       "   -0.6053868532180786,\n",
       "   0.15387150645256042,\n",
       "   -0.31440335512161255,\n",
       "   0.01919567584991455,\n",
       "   -0.8628644943237305,\n",
       "   0.8346052765846252,\n",
       "   0.5519157648086548,\n",
       "   0.0058332267217338085,\n",
       "   -0.0809682235121727,\n",
       "   -0.32482481002807617,\n",
       "   -0.15598852932453156,\n",
       "   0.15913932025432587,\n",
       "   -0.0631977990269661,\n",
       "   0.34587880969047546,\n",
       "   0.0007768519571982324,\n",
       "   -0.3449370265007019,\n",
       "   -0.19807736575603485,\n",
       "   -0.05000721663236618,\n",
       "   -0.11227430403232574,\n",
       "   0.05673510581254959,\n",
       "   -0.3678137958049774,\n",
       "   -0.33050432801246643,\n",
       "   0.3967895805835724,\n",
       "   -0.3602331876754761,\n",
       "   -0.6372107863426208,\n",
       "   -0.12956321239471436,\n",
       "   -0.262824147939682,\n",
       "   0.1033247634768486,\n",
       "   -0.20214836299419403,\n",
       "   -0.39921388030052185,\n",
       "   0.3106655776500702,\n",
       "   -0.4703994691371918,\n",
       "   0.005400031339377165,\n",
       "   -0.160225972533226,\n",
       "   0.12886947393417358,\n",
       "   0.26433122158050537,\n",
       "   0.4062960147857666,\n",
       "   -0.5570682883262634,\n",
       "   -0.2315567582845688,\n",
       "   0.042844876646995544,\n",
       "   0.9930727481842041,\n",
       "   0.010312853381037712,\n",
       "   0.05796217545866966,\n",
       "   -0.05208451300859451,\n",
       "   -0.07781332731246948,\n",
       "   -0.5032780766487122,\n",
       "   0.2482862025499344,\n",
       "   0.3395996689796448,\n",
       "   0.16209392249584198,\n",
       "   0.054845307022333145,\n",
       "   0.04987610504031181,\n",
       "   0.05022093653678894,\n",
       "   0.09724532067775726,\n",
       "   -0.14259417355060577,\n",
       "   0.5513696670532227,\n",
       "   0.011149817146360874,\n",
       "   -0.011863557621836662,\n",
       "   0.3522879183292389,\n",
       "   0.23027729988098145,\n",
       "   -0.27351298928260803,\n",
       "   0.04976154863834381,\n",
       "   -0.39139828085899353,\n",
       "   -0.30829301476478577,\n",
       "   -0.10738871991634369,\n",
       "   -0.5475698113441467,\n",
       "   -0.04023747518658638,\n",
       "   0.415730744600296,\n",
       "   0.06600765883922577,\n",
       "   0.2638891637325287,\n",
       "   -0.47503671050071716,\n",
       "   -0.05310150235891342,\n",
       "   0.13399085402488708,\n",
       "   -0.1496351659297943,\n",
       "   0.12503576278686523,\n",
       "   -0.4453999996185303,\n",
       "   -0.020008575171232224,\n",
       "   0.10009928792715073,\n",
       "   -0.08962807804346085,\n",
       "   0.28157681226730347,\n",
       "   -0.1435977667570114,\n",
       "   0.19875967502593994,\n",
       "   0.0633322149515152,\n",
       "   -0.23100945353507996,\n",
       "   0.0521077997982502,\n",
       "   -0.3070213496685028,\n",
       "   0.12845639884471893,\n",
       "   -0.019365107640624046,\n",
       "   0.10840237885713577,\n",
       "   -0.2854316234588623,\n",
       "   0.09791962802410126,\n",
       "   -0.39573773741722107,\n",
       "   0.3237033486366272,\n",
       "   0.40673336386680603,\n",
       "   -0.30319008231163025,\n",
       "   0.2642914056777954,\n",
       "   0.18984997272491455,\n",
       "   -0.1486131101846695,\n",
       "   0.22332777082920074,\n",
       "   0.02932225912809372,\n",
       "   -0.3148611783981323,\n",
       "   0.12900416553020477,\n",
       "   -0.5674058794975281,\n",
       "   -0.3473989963531494,\n",
       "   -0.16314806044101715,\n",
       "   0.1066090539097786,\n",
       "   -0.2072911560535431,\n",
       "   -0.45603474974632263,\n",
       "   -0.1481599658727646,\n",
       "   -0.2616688907146454,\n",
       "   0.14073704183101654,\n",
       "   0.027411190792918205,\n",
       "   0.1516803354024887,\n",
       "   0.15766377747058868,\n",
       "   0.17745597660541534,\n",
       "   -0.04731939733028412,\n",
       "   0.22014401853084564,\n",
       "   -0.632477879524231,\n",
       "   0.07844548672437668,\n",
       "   -0.05542378127574921,\n",
       "   -0.06431834399700165,\n",
       "   -0.14774110913276672,\n",
       "   0.2943074703216553,\n",
       "   -0.2886044383049011,\n",
       "   0.04944414645433426,\n",
       "   -0.008627933450043201,\n",
       "   -0.16417986154556274,\n",
       "   -0.30130064487457275,\n",
       "   -0.4188293516635895,\n",
       "   0.10776587575674057,\n",
       "   -0.41280925273895264,\n",
       "   0.1078711748123169,\n",
       "   -0.051771923899650574,\n",
       "   -0.08718062192201614,\n",
       "   0.31653547286987305,\n",
       "   -0.02305823378264904],\n",
       "  [0.9966167211532593,\n",
       "   0.5051444172859192,\n",
       "   0.07963517308235168,\n",
       "   1.0646257400512695,\n",
       "   -0.08048895001411438,\n",
       "   -0.6290386319160461,\n",
       "   0.42979326844215393,\n",
       "   0.4789952337741852,\n",
       "   0.11691746115684509,\n",
       "   0.04389882832765579,\n",
       "   -0.2538796365261078,\n",
       "   0.21232983469963074,\n",
       "   -1.1743444204330444,\n",
       "   -0.17467455565929413,\n",
       "   -1.2236231565475464,\n",
       "   -0.5252821445465088,\n",
       "   -0.26662757992744446,\n",
       "   0.008942061103880405,\n",
       "   0.421027809381485,\n",
       "   0.4982360899448395,\n",
       "   -0.08733541518449783,\n",
       "   0.22606505453586578,\n",
       "   -0.47304099798202515,\n",
       "   -0.03792496398091316,\n",
       "   0.22226671874523163,\n",
       "   -0.3419150114059448,\n",
       "   0.6340206265449524,\n",
       "   2.0982701778411865,\n",
       "   0.028519276529550552,\n",
       "   1.1393637657165527,\n",
       "   0.21930691599845886,\n",
       "   -0.004971800372004509,\n",
       "   0.488019198179245,\n",
       "   0.5020009279251099,\n",
       "   -0.5341130495071411,\n",
       "   0.49867430329322815,\n",
       "   -0.321737140417099,\n",
       "   0.720301628112793,\n",
       "   -0.721585750579834,\n",
       "   0.5728517770767212,\n",
       "   -0.15725590288639069,\n",
       "   0.41489559412002563,\n",
       "   0.7037243247032166,\n",
       "   -0.8287933468818665,\n",
       "   0.7197133302688599,\n",
       "   -0.5470277667045593,\n",
       "   -0.6034749746322632,\n",
       "   0.5604392290115356,\n",
       "   -1.0745936632156372,\n",
       "   1.184788465499878,\n",
       "   -0.10053902119398117,\n",
       "   -0.7975967526435852,\n",
       "   0.2100946307182312,\n",
       "   0.4220268726348877,\n",
       "   0.7596240639686584,\n",
       "   -0.6827672719955444,\n",
       "   -0.1840948760509491,\n",
       "   0.28066685795783997,\n",
       "   -0.4203239977359772,\n",
       "   0.2819865643978119,\n",
       "   -0.35841187834739685,\n",
       "   0.11121611297130585,\n",
       "   1.0599955320358276,\n",
       "   0.2628479301929474,\n",
       "   -1.1338157653808594,\n",
       "   0.2574005126953125,\n",
       "   0.77827388048172,\n",
       "   -0.5635044574737549,\n",
       "   -0.13030032813549042,\n",
       "   -0.1581413894891739,\n",
       "   1.0779772996902466,\n",
       "   0.08295027166604996,\n",
       "   -0.10979297012090683,\n",
       "   -0.7740589380264282,\n",
       "   0.2853121757507324,\n",
       "   -0.8226655721664429,\n",
       "   0.23389515280723572,\n",
       "   -0.1773078888654709,\n",
       "   -0.6278401017189026,\n",
       "   0.5158250331878662,\n",
       "   0.6460165977478027,\n",
       "   0.509499192237854,\n",
       "   -0.6028516888618469,\n",
       "   -0.2548487186431885,\n",
       "   0.9487503170967102,\n",
       "   1.5683307647705078,\n",
       "   0.06184373050928116,\n",
       "   -0.8710122108459473,\n",
       "   -0.06904257833957672,\n",
       "   -0.25585559010505676,\n",
       "   0.4358596205711365,\n",
       "   -0.3820737302303314,\n",
       "   0.1390356719493866,\n",
       "   -0.056465111672878265,\n",
       "   -0.1614115834236145,\n",
       "   -0.07046964764595032,\n",
       "   -0.8051491379737854,\n",
       "   -0.06709631532430649,\n",
       "   -0.6068913340568542,\n",
       "   0.4388497471809387,\n",
       "   -0.7309331297874451,\n",
       "   0.36923274397850037,\n",
       "   0.12787163257598877,\n",
       "   0.6440833210945129,\n",
       "   0.3531394898891449,\n",
       "   -0.2366165667772293,\n",
       "   -0.11988411098718643,\n",
       "   -1.7992644309997559,\n",
       "   -0.14372330904006958,\n",
       "   1.2027791738510132,\n",
       "   -0.5309548377990723,\n",
       "   0.3293686807155609,\n",
       "   -0.023165186867117882,\n",
       "   -0.8793081641197205,\n",
       "   -0.28262436389923096,\n",
       "   0.5906857252120972,\n",
       "   0.16178852319717407,\n",
       "   0.24590609967708588,\n",
       "   0.15462522208690643,\n",
       "   0.23513931035995483,\n",
       "   0.31806328892707825,\n",
       "   0.6830689907073975,\n",
       "   -0.44232746958732605,\n",
       "   0.24310755729675293,\n",
       "   0.22383779287338257,\n",
       "   0.688248872756958,\n",
       "   -0.7403793931007385,\n",
       "   -0.7198989987373352,\n",
       "   0.686161458492279,\n",
       "   0.5845158696174622,\n",
       "   -0.35918930172920227,\n",
       "   -0.3482935428619385,\n",
       "   0.8548257946968079,\n",
       "   0.6806475520133972,\n",
       "   0.562252938747406,\n",
       "   -0.32170572876930237,\n",
       "   0.02165689878165722,\n",
       "   0.14444051682949066,\n",
       "   -5.382903575897217,\n",
       "   -0.7796133756637573,\n",
       "   -0.8941164016723633,\n",
       "   -0.0056001655757427216,\n",
       "   1.3077341318130493,\n",
       "   -0.6629660129547119,\n",
       "   0.08606036007404327,\n",
       "   0.20400215685367584,\n",
       "   -0.3835962414741516,\n",
       "   0.4012027084827423,\n",
       "   -0.43124517798423767,\n",
       "   0.39637699723243713,\n",
       "   -0.6387621164321899,\n",
       "   -0.24249878525733948,\n",
       "   0.039793163537979126,\n",
       "   -0.8439096808433533,\n",
       "   -0.43808513879776,\n",
       "   -0.5735242962837219,\n",
       "   0.276488333940506,\n",
       "   0.05580123886466026,\n",
       "   0.5104988813400269,\n",
       "   0.051193080842494965,\n",
       "   -0.6289898753166199,\n",
       "   -0.7949180603027344,\n",
       "   -0.4085777699947357,\n",
       "   -0.5195654034614563,\n",
       "   0.5105633735656738,\n",
       "   -0.684846043586731,\n",
       "   0.36681538820266724,\n",
       "   0.2042340785264969,\n",
       "   -1.09653902053833,\n",
       "   1.04800283908844,\n",
       "   0.45618006587028503,\n",
       "   -0.8430498242378235,\n",
       "   -0.817779541015625,\n",
       "   0.08611997216939926,\n",
       "   0.5965107083320618,\n",
       "   0.88656085729599,\n",
       "   0.34649017453193665,\n",
       "   0.0013873884454369545,\n",
       "   0.16569647192955017,\n",
       "   -0.5799516439437866,\n",
       "   0.21883176267147064,\n",
       "   0.40203648805618286,\n",
       "   -0.22860175371170044,\n",
       "   -0.42904582619667053,\n",
       "   -0.363252729177475,\n",
       "   -0.7682756781578064,\n",
       "   0.15228836238384247,\n",
       "   0.3736923635005951,\n",
       "   1.6406632661819458,\n",
       "   0.1124720498919487,\n",
       "   0.24283510446548462,\n",
       "   -0.682517409324646,\n",
       "   -1.2728900909423828,\n",
       "   0.26964643597602844,\n",
       "   -0.5900672674179077,\n",
       "   -0.7516137957572937,\n",
       "   -0.9755035638809204,\n",
       "   -0.15209747850894928,\n",
       "   0.3823690414428711,\n",
       "   1.0102916955947876,\n",
       "   -0.5913686156272888,\n",
       "   -0.013176807202398777,\n",
       "   -0.44378629326820374,\n",
       "   0.06705768406391144,\n",
       "   -0.07608476281166077,\n",
       "   -0.13592186570167542,\n",
       "   -0.30840909481048584,\n",
       "   0.05930546298623085,\n",
       "   -0.3021993339061737,\n",
       "   -1.9560682773590088,\n",
       "   -0.27834460139274597,\n",
       "   0.28361114859580994,\n",
       "   0.46100154519081116,\n",
       "   -0.20101298391819,\n",
       "   0.26015958189964294,\n",
       "   -0.7664265632629395,\n",
       "   -0.5993595719337463,\n",
       "   -0.05550796538591385,\n",
       "   0.024367036297917366,\n",
       "   -0.24750062823295593,\n",
       "   0.4292956292629242,\n",
       "   0.021416444331407547,\n",
       "   0.24583375453948975,\n",
       "   -0.8333985805511475,\n",
       "   0.2704003155231476,\n",
       "   0.8741840124130249,\n",
       "   1.235162615776062,\n",
       "   0.41042467951774597,\n",
       "   0.9193058013916016,\n",
       "   0.294432133436203,\n",
       "   0.9814884662628174,\n",
       "   0.336222380399704,\n",
       "   -0.33557313680648804,\n",
       "   -1.087821125984192,\n",
       "   -0.25340890884399414,\n",
       "   0.5105874538421631,\n",
       "   0.09396059811115265,\n",
       "   0.41363081336021423,\n",
       "   -0.7450940012931824,\n",
       "   -0.16089190542697906,\n",
       "   -0.338284969329834,\n",
       "   -0.030273091048002243,\n",
       "   0.7167381048202515,\n",
       "   -0.5249876379966736,\n",
       "   -0.01639574021100998,\n",
       "   0.3375850319862366,\n",
       "   0.03130893036723137,\n",
       "   0.7587049007415771,\n",
       "   0.11407054215669632,\n",
       "   0.37892046570777893,\n",
       "   -0.05915989354252815,\n",
       "   0.009813039563596249,\n",
       "   -0.43507328629493713,\n",
       "   -0.14671221375465393,\n",
       "   0.7179781198501587,\n",
       "   -0.19640089571475983,\n",
       "   0.6415849924087524,\n",
       "   0.31631308794021606,\n",
       "   -0.06818745285272598,\n",
       "   -0.822093665599823,\n",
       "   0.11204162240028381,\n",
       "   -0.1481219381093979,\n",
       "   0.5145490169525146,\n",
       "   -0.19566601514816284,\n",
       "   -0.1641712337732315,\n",
       "   0.1281445324420929,\n",
       "   -0.2784130573272705,\n",
       "   0.3698030710220337,\n",
       "   -0.42356592416763306,\n",
       "   0.47679442167282104,\n",
       "   0.36251965165138245,\n",
       "   -0.5520099401473999,\n",
       "   -0.04420418664813042,\n",
       "   0.4539160132408142,\n",
       "   -0.07958554476499557,\n",
       "   1.0332530736923218,\n",
       "   0.5768612027168274,\n",
       "   -0.1428653597831726,\n",
       "   -0.33160561323165894,\n",
       "   -1.1887601613998413,\n",
       "   -0.2750045955181122,\n",
       "   0.5394752621650696,\n",
       "   -0.32952746748924255,\n",
       "   1.374557375907898,\n",
       "   -0.9100776314735413,\n",
       "   0.6111758947372437,\n",
       "   0.059577152132987976,\n",
       "   0.7276814579963684,\n",
       "   -0.2569984495639801,\n",
       "   -0.09530472755432129,\n",
       "   -0.07372845709323883,\n",
       "   -0.7207300066947937,\n",
       "   -2.1627891063690186,\n",
       "   -0.6804240345954895,\n",
       "   0.3307885229587555,\n",
       "   -0.6422058939933777,\n",
       "   -0.5625907182693481,\n",
       "   0.15342599153518677,\n",
       "   -0.7942053079605103,\n",
       "   -0.270717054605484,\n",
       "   0.41012999415397644,\n",
       "   0.4917697012424469,\n",
       "   0.10126315802335739,\n",
       "   -0.17397314310073853,\n",
       "   -6.409374713897705,\n",
       "   -0.04659036919474602,\n",
       "   0.36371612548828125,\n",
       "   0.4133487641811371,\n",
       "   -0.5569242835044861,\n",
       "   0.10336039215326309,\n",
       "   1.49155592918396,\n",
       "   0.32203415036201477,\n",
       "   -0.341881662607193,\n",
       "   0.5405738353729248,\n",
       "   0.09602543711662292,\n",
       "   -1.0446789264678955,\n",
       "   0.05905568599700928,\n",
       "   -0.29885333776474,\n",
       "   -0.7720312476158142,\n",
       "   0.26325488090515137,\n",
       "   0.4404219388961792,\n",
       "   1.3648991584777832,\n",
       "   -0.5464104413986206,\n",
       "   0.6322763562202454,\n",
       "   -0.9537042379379272,\n",
       "   -0.06843580305576324,\n",
       "   -0.7914626598358154,\n",
       "   0.9899358749389648,\n",
       "   0.08795978128910065,\n",
       "   -1.270814061164856,\n",
       "   0.8117464184761047,\n",
       "   -0.5911768078804016,\n",
       "   -0.1647975891828537,\n",
       "   -0.3419404625892639,\n",
       "   -0.8356942534446716,\n",
       "   -0.2520710229873657,\n",
       "   -0.2521527409553528,\n",
       "   0.33939582109451294,\n",
       "   -0.8077852129936218,\n",
       "   0.36378100514411926,\n",
       "   0.9284151792526245,\n",
       "   0.24213586747646332,\n",
       "   -0.034690942615270615,\n",
       "   -0.7579532265663147,\n",
       "   -0.052956484258174896,\n",
       "   -0.051279325038194656,\n",
       "   -0.12210492044687271,\n",
       "   -0.14752282202243805,\n",
       "   -0.35739973187446594,\n",
       "   0.14026321470737457,\n",
       "   -0.4730013608932495,\n",
       "   -0.14283967018127441,\n",
       "   -0.2983217239379883,\n",
       "   -1.0388351678848267,\n",
       "   0.21773701906204224,\n",
       "   -0.05868032947182655,\n",
       "   0.3419225513935089,\n",
       "   0.2445373237133026,\n",
       "   -0.4015956521034241,\n",
       "   1.124936580657959,\n",
       "   0.2955513894557953,\n",
       "   0.23193201422691345,\n",
       "   0.8642783164978027,\n",
       "   -0.2646960914134979,\n",
       "   -1.0354654788970947,\n",
       "   0.07946754992008209,\n",
       "   -0.5151578783988953,\n",
       "   0.0097055584192276,\n",
       "   0.00445003854110837,\n",
       "   0.7084026336669922,\n",
       "   1.580003023147583,\n",
       "   -0.1273595094680786,\n",
       "   0.12497902661561966,\n",
       "   -0.211493581533432,\n",
       "   0.42236971855163574,\n",
       "   0.4437708854675293,\n",
       "   -0.12323924899101257,\n",
       "   -0.17022542655467987,\n",
       "   -0.37636297941207886,\n",
       "   -2.525871515274048,\n",
       "   0.12171847373247147,\n",
       "   0.20285771787166595,\n",
       "   0.4640074074268341,\n",
       "   -0.3288492262363434,\n",
       "   0.3366895914077759,\n",
       "   0.07762784510850906,\n",
       "   -0.101442851126194,\n",
       "   -0.07687701284885406,\n",
       "   0.6196879148483276,\n",
       "   0.6219304800033569,\n",
       "   0.7304767370223999,\n",
       "   -0.17463262379169464,\n",
       "   0.41307011246681213,\n",
       "   -0.8491216897964478,\n",
       "   -1.5872985124588013,\n",
       "   -0.26810356974601746,\n",
       "   -0.9491361379623413,\n",
       "   0.41002073884010315,\n",
       "   -0.5752221941947937,\n",
       "   -0.3670579791069031,\n",
       "   -0.7260394096374512,\n",
       "   -0.15508471429347992,\n",
       "   -0.6914822459220886,\n",
       "   0.05365840718150139,\n",
       "   -0.6965556144714355,\n",
       "   -0.4468706548213959,\n",
       "   -0.06434604525566101,\n",
       "   0.5237185955047607,\n",
       "   -0.04626397415995598,\n",
       "   0.2958783805370331,\n",
       "   -0.7005384564399719,\n",
       "   -0.728310227394104,\n",
       "   0.2924778461456299,\n",
       "   1.0486493110656738,\n",
       "   0.7302685976028442,\n",
       "   -1.5917117595672607,\n",
       "   0.1862296164035797,\n",
       "   0.12528210878372192,\n",
       "   0.519064724445343,\n",
       "   0.6232799887657166,\n",
       "   0.7696927785873413,\n",
       "   -0.2554882764816284,\n",
       "   0.06644042581319809,\n",
       "   -0.4419977068901062,\n",
       "   1.3128169775009155,\n",
       "   0.4945218861103058,\n",
       "   0.5364684462547302,\n",
       "   -0.24784627556800842,\n",
       "   0.029971592128276825,\n",
       "   0.4333285689353943,\n",
       "   -0.009822646155953407,\n",
       "   0.3251954913139343,\n",
       "   -0.6948190331459045,\n",
       "   0.6346620321273804,\n",
       "   -0.7995100617408752,\n",
       "   -0.09101205319166183,\n",
       "   -0.6642326712608337,\n",
       "   0.6526352763175964,\n",
       "   -0.723274827003479,\n",
       "   -0.5824556350708008,\n",
       "   -0.022724585607647896,\n",
       "   -0.9558699131011963,\n",
       "   -0.3048466444015503,\n",
       "   -1.6428205966949463,\n",
       "   -0.1611068695783615,\n",
       "   -0.5148822665214539,\n",
       "   -0.11817057430744171,\n",
       "   0.4577779173851013,\n",
       "   0.8849972486495972,\n",
       "   -1.0239150524139404,\n",
       "   -0.14930519461631775,\n",
       "   0.1370181292295456,\n",
       "   -0.19286991655826569,\n",
       "   -0.44693276286125183,\n",
       "   0.5568044781684875,\n",
       "   0.22600196301937103,\n",
       "   0.9051111936569214,\n",
       "   0.09258165955543518,\n",
       "   -0.10196694731712341,\n",
       "   1.2752866744995117,\n",
       "   -1.3741153478622437,\n",
       "   1.2203528881072998,\n",
       "   -1.0049391984939575,\n",
       "   1.2225937843322754,\n",
       "   5.112768173217773,\n",
       "   0.4303189218044281,\n",
       "   0.05519963800907135,\n",
       "   1.098520278930664,\n",
       "   -0.6836615800857544,\n",
       "   -0.05134158954024315,\n",
       "   -0.4795703589916229,\n",
       "   0.2990189790725708,\n",
       "   1.598543405532837,\n",
       "   1.5668604373931885,\n",
       "   -0.40182387828826904,\n",
       "   -0.07427643239498138,\n",
       "   -0.5434643626213074,\n",
       "   0.5273316502571106,\n",
       "   -0.12276145070791245,\n",
       "   0.6446517109870911,\n",
       "   0.3823082447052002,\n",
       "   0.24053871631622314,\n",
       "   -0.6423052549362183,\n",
       "   -0.4800141453742981,\n",
       "   1.1436493396759033,\n",
       "   0.10598567873239517,\n",
       "   0.7766119837760925,\n",
       "   -0.029251111671328545,\n",
       "   -0.01646958850324154,\n",
       "   0.5901612043380737,\n",
       "   0.2668982744216919,\n",
       "   0.15016524493694305,\n",
       "   -0.7249329090118408,\n",
       "   0.39419037103652954,\n",
       "   0.4448665678501129,\n",
       "   0.8646467924118042,\n",
       "   -0.0587930753827095,\n",
       "   0.2969025671482086,\n",
       "   -0.1851053088903427,\n",
       "   0.36294394731521606,\n",
       "   0.13071677088737488,\n",
       "   -0.3848944902420044,\n",
       "   -0.13766945898532867,\n",
       "   0.45853739976882935,\n",
       "   0.09381802380084991,\n",
       "   1.0631780624389648,\n",
       "   0.06681740283966064,\n",
       "   -0.5895481705665588,\n",
       "   -0.14019137620925903,\n",
       "   -1.2560518980026245,\n",
       "   -0.4526907205581665,\n",
       "   -1.6102303266525269,\n",
       "   -0.15786339342594147,\n",
       "   -0.032844170928001404,\n",
       "   -0.16954612731933594,\n",
       "   0.09798015654087067,\n",
       "   0.9523454308509827,\n",
       "   0.11758311092853546,\n",
       "   0.4550606608390808,\n",
       "   0.19058789312839508,\n",
       "   -0.427968829870224,\n",
       "   -0.4608757495880127,\n",
       "   -1.6068915128707886,\n",
       "   -0.011250930838286877,\n",
       "   0.10770045965909958,\n",
       "   0.08881446719169617,\n",
       "   0.8807703852653503,\n",
       "   0.6028741598129272,\n",
       "   0.47142648696899414,\n",
       "   0.11569840461015701,\n",
       "   -1.0927021503448486,\n",
       "   -0.5552634596824646,\n",
       "   -0.18707242608070374,\n",
       "   -0.18943868577480316,\n",
       "   1.259253978729248,\n",
       "   -0.08302183449268341,\n",
       "   -0.4243983328342438,\n",
       "   0.004647224675863981,\n",
       "   0.12677505612373352,\n",
       "   0.28445255756378174,\n",
       "   0.17360198497772217,\n",
       "   0.9756999015808105,\n",
       "   0.014935598708689213,\n",
       "   -0.36958152055740356,\n",
       "   -0.10539411753416061,\n",
       "   0.4223752021789551,\n",
       "   -0.22875675559043884,\n",
       "   1.0328477621078491,\n",
       "   0.2999251186847687,\n",
       "   0.49591541290283203,\n",
       "   0.4197918772697449,\n",
       "   0.84428870677948,\n",
       "   -1.7034052610397339,\n",
       "   0.5264266729354858,\n",
       "   1.2397979497909546,\n",
       "   0.08535744249820709,\n",
       "   -0.9382041692733765,\n",
       "   -1.5194193124771118,\n",
       "   0.45302435755729675,\n",
       "   0.14635130763053894,\n",
       "   0.18718239665031433,\n",
       "   -0.20729340612888336,\n",
       "   0.36814209818840027,\n",
       "   0.7737175822257996,\n",
       "   0.043797168880701065,\n",
       "   0.45840984582901,\n",
       "   0.42265403270721436,\n",
       "   -0.7509164214134216,\n",
       "   0.5669091939926147,\n",
       "   0.23469781875610352,\n",
       "   -0.04661611467599869,\n",
       "   -0.33819320797920227,\n",
       "   0.18430380523204803,\n",
       "   -0.03263084590435028,\n",
       "   -0.7767742872238159,\n",
       "   0.8298144936561584,\n",
       "   0.777611494064331,\n",
       "   0.536408007144928,\n",
       "   0.4809402823448181,\n",
       "   -0.07273354381322861,\n",
       "   0.2522663176059723,\n",
       "   0.5163940787315369,\n",
       "   0.7864490747451782,\n",
       "   -0.30754607915878296,\n",
       "   -0.3332899510860443,\n",
       "   0.6611071228981018,\n",
       "   -0.7583495378494263,\n",
       "   -0.42717722058296204,\n",
       "   0.4360656440258026,\n",
       "   -0.5050910711288452,\n",
       "   -0.0008139469427987933,\n",
       "   0.2606763243675232,\n",
       "   -1.3681902885437012,\n",
       "   0.1644117683172226,\n",
       "   -0.047917354851961136,\n",
       "   0.04641428962349892,\n",
       "   -1.6265344619750977,\n",
       "   -0.6035731434822083,\n",
       "   0.16399438679218292,\n",
       "   -0.3901691138744354,\n",
       "   0.9699589610099792,\n",
       "   0.5005282163619995,\n",
       "   0.034055184572935104,\n",
       "   -0.8668608069419861,\n",
       "   0.24439896643161774,\n",
       "   0.18158771097660065,\n",
       "   0.49089688062667847,\n",
       "   5.377180099487305,\n",
       "   0.2553401589393616,\n",
       "   -0.5807622671127319,\n",
       "   -0.8583126068115234,\n",
       "   -0.40704962611198425,\n",
       "   0.4460916817188263,\n",
       "   -0.30213963985443115,\n",
       "   0.7278069853782654,\n",
       "   0.4642864763736725,\n",
       "   -0.6722916960716248,\n",
       "   -0.48416516184806824,\n",
       "   -0.5266543030738831,\n",
       "   1.877712607383728,\n",
       "   -0.1546204537153244,\n",
       "   -1.058833122253418,\n",
       "   0.976896345615387,\n",
       "   -0.9349130392074585,\n",
       "   0.2663828730583191,\n",
       "   0.6076732277870178,\n",
       "   -0.6981245875358582,\n",
       "   -1.0044283866882324,\n",
       "   -0.28439271450042725,\n",
       "   0.36444181203842163,\n",
       "   -0.10854543000459671,\n",
       "   -0.10744282603263855,\n",
       "   -0.16358964145183563,\n",
       "   -1.286491870880127,\n",
       "   0.2194775640964508,\n",
       "   -1.0823618173599243,\n",
       "   -0.3427114188671112,\n",
       "   -0.6361168026924133,\n",
       "   0.38542988896369934,\n",
       "   1.1239668130874634,\n",
       "   -0.3336487412452698,\n",
       "   0.20754043757915497,\n",
       "   -1.0489848852157593,\n",
       "   -0.006864205934107304,\n",
       "   0.5844831466674805,\n",
       "   0.3371310532093048,\n",
       "   0.7886576652526855,\n",
       "   -0.16389770805835724,\n",
       "   -0.4167417287826538,\n",
       "   -0.5695077180862427,\n",
       "   0.22339126467704773,\n",
       "   -1.2867095470428467,\n",
       "   -0.26526686549186707,\n",
       "   -0.20364105701446533,\n",
       "   0.36555132269859314,\n",
       "   0.3810237646102905,\n",
       "   0.822575569152832,\n",
       "   -1.0550479888916016,\n",
       "   -0.3023073971271515,\n",
       "   -0.8102005124092102,\n",
       "   -0.3459678590297699,\n",
       "   0.4030509293079376,\n",
       "   -0.5622060894966125,\n",
       "   -0.009597983211278915,\n",
       "   -0.2411860227584839,\n",
       "   -0.43737804889678955,\n",
       "   0.13732852041721344,\n",
       "   -0.08908013999462128,\n",
       "   -0.22049954533576965,\n",
       "   0.8654423952102661,\n",
       "   0.2782460153102875,\n",
       "   -0.05349395424127579,\n",
       "   0.7777310013771057,\n",
       "   0.7855016589164734,\n",
       "   -0.4795176088809967,\n",
       "   0.149807408452034,\n",
       "   -0.19033168256282806,\n",
       "   0.4283086061477661,\n",
       "   -1.2399771213531494,\n",
       "   0.32876190543174744,\n",
       "   -0.30284571647644043,\n",
       "   0.09259052574634552,\n",
       "   -0.32082295417785645,\n",
       "   0.24108871817588806,\n",
       "   -0.562587320804596,\n",
       "   -0.0696595087647438,\n",
       "   -0.33742204308509827,\n",
       "   1.1417276859283447,\n",
       "   0.24445398151874542,\n",
       "   -0.759178638458252,\n",
       "   -0.016099650412797928,\n",
       "   0.5024926662445068,\n",
       "   -0.403480589389801,\n",
       "   0.007405269891023636,\n",
       "   -0.8329917192459106,\n",
       "   -0.9079859852790833,\n",
       "   -0.1495819091796875,\n",
       "   -0.8839244842529297,\n",
       "   0.08374495804309845,\n",
       "   0.6902533173561096,\n",
       "   0.2695488929748535,\n",
       "   1.0885688066482544,\n",
       "   -0.47279030084609985,\n",
       "   -0.17973633110523224,\n",
       "   -0.4755559265613556,\n",
       "   -0.3599908649921417,\n",
       "   0.7644364237785339,\n",
       "   -0.07562985271215439,\n",
       "   0.23733824491500854,\n",
       "   0.15481601655483246,\n",
       "   -0.8402557373046875,\n",
       "   0.16034701466560364,\n",
       "   -0.5869194269180298,\n",
       "   0.4438451826572418,\n",
       "   0.3376540541648865,\n",
       "   -0.7756620049476624,\n",
       "   -0.344745934009552,\n",
       "   -1.3999539613723755,\n",
       "   -0.3533537983894348,\n",
       "   0.1645323932170868,\n",
       "   -0.6445151567459106,\n",
       "   -0.39252933859825134,\n",
       "   0.48214778304100037,\n",
       "   -0.6707288026809692,\n",
       "   -0.35032251477241516,\n",
       "   0.5845313668251038,\n",
       "   0.49622541666030884,\n",
       "   -0.059253543615341187,\n",
       "   -0.3247281014919281,\n",
       "   0.38811177015304565,\n",
       "   0.8882664442062378,\n",
       "   -0.019735991954803467,\n",
       "   0.01069753710180521,\n",
       "   0.19899943470954895,\n",
       "   -0.05850212648510933,\n",
       "   0.6227821707725525,\n",
       "   -0.3181813359260559,\n",
       "   0.13129013776779175,\n",
       "   -0.15552867949008942,\n",
       "   0.02386552095413208,\n",
       "   -0.5985141396522522,\n",
       "   0.31483614444732666,\n",
       "   -0.0944097489118576,\n",
       "   0.46019697189331055,\n",
       "   0.6278021335601807,\n",
       "   -0.5441287159919739,\n",
       "   0.3171497881412506,\n",
       "   -0.9902050495147705,\n",
       "   -0.4453067183494568,\n",
       "   -0.7756794691085815,\n",
       "   1.148485779762268,\n",
       "   0.5179709792137146,\n",
       "   0.39670154452323914,\n",
       "   -0.23297858238220215,\n",
       "   -1.2480065822601318,\n",
       "   -0.3433881103992462,\n",
       "   0.9208337068557739,\n",
       "   0.4773138463497162,\n",
       "   -0.5381412506103516,\n",
       "   0.6531148552894592,\n",
       "   -0.3053356409072876,\n",
       "   -0.2754022181034088,\n",
       "   0.3247191905975342,\n",
       "   0.4298000931739807,\n",
       "   0.364690363407135,\n",
       "   -0.1615821272134781,\n",
       "   0.6605650186538696,\n",
       "   -0.16736279428005219]]]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The huggingface/transformers repository lists the other pipeline functions, such as ner extraction, sequence classification, and masking. You are encouraged to explore them. \n",
    "https://github.com/huggingface/transformers#quick-tour-of-pipelines\n",
    "\n",
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, use the pipeline functions or the word or sentence vector functions (e.g., similarity) to explore the social game underlying the production and meaning of texts associated with your final project. You have used similar, but often weaker versions in previous weeks. How does BERT help you gain insight regarding your research question that is similar and different from prior methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = ndf[ndf.label == 0].sample(100, random_state=20210310)[['text']]\n",
    "post_df = ndf[ndf.label == 1].sample(100, random_state=20210310)[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 100 samples out of each label, compute their sentiments with pipeline\n",
    "pre_df['sentiment'] = pre_df.text.apply(lambda x: nlp_sentiment(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df['sentiment'] = post_df.text.apply(lambda x: nlp_sentiment(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df['sentiment_tag'] = pre_df.sentiment.apply(lambda x: x['label'])\n",
    "pre_df['sentiment_score'] = pre_df.sentiment.apply(lambda x: x['score'])\n",
    "post_df['sentiment_tag'] = post_df.sentiment.apply(lambda x: x['label'])\n",
    "post_df['sentiment_score'] = post_df.sentiment.apply(lambda x: x['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_tag\n",
       "NEGATIVE    56\n",
       "POSITIVE    44\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df.groupby('sentiment_tag')['text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_tag\n",
       "NEGATIVE    55\n",
       "POSITIVE    45\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df.groupby('sentiment_tag')['text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0021677130460739137"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(pre_df.sentiment_score) - np.array(post_df.sentiment_score)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the random sample of the text from two labels, we can see that Nietzsche's text after 1879 is slightly more negative than his work before, but this difference is minor. The BERT embeddings is better because it takes account in sentence structures more so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation using BERT\n",
    "\n",
    "The last method which we will explore is text generation. While some may regard it as a parlour trick due to unpredictability, recent dramatic improvements in text generation suggest that these kind of models can find themselves being used in more serious social scientific applications, such as in survey design and construction, idiomatic translation, and the normalization of phrase and sentence meanings.\n",
    "\n",
    "These models can be quite impressive, even uncanny in how human like they sound. Check out this [cool website](https://transformer.huggingface.co), which allows you to write with a transformer. The website is built by the folks who wrote the package we are using. The code underneath the website can be found in their examples: [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py).\n",
    "\n",
    "We will be using the built in generate function, but the example file has more detailed code which allows you to set the seed differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc767c883934e148e2618b3f352d78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=665.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b721e3c2694b0098b052d143436efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1042301.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4862adf2e9b6470a9f10892fb26e8312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=456318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ab465b4fc249f58070e802cc60b3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1355256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5f06cd723946e0a8136c70bffdfe99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=548118077.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing that we like to do more than analyse data all day long and then try to figure out what's going on.\n",
      "\n",
      "\"We're not going to be able to do that. We're not going to be able to do that. We\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Nothing that we like to do more than analyse data all day long and\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. A little creepy, and as we can see, far from perfect: GPT doesn't alwats work out flawlessly, but it sometimes can, and we will try and see if fine-tuning helps. We are going to tune the model on a complete dataset of Trump tweets, as they have a set of distinctive, highly identifiable qualities.\n",
    "\n",
    "### Creating a domain-specific language model\n",
    "\n",
    "One of the most exciting things about BERT and GPT is being able to retune them the way we want to. We will be training models to perform two tasks - one is to create a BERT with an \"accent\", by traning a model with english news data from the UK, from the US, and from India. We will also train a language generation model with a bunch of Trump tweets. \n",
    "\n",
    "We can train models specifically over a certain domain to make its language generation similar to that domain. \n",
    "[run_language modelling.py](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py), followed by [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py). I've downloaded these files and added them to this directory so we can run them through the notebook. You are encouraged to look at these files to get a rough idea of what is going on.\n",
    "\n",
    "### Loading Data \n",
    "\n",
    "We want to now get our Trump tweets and our English news datasets ready. The data the scripts expect is just a text file with relevant data. We load the Trump tweets and then write them to disk as train and test files with only data. I leave the original dataframes in case you would like to use it for your own purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for file in os.listdir(\"../data/trump_tweets\"):\n",
    "    dfs.append(pd.read_json(\"../data/trump_tweets/\" + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>id_str</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 13:37:52</td>\n",
       "      <td>51473</td>\n",
       "      <td>947824196909961216</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>8237</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Will be leaving Florida for Washington (D.C.) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 12:44:40</td>\n",
       "      <td>53557</td>\n",
       "      <td>947810806430826496</td>\n",
       "      <td>25073877.0</td>\n",
       "      <td>False</td>\n",
       "      <td>14595</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Iran is failing at every level despite the ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 12:12:00</td>\n",
       "      <td>138808</td>\n",
       "      <td>947802588174577664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>49566</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>The United States has foolishly given Pakistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-31 23:43:04</td>\n",
       "      <td>154769</td>\n",
       "      <td>947614110082043904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>35164</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>HAPPY NEW YEAR! We are MAKING AMERICA GREAT AG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-31 22:18:20</td>\n",
       "      <td>157655</td>\n",
       "      <td>947592785519173632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>39428</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>As our Country rapidly grows stronger and smar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           created_at  favorite_count              id_str  \\\n",
       "0 2018-01-01 13:37:52           51473  947824196909961216   \n",
       "1 2018-01-01 12:44:40           53557  947810806430826496   \n",
       "2 2018-01-01 12:12:00          138808  947802588174577664   \n",
       "3 2017-12-31 23:43:04          154769  947614110082043904   \n",
       "4 2017-12-31 22:18:20          157655  947592785519173632   \n",
       "\n",
       "   in_reply_to_user_id_str  is_retweet  retweet_count              source  \\\n",
       "0                      NaN       False           8237  Twitter for iPhone   \n",
       "1               25073877.0       False          14595  Twitter for iPhone   \n",
       "2                      NaN       False          49566  Twitter for iPhone   \n",
       "3                      NaN       False          35164  Twitter for iPhone   \n",
       "4                      NaN       False          39428  Twitter for iPhone   \n",
       "\n",
       "                                                text  \n",
       "0  Will be leaving Florida for Washington (D.C.) ...  \n",
       "1  Iran is failing at every level despite the ter...  \n",
       "2  The United States has foolishly given Pakistan...  \n",
       "3  HAPPY NEW YEAR! We are MAKING AMERICA GREAT AG...  \n",
       "4  As our Country rapidly grows stronger and smar...  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text = train_test_split(df['text'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1239    The @WashingtonPost quickly put together a hit...\n",
       "4335    WOW, SO NICE AND SO TRUE. THANK YOU! \"@not_tha...\n",
       "2894    It is time to send someone from the outside to...\n",
       "4179    \"@jkapper15: @realDonaldTrump please deeply co...\n",
       "676     \"@twins44: @realDonaldTrump I think you are re...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text.to_frame().to_csv(r'train_text_trump', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_text.to_frame().to_csv(r'test_text_trump', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now used the Google Colab GPUs to train the Trump tweet models. We'll be doing the same for our blog posts too.\n",
    "\n",
    "### GloWBe dataset\n",
    "\n",
    "We'll now load up the GloWbe (Corpus of Global Web-Based English) dataset which have different texts from different countries. We'll try and draw out texts from only the US, UK and India. We'll then save these to disk. Note that this is a Davies Corpora dataset: the full download can be done with the Dropbox link I sent in an announcement a few weeks ago. The whole download is about 3.5 GB but we only need two files, which are anout 250 MB each. The other files might be useful for your research purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lucem_illud_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "address = \"/Users/bhargavvader/Downloads/Academics_tech/corpora/GloWbE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these are the exact name of the files\n",
    "us = \"/text_us_blog_jfy.zip\"\n",
    "gb = \"/text_gb_blog_akq.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_us_blog_jfy.zip\n"
     ]
    }
   ],
   "source": [
    "us_texts = lucem_illud_2020.loadDavies(address, corpus_style=\"us_blog\", num_files=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_gb_blog_akq.zip\n"
     ]
    }
   ],
   "source": [
    "gb_texts = lucem_illud_2020.loadDavies(address, corpus_style=\"gb_blog\", num_files=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dictionary with document ids mapping to text. Since we don't need any information but the text, we can just save these to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'< p > Many workers within the UK are required to wear an uniform for work but very few are aware they could really claim back some money from the tax man to help with the price of washing or repairing the uniform HMRC will actually pay money back once again to those individuals who are eligible even dating back four years worth of washing < p > In order to find out about claiming a tax rebate on uniform it is worth having a look online to find out whether or not you will be eligible The conditions are fairly straightforward you just have to wear a recognisable work uniform which might contain a T shirt which displays a logo design or possibly some specialist protective clothing The type of the occupation is unrelated anyone from nurses to cops to electricians are able to claim As long as the uniform is worn at work is washed on your own and you are an UK tax payer then your chances are you will be eligible to claim are eligible how do you actually go about claiming your tax refund for washing uniform You can access the HMRC web site directly and complete the important forms to help you to claim your hard earned money back Or you can use an agent that is registered with the HMRC who are able to deal with the whole tax rebate on your behalf This may save you a great deal of time and effort and they may also have the ability to advise you on any other areas where you could claim a rebate for instance if you are required to supply your own tools within your job < p > The amount of money you can claim as a tax rebate on uniform will depend on your profession and how long you have already been wearing a work uniform for You might be in a position to claim a rebate for the past four years which could add up to a fine sum of money < p > In those times of economic decline most people are seeking to save lots of washing uniform can be an effective way to start saving a couple of pounds every month which over the course of a'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(list(us_texts.values())[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict_to_texts(texts, file_name):\n",
    "    text = []\n",
    "    for doc in list(texts.values()):\n",
    "        text.append(' '.join(doc).replace(\"< h >\", \"\").replace(\"< p >\", \"\"))\n",
    "    train_text, test_text = train_test_split(text, test_size=0.2)\n",
    "    with open(file_name + \"_train\", 'w') as f:\n",
    "        for item in train_text:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n",
    "    with open(file_name + \"_test\", 'w') as f:\n",
    "        for item in test_text:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_to_texts(us_texts, \"us_blog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_to_texts(gb_texts, \"gb_blog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the training and testing files for both US and GB blogs in English. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WARNING - SHIFT TO GOOGLE COLAB OR GPU ENABLED MACHINE\n",
    "\n",
    "The [Google Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) walks you through the process of fine-tuning models, as we did before for the classification task. Move now to the colab file to fine tune your models. Once you downloaded all the models and their information, place those files in the directory of the HW to use them as demonstrated below. \n",
    "\n",
    "\n",
    "\n",
    "### Running Scripts\n",
    "\n",
    "We use the scripts to do language modelling and text generation. The following cells run the code as if you would have run it in a terminal. I trained all of these models using the Googlr Colab file, and then saved the models to disk.\n",
    "\n",
    "#### Trump GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python run_language_modelling.py --output_dir=output_gpt_trump --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=train_text_trump --do_eval --eval_data_file=test_text_trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoBERTa US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python run_language_modeling.py --output_dir=output_roberta_US --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=$TRAIN_FILE --do_eval --eval_data_file=$TEST_FILE --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoBERTa UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python run_language_modeling.py --output_dir=output_roberta_UK --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=$TRAIN_FILE --do_eval --eval_data_file=$TEST_FILE --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COME BACK TO THIS NOTEBOOK to load and work with your trained model\n",
    "\n",
    "### Loading and using models\n",
    "\n",
    "Let us now load the four models we have and see how we can use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now - let us see what our Trump Tweet Bot looks like!\n",
    "You can generate text via command line using the command below. You can also load a model once it is saved - I trained my model using Google Colab, downloaded the model, and am loading it again via the command below. Note that you have to download all the files in your folder of the fine-tuned model to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !python run_generation.py --model_type=gpt2 --model_name_or_path=output_trump_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer_trump = AutoTokenizer.from_pretrained(\"output_trump_gpt\")\n",
    "model_trump = AutoModelWithLMHead.from_pretrained(\"output_trump_gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama is going to be a disaster for the United States. He is a total loser. He is a total loser!\"\n",
      "\"\"\"@jeff_mcclaren: @realDonaldTrump @realDonaldTrump @foxandfriends @megynkelly @\"\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Obama is going to\"\n",
    "\n",
    "input = tokenizer_trump.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_trump.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_trump.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - our Trump bot is nasty, so we know our model trained well. What happens if we try the same sentence for our non-fine tuned model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama is going to be a very good president,\" said Sen. John McCain (R-Ariz.). \"He's going to be a very good president. He's going to be a very good president. He's going to be a very\"\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Obama is going to\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite the contrast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that generate a BERT-powered chatbot tuned on text related to your final project. What is interesting about this model, and how to does it compare to an untrained model? What does it reveal about the social game involved with your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life is nothing but a game of chance.\n",
      "\n",
      "\"I'm not going to say that I'm going to be a bad player, but I'm not going to be a bad player,\" he said. \"I'm not going to be a\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Life is nothing but\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One should live like a man, and live like a woman.\n",
      "\n",
      "The first thing you should do is to be a man.\n",
      "\n",
      "The second thing you should do is to be a woman.\n",
      "\n",
      "The third thing you should do is\n"
     ]
    }
   ],
   "source": [
    "sequence = \"One should live like\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My dear friend, I am so sorry for your loss. I am so sorry for your loss. I am so sorry for your loss. I am so sorry for your loss. I am so sorry for your loss. I am so sorry for your\n"
     ]
    }
   ],
   "source": [
    "sequence = \"My dear friend\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Nietzsche Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text = train_test_split(ndf['text'], test_size=0.2, random_state=20210310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text.to_frame().to_csv(r'train_text_nietzsche', header=None, index=None, sep=' ', mode='a')\n",
    "test_text.to_frame().to_csv(r'test_text_nietzsche', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading trained model\n",
    "tokenizer_nietzsche = AutoTokenizer.from_pretrained(\"output_nietzsche\")\n",
    "model_nietzsche = AutoModelWithLMHead.from_pretrained(\"output_nietzsche\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life is nothing but a _dramatic_ phenomenon, and the _dramatic_ phenomenon is the _dramatic_ phenomenon.\"\n",
      "\"The _dramatic_ phenomenon is the _dramatic_ phenomenon, and the _\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Life is nothing but\"\n",
    "\n",
    "input = tokenizer_nietzsche.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_nietzsche.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_nietzsche.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One should live like a man, and not like a woman.\"\n",
      "\"The _décadence_ of the _décadence_ is the _décadence_ of the _décadence_\n"
     ]
    }
   ],
   "source": [
    "sequence = \"One should live like\"\n",
    "\n",
    "input = tokenizer_nietzsche.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_nietzsche.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_nietzsche.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My dear friend, I am not a man of the future, but a man of the past.\"\n",
      "\"The _décadence_ of the _décadence_ is the _décadence_ of the\n"
     ]
    }
   ],
   "source": [
    "sequence = \"My dear friend\"\n",
    "\n",
    "input = tokenizer_nietzsche.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_nietzsche.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_nietzsche.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you gaze for long into an abyss, you will find that it is a labyrinth, and that it is a labyrinth of labyrinths.\"\n",
      "\"The _décadence_ of the _décadence_ is the _\n"
     ]
    }
   ],
   "source": [
    "sequence = \"If you gaze for long into an abyss\"\n",
    "\n",
    "input = tokenizer_nietzsche.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_nietzsche.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_nietzsche.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment\n",
    "Comparing to the untrained model, the gpt model tuned with Nietzsche's works shows more of Nietzsche's personal touch. We can see that the Nietzsche bot answers with \"Life is nothing but [a dramatic phenomenon]\" instead of \"[a game of chance].\" The two bots also differs in their attitudes toward men and women. The untrained bot suggest that one should live like a woman while the Nietzsche bot suggests that one should not. This contrast matches Nietzsche's dislike for women in his works. In addition, when we start with \"My dear friend,\" the untrained bot shows a common usage of the phrase, while the Nietzsche bot displays a very philosopher-like response (although I can't be sure what theory this can be related to).\n",
    "\n",
    "Lastly, when we input the first part of Nietzsche's abyss quote, the bot didn't give the latter part (\"the abyss also gaze into you\"), but provides an equally interesting answer, that one will find an abyss to be an labyrinth. Perhaps the bot is displaying a dark parallel relationship between the abyss and the labyrinth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Plato Model (part of exercise 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>sentence_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plato - Complete Works</td>\n",
       "      <td>What's new, Socrates, to make you leave your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Plato - Complete Works</td>\n",
       "      <td>Surely you are not prosecuting anyone before t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Plato - Complete Works</td>\n",
       "      <td>The Athenians do not call this a prosecution b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Plato - Complete Works</td>\n",
       "      <td>What is this you say?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Plato - Complete Works</td>\n",
       "      <td>Someone must have indicted you, for you are no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    title                                       sentence_str\n",
       "0  Plato - Complete Works   What's new, Socrates, to make you leave your ...\n",
       "1  Plato - Complete Works  Surely you are not prosecuting anyone before t...\n",
       "2  Plato - Complete Works  The Athenians do not call this a prosecution b...\n",
       "3  Plato - Complete Works                              What is this you say?\n",
       "4  Plato - Complete Works  Someone must have indicted you, for you are no..."
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phil_df = pd.read_csv('phil_nlp.csv')\n",
    "\n",
    "plato_df = phil_df[phil_df.author == 'Plato'][['title', 'sentence_str']]\n",
    "plato_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text = train_test_split(plato_df['sentence_str'], test_size=0.2, random_state=20210310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text.to_frame().to_csv(r'train_text_plato', header=None, index=None, sep=' ', mode='a')\n",
    "test_text.to_frame().to_csv(r'test_text_plato', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_plato = AutoTokenizer.from_pretrained(\"output_plato\")\n",
    "model_plato = AutoModelWithLMHead.from_pretrained(\"output_plato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life is nothing but a mixture of the two.\"\n",
      "\"And if you are to be a good man, you must be a good man, and you must be a good man, and you must be a good man, and you must be a\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Life is nothing but\"\n",
    "\n",
    "input = tokenizer_plato.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_plato.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_plato.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One should live like a man, and not be ashamed of his own nature.\"\n",
      "\"And if he is not, he must be a slave to the gods.\"\n",
      "\"And if he is not, he must be a slave to the gods.\"\n"
     ]
    }
   ],
   "source": [
    "sequence = \"One should live like\"\n",
    "\n",
    "input = tokenizer_plato.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_plato.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_plato.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My dear friend, I think you're right to say that the best way to deal with the problem is to make it clear to the people who are concerned about it.\"\n",
      "\"And if you are to be a good man, you must be a\n"
     ]
    }
   ],
   "source": [
    "sequence = \"My dear friend\"\n",
    "\n",
    "input = tokenizer_plato.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_plato.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_plato.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment\n",
    "I also trained a gpt model for Plato for comparison. Although we inputted the same starting phrase to these two bots, they display very different answers, indicating that the two bots are operating on very different semantic spaces.\n",
    "\n",
    "The Plato bot resembles Plato himself to a great extent. We observe that the Plato bot keeps emphasizing that one should be a good man, which is central to Plato's philosophy, and that one should \"not be ashamed of his own nature,\" a very platonian quote. The style the Plato bot speaks is alsp very much like that of Greek philosophers, as observed in the \"My dear friend\" answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check out our UK and GB embeddings - how do you think the two models will differ? Maybe in the way different words relate to each other in the same sentence? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_us_model_embedding = RobertaModel.from_pretrained('roberta_us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roberta_us_tokenizer = RobertaTokenizer.from_pretrained('roberta_us')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to visualise how words in a sentence or different or similar to each other. We will try to construct sentences where words might mean different things in different countries - in the US, people might eat chips with salsa, but in the UK, chips are what Americans call french fries, and might eat it fried fish instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Do you have your chips with fish or with salsa?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1 = \"He went out in just his undershirt and pants.\" #pants are underwear in Britain; maybe closer to an undershirt\n",
    "text2 = \"His braces completed the outfit.\" #braces are suspenders (in Britain); maybe closer to an outfit\n",
    "text3 = \"Does your pencil have a rubber on it?\" #rubber is an eraser in Britain); maybe closer to a pencil\n",
    "text4 = \"Was the bog closer to the forest or the house?\" #bog is a toilen in Britain); maybe closer to a house\n",
    "text5 = \"Are you taking the trolley or the train to the grocery market\" #trolley is a food carriage; possibly closer to a market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_diffs(text, model, tokenizer):\n",
    "    word_vecs = []\n",
    "    for i in range(0, len(text.split())):\n",
    "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
    "    L = []\n",
    "    for p in word_vecs:\n",
    "        l = []\n",
    "        for q in word_vecs:\n",
    "            l.append(1 - cosine(p, q))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "    fig = plt.figure()\n",
    "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
    "    ax = sns.heatmap(div)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'roberta_us_model_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-d828e616470b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualise_diffs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroberta_us_model_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroberta_us_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'roberta_us_model_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "visualise_diffs(text, roberta_us_model_embedding, roberta_us_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roberta_gb_model_embedding = RobertaModel.from_pretrained('roberta_gb')\n",
    "roberta_gb_tokenizer = RobertaTokenizer.from_pretrained('roberta_gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, roberta_gb_model_embedding, roberta_gb_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see regarding the relations with chips and sala/fish? What about the other sentences? How about comparing sentence embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that tune BERT to at least two different textual samples. These could be from different corpora, distinct time periods, separate authors, alternative publishing outlets, etc. Then compare the meaning of words, phrases and sentences to each other across the separate models. What do they reveal about the social worlds inscribed by the distinctive samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Plato and Nietzsche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at output_nietzsche_roberta and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at output_plato_roberta and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "roberta_nietzsche_model_embedding = RobertaModel.from_pretrained('output_nietzsche_roberta')\n",
    "roberta_nietzsche_tokenizer = RobertaTokenizer.from_pretrained('output_nietzsche_roberta')\n",
    "roberta_plato_model_embedding = RobertaModel.from_pretrained('output_plato_roberta')\n",
    "roberta_plato_tokenizer = RobertaTokenizer.from_pretrained('output_plato_roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = 'Life can be a beautiful thing, or it can be very painful.'\n",
    "visualise_diffs(text, roberta_nietzsche_model_embedding, roberta_nietzsche_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = 'Life can be a beautiful thing, or it can be very painful.'\n",
    "visualise_diffs(text, roberta_plato_model_embedding, roberta_plato_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we input the sentence \"Life can be a beautiful thing, or it can be very painful.\" into the Nietzsche and Plato RoBERTa models, minor differences arises. The pairs \"be/very\", \"very/painful\" are more similar in Nietzsche's work, than in Plato's work. The pair \"life/very,\" however, appears more similar in Plato's text than in Nietzsche's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"Is man merely a mistake of God's? Or God merely a mistake of man?\"\n",
    "visualise_diffs(text, roberta_nietzsche_model_embedding, roberta_nietzsche_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"Is man merely a mistake of God's? Or God merely a mistake of man?\"\n",
    "visualise_diffs(text, roberta_plato_model_embedding, roberta_plato_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then input Nietzsche's quote \"Is man merely a mistake of God's? Or God merely a mistake of man?\" Interestingly, the pair of \"God/man\" is more similar in Plato's text than in Nietzsche's text, which makes sense since Nietzsche puts man more in opposition with God than Plato, and Plato lives in a polytheistic society (he will more often mention \"gods\").\n",
    "\n",
    "Overall, we can see more differing colors in these two graphs. Although the differences are minuscule, they still show that the Nietzsche corpus differs from the Plato corpus in semantic space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
